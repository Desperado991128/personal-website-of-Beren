<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SY&#39;s Gaze LLM Daily Review - 2026-03-01</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['\\(','\\)']], displayMath: [['\\[','\\]']]}
        });
    </script>
    <style>
        body {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            font-size: 18px;
        }
        header {
            margin-bottom: 2rem;
            border-bottom: 1px solid #eaecef;
        }
        .site-title {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2em;
            margin-bottom: 0.5em;
        }
        nav ul {
            display: flex;
            list-style: none;
            padding: 0;
            margin: 1rem 0;
        }
        nav ul li {
            margin-right: 1.5rem;
        }
        nav ul li a {
            text-decoration: none;
            color: #0366d6;
            font-weight: 600;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        .container {
            width: 100%;
            max-width: 800px;
        }
        h1 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2.4em;
            margin-bottom: 0.5em;
            line-height: 1.2;
        }
        h2 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.8em;
            margin-top: 1.5em;
            margin-bottom: 0.75em;
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }
        h3 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.5em;
            margin-top: 1.2em;
            margin-bottom: 0.6em;
        }
        h4 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.2em;
            margin-top: 1em;
            margin-bottom: 0.5em;
        }
        p {
            margin: 1em 0;
        }
        code {
            font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
            background: #f6f8fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 0.9em;
        }
        pre {
            background: #f6f8fa;
            padding: 16px;
            border-radius: 6px;
            overflow: auto;
            font-size: 0.9em;
        }
        blockquote {
            margin: 1em 0;
            padding: 0 1em;
            color: #6a737d;
            border-left: 0.25em solid #dfe2e5;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1.5em auto;
        }
        .meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
        }
        .post-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
        }
        /* SY Profile specific styling - green tint */
        .label {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: 600;
        }
        .figure-caption {
            text-align: center;
            color: #666;
            font-size: 0.9em;
            margin-top: -1em;
            margin-bottom: 2em;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1.5em 0;
        }
        th, td {
            border: 1px solid #dfe2e5;
            padding: 8px 12px;
            text-align: left;
        }
        th {
            background-color: #f6f8fa;
        }
        tr:nth-child(even) {
            background-color: #f6f8fa;
        }
        #table-of-contents {
            background-color: #f0f7f0;
            padding: 1.5rem;
            border-radius: 5px;
            margin: 2rem 0;
            border-left: 4px solid #137333;
        }
        #table-of-contents h3 {
            margin-top: 0;
            margin-bottom: 1rem;
            color: #137333;
        }
        #table-of-contents ol, #table-of-contents ul {
            margin-bottom: 0;
        }
        #table-of-contents a {
            text-decoration: none;
            color: #0366d6;
        }
        #table-of-contents a:hover {
            text-decoration: underline;
        }
        .post-content {
            margin-top: 2rem;
        }
        .paper-entry {
            margin-bottom: 2.5rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid #eaecef;
        }
        .paper-entry:last-child {
            border-bottom: none;
        }
        .paper-title {
            font-size: 1.3em;
            margin-bottom: 0.5em;
        }
        .paper-title a {
            color: #0366d6;
            text-decoration: none;
        }
        .paper-title a:hover {
            text-decoration: underline;
        }
        .paper-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 1em;
        }
        .paper-topics {
            display: inline-flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 0.5rem;
        }
        /* SY Profile topic tags - green tint */
        .topic-tag {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
        }
        footer {
            margin-top: 3rem;
            padding-top: 1rem;
            border-top: 1px solid #eaecef;
            font-size: 0.9em;
            color: #6a737d;
        }
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            h1 {
                font-size: 2em;
            }
            h2 {
                font-size: 1.6em;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1 class="site-title">Beren's Blog</h1>
            <nav>
                <ul>
                  <li><a href="../index.html">Home</a></li>
                  <li><a href="../blog.html">Blog</a></li>
                  <li><a href="../gallery.html">Gallery</a></li>
                  <li><a href="../about.html">About</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="container">
        <article class="blog-post">
            <h2 class="post-title">SY&#39;s Gaze LLM Daily Review</h2>
            <div class="post-meta">
                <span class="date">March 01, 2026</span>
                <span class="author">SY</span>
                <span class="label">Gaze LLM Research</span>
            </div>

            <div id="table-of-contents">
              <h3>Table of Contents</h3>
              <ol>
                <li><a href="#daily-concept">Daily Concept</a></li>
                <li><a href="#papers">Paper Summaries</a></li>
                <li><a href="#references">References</a></li>
              </ol>
            </div>

            <div class="post-content">
                <p>Here are selected research papers at the intersection of gaze/eye-tracking and LLMs for March 01, 2026.</p>

                <h2 id="daily-concept">Daily Concept: Gaze patterns in human-AI interaction</h2>
                <p><strong>1. What is it?</strong><br>
Gaze patterns in human-AI interaction refer to the spatial and temporal characteristics of eye movements exhibited by users while engaging with AI systems, such as Large Language Models. This concept involves analyzing fixations, saccades, and scanpaths to infer cognitive states like attention, confusion, or trust during tasks like prompt formulation or output verification. Researchers use these patterns to model how humans process and evaluate AI-generated content.</p>

<p><strong>2. How does it work?</strong><br>
The process begins by capturing raw eye-tracking data and mapping gaze coordinates onto specific interface elements or text tokens. These signals are transformed into feature vectors representing fixation duration \(F_d\) and saccade velocity, which are synchronized with the LLM's token stream. Machine learning models then learn a mapping function \(f: (X_{text}, X_{gaze}) \rightarrow Y\) to predict user intent or cognitive load based on where the user looks. This effectively aligns human visual attention with the model's computational attention weights.</p>

<p><strong>3. Why does it matter?</strong><br>
Understanding these patterns allows researchers to bridge the gap between explicit user inputs and implicit cognitive reactions, enabling AI systems to adapt to user attention in real time. It provides a novel signal for Reinforcement Learning from Human Feedback (RLHF), where gaze duration on specific tokens can indicate perceived accuracy or hallucination without explicit user rating. This intersection is critical for designing transparent and trustworthy AI interfaces that align model reasoning with human cognitive processes.</p>

<p><strong>4. Key insight</strong><br>
Gaze acts as an implicit feedback signal that reveals user attention and cognitive load before explicit interaction occurs. This allows LLMs to move beyond text-only understanding to a multi-modal perception of user intent.</p>

                <h2 id="papers">Paper Summaries</h2>

                <div class="paper-entry" id="paper-1">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.01452v1" target="_blank">Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Penghao Deng, Jidong J. Yang, Jiachen Bian<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.01452v1" target="_blank">2602.01452v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p>### 1. Research Question. The paper aims to solve the problem of accurately identifying what objects a driver is looking at to improve Advanced Driver-Assistance Systems (ADAS). The core research question asks which computer vision paradigm most effectively maps driver gaze points to semantic objects in dynamic road scenes.</p>

<p>### 2. Positioning. This work positions itself at the intersection of gaze analysis and computer vision for intelligent vehicles. It fills a gap in the literature by moving beyond single-approach studies to provide a cross-paradigm evaluation. It contrasts traditional detection methods with modern segmentation and emerging Vision-Language Models (VLMs) to guide future system design.</p>

<p>### 3. Key Contribution. The main contribution is the demonstration that large Vision-Language Models (specifically Qwen2.5-VL-32b) can match or exceed traditional object detectors like YOLOv13, achieving Macro F1-Scores over \(0.84\). The study highlights that these VLMs offer superior robustness for identifying small, safety-critical objects like traffic lights in adverse nighttime conditions. Additionally, it identifies a critical failure mode in segmentation-assisted approaches caused by a "part-versus-whole" semantic gap.</p>

<p>### 4. Methodology. The authors evaluated three distinct vision paradigms for semantic identification using front-view camera footage collocated with driver gaze data. They tested direct object detection using YOLOv13, segmentation-assisted classification combining SAM2 with EfficientNetV2, and query-based reasoning using Qwen2.5-VL models in 7b and 32b variants. Performance was assessed by comparing the models' ability to correctly label objects at specific gaze coordinates.</p>

<p>### 5. Limitations. A primary limitation acknowledged is the trade-off between the high performance of large VLMs and their computational cost, which challenges real-time deployment compared to efficient detectors like YOLO. The segmentation-assisted approach also demonstrated poor recall, limiting its viability for safety-critical tasks. Furthermore, the study focuses on semantic identification accuracy, leaving the latency constraints of deploying 32-billion parameter models in vehicles as an unresolved challenge.</p>

<p>### 6. Critical Evaluation. This paper offers a compelling argument for the integration of large VLMs into driver monitoring systems, moving the field beyond the limitations of traditional bounding box detectors. The finding that VLMs excel at identifying small objects in low light is particularly significant for road safety. However, the practical utility of these findings depends heavily on future optimizations to reduce the latency of large models for edge deployment in vehicles.</p>
                </div>

                <hr>

                <h3 id="references">References</h3>
                <ul>
                    <li><a href="https://arxiv.org/abs/2602.01452v1" target="_blank">Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles</a></li>
                </ul>

                <p><em>Feel free to reach out for discussion. Thanks for reading!</em></p>
            </div>
        </article>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2026 SY. All rights reserved. | Gaze/LLM Research Daily Review</p>
        </div>
    </footer>
</body>
</html>