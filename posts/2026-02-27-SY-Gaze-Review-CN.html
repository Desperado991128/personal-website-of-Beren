<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SY眼动LLM研究日报 - 2026-02-27</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['\\(','\\)']], displayMath: [['\\[','\\]']]}
        });
    </script>
    <style>
        body {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            font-size: 18px;
        }
        header {
            margin-bottom: 2rem;
            border-bottom: 1px solid #eaecef;
        }
        .site-title {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2em;
            margin-bottom: 0.5em;
        }
        nav ul {
            display: flex;
            list-style: none;
            padding: 0;
            margin: 1rem 0;
        }
        nav ul li {
            margin-right: 1.5rem;
        }
        nav ul li a {
            text-decoration: none;
            color: #0366d6;
            font-weight: 600;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        .container {
            width: 100%;
            max-width: 800px;
        }
        h1 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2.4em;
            margin-bottom: 0.5em;
            line-height: 1.2;
        }
        h2 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.8em;
            margin-top: 1.5em;
            margin-bottom: 0.75em;
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }
        h3 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.5em;
            margin-top: 1.2em;
            margin-bottom: 0.6em;
        }
        h4 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.2em;
            margin-top: 1em;
            margin-bottom: 0.5em;
        }
        p {
            margin: 1em 0;
        }
        code {
            font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
            background: #f6f8fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 0.9em;
        }
        pre {
            background: #f6f8fa;
            padding: 16px;
            border-radius: 6px;
            overflow: auto;
            font-size: 0.9em;
        }
        blockquote {
            margin: 1em 0;
            padding: 0 1em;
            color: #6a737d;
            border-left: 0.25em solid #dfe2e5;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1.5em auto;
        }
        .meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
        }
        .post-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
        }
        /* SY Profile specific styling - green tint */
        .label {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: 600;
        }
        .figure-caption {
            text-align: center;
            color: #666;
            font-size: 0.9em;
            margin-top: -1em;
            margin-bottom: 2em;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1.5em 0;
        }
        th, td {
            border: 1px solid #dfe2e5;
            padding: 8px 12px;
            text-align: left;
        }
        th {
            background-color: #f6f8fa;
        }
        tr:nth-child(even) {
            background-color: #f6f8fa;
        }
        #table-of-contents {
            background-color: #f0f7f0;
            padding: 1.5rem;
            border-radius: 5px;
            margin: 2rem 0;
            border-left: 4px solid #137333;
        }
        #table-of-contents h3 {
            margin-top: 0;
            margin-bottom: 1rem;
            color: #137333;
        }
        #table-of-contents ol, #table-of-contents ul {
            margin-bottom: 0;
        }
        #table-of-contents a {
            text-decoration: none;
            color: #0366d6;
        }
        #table-of-contents a:hover {
            text-decoration: underline;
        }
        .post-content {
            margin-top: 2rem;
        }
        .paper-entry {
            margin-bottom: 2.5rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid #eaecef;
        }
        .paper-entry:last-child {
            border-bottom: none;
        }
        .paper-title {
            font-size: 1.3em;
            margin-bottom: 0.5em;
        }
        .paper-title a {
            color: #0366d6;
            text-decoration: none;
        }
        .paper-title a:hover {
            text-decoration: underline;
        }
        .paper-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 1em;
        }
        .paper-topics {
            display: inline-flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 0.5rem;
        }
        /* SY Profile topic tags - green tint */
        .topic-tag {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
        }
        footer {
            margin-top: 3rem;
            padding-top: 1rem;
            border-top: 1px solid #eaecef;
            font-size: 0.9em;
            color: #6a737d;
        }
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            h1 {
                font-size: 2em;
            }
            h2 {
                font-size: 1.6em;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1 class="site-title">Beren's Blog</h1>
            <nav>
                <ul>
                  <li><a href="../index.html">Home</a></li>
                  <li><a href="../blog.html">Blog</a></li>
                  <li><a href="../gallery.html">Gallery</a></li>
                  <li><a href="../about.html">About</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="container">
        <article class="blog-post">
            <h2 class="post-title">SY眼动LLM研究日报</h2>
            <div class="post-meta">
                <span class="date">February 27, 2026</span>
                <span class="author">SY</span>
                <span class="label">眼动LLM研究</span>
            </div>

            <div id="table-of-contents">
              <h3>目录</h3>
              <ol>
                <li><a href="#daily-concept">每日概念</a></li>
                <li><a href="#papers">论文摘要</a></li>
                <li><a href="#references">参考链接</a></li>
              </ol>
            </div>

            <div class="post-content">
                <p>以下是 2026年02月27日 精选的眼动追踪与LLM交叉研究论文摘要。</p>

                <h2 id="daily-concept">每日概念: Cognitive load measurement via eye tracking</h2>
                <p><strong>1. 概念定义</strong><br>
通过眼动追踪测量认知负荷是指利用眼动特征来量化个体在处理特定信息时的心理资源消耗程度。这种方法不依赖主观问卷，而是通过捕捉瞳孔直径变化、注视时长和眼跳模式等生理信号来客观推断大脑的认知加工强度。</p>

<p><strong>2. 核心原理</strong><br>
核心机制建立在自主神经系统与眼动行为的生理联结之上，当认知任务难度增加时，大脑资源占用率上升会引发瞳孔扩张和注视时间延长等生理反应。在机器学习视角下，这通常被建模为一个回归或分类问题。算法将多维眼动特征向量 \(\mathbf{x}\) 映射到认知负荷水平 \(L\)，其数学表达通常为<br>
\[ L = f(\mathbf{x}) \quad \text{where} \quad \mathbf{x} = [x_{\text{pupil}}, x_{\text{fixation}}, x_{\text{blink}}, \dots] \]<br>
其中 \(x_{\text{pupil}}\) 常指任务诱发的瞳孔直径变化，是衡量负荷的关键指标。</p>

<p><strong>3. 研究意义</strong><br>
在AI与LLM研究中，该方法提供了一种客观评估文本生成质量的生理反馈机制。它可以作为强化学习中的奖励信号，帮助模型优化输出内容以降低用户的阅读难度和理解障碍。相比传统的模型困惑度，这种生理指标更能直接反映真实的人类阅读体验。</p>

<p><strong>4. 关键洞见</strong><br>
眼动追踪将不可见的认知过程显性化，从而填补了模型计算困惑度与人类实际理解难度之间的鸿沟。</p>

                <h2 id="papers">论文摘要</h2>

                <div class="paper-entry" id="paper-1">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.22774v1" target="_blank">Transformer Actor-Critic for Efficient Freshness-Aware Resource Allocation</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>作者:</strong> Maryam Ansarifard, Mohit K. Sharma, Kishor C. Joshi et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.22774v1" target="_blank">2602.22774v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>摘要</h4>
                    <p><strong>1. 研究问题</strong><br>
这篇论文旨在解决多用户上行链路无线网络中，如何在非正交多址接入(NOMA)约束下最小化信息年龄的问题。核心挑战在于系统需要处理用户在任务大小、AoI阈值和惩罚敏感性方面的异构性。</p>

<p><strong>2. 研究定位</strong><br>
这项工作将自然语言处理领域的Transformer架构引入无线通信资源分配领域，填补了传统深度强化学习方法在处理大规模用户状态和复杂用户间依赖关系方面的空白。它展示了LLM核心组件在非文本序列决策任务中的强大迁移能力。</p>

<p><strong>3. 核心贡献</strong><br>
论文提出了一种结合近端策略优化(PPO)与Transformer编码器的深度强化学习框架。主要创新在于利用注意力机制捕获用户间的依赖关系，使模型能够动态聚焦于关键用户状态，显著提升了策略的性能和可扩展性。</p>

<p><strong>4. 方法概述</strong><br>
该方法构建了一个基于Transformer编码器的Actor-Critic网络，利用自注意力机制处理变长的用户状态序列。训练过程中，模型通过注意力权重演变逐步学习用户优先级，从早期的均匀关注模式转变为后期符合NOMA约束的聚焦模式。</p>

<p><strong>5. 局限与不足</strong><br>
尽管Transformer架构提升了性能，但其自注意力机制较高的计算复杂度可能限制在超大规模用户场景下的实时推理速度。此外，研究主要基于仿真环境，缺乏在真实无线通信硬件平台上的验证，实际部署可能面临信道不确定性等挑战。</p>

<p><strong>6. 评价与思考</strong><br>
这项工作巧妙地将LLM中的注意力机制应用于通信网络优化，证明了注意力图在解释模型决策逻辑方面的价值。这种机器生成的注意力热力图与人类眼动追踪的热力图有异曲同工之妙，都揭示了智能体如何从全局扫描过渡到聚焦关键信息区域，为理解AI决策过程提供了直观视角。</p>
                </div>
                <div class="paper-entry" id="paper-2">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.22368v1" target="_blank">EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>作者:</strong> Jiahao Zhang, Yifan Zhang, Kevin Leach et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.22368v1" target="_blank">2602.22368v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>摘要</h4>
                    <p><strong>1. 研究问题</strong><br>
这篇论文旨在解决如何将人类在代码理解过程中的专业知识整合到大语言模型(LLM)中以提升代码摘要生成质量的问题。核心探究人类眼动注意力模式能否作为有效的先验信号，弥补LLM在语义聚焦上的不足。</p>

<p><strong>2. 研究定位</strong><br>
现有代码摘要研究多集中于模型架构优化或大规模预训练，往往忽略了人类认知过程中的注意力机制。该工作填补了认知科学与深度学习之间的空白，探索了利用人类生理信号(眼动追踪)增强LLM表征能力的可行性。</p>

<p><strong>3. 核心贡献</strong><br>
论文提出了EyeLayer，一种轻量级的注意力增强模块，能够将人类眼动模式无缝集成到LLM中。主要创新在于利用多模态高斯混合模型对人类注意力进行建模，并证明了这种人类注意力信号在不同模型架构(如LLaMA, Qwen, CodeBERT)间具有显著的互补性和迁移能力，使BLEU-4指标最高提升13.17%。</p>

<p><strong>4. 方法概述</strong><br>
EyeLayer通过多模态高斯混合模型模拟人类阅读代码时的注意力分布，捕捉开发者关注的焦点位置和强度。该方法利用学习到的参数 \((\mu_i, \sigma_i^2)\) 对token嵌入进行重新分配，从而在不破坏LLM原有表征的前提下，将人类注意力先验注入到模型的推理过程中。</p>

<p><strong>5. 局限与不足</strong><br>
该方法的有效性高度依赖于眼动追踪数据的质量和可用性，而这类生理数据的采集成本较高且通常具有稀疏性。虽然模块设计为轻量级，但在更广泛的代码相关任务(如代码生成或错误检测)中的泛化能力尚未得到验证。此外，不同开发者阅读习惯的差异可能引入噪声，影响模型的稳定性。</p>

<p><strong>6. 评价与思考</strong><br>
这项工作巧妙地搭建了人类认知信号与深度学习模型之间的桥梁，为利用生物信号增强AI提供了极具启发性的思路。其优点在于方法具有模型无关性，能够以较小的代价显著提升性能。未来的研究可以进一步探索如何减少对标注眼动数据的依赖，或者利用模型生成的注意力去模拟人类注意力，从而解决数据稀缺的瓶颈。</p>
                </div>
                <div class="paper-entry" id="paper-3">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.21983v1" target="_blank">Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>作者:</strong> Jingchao Wei, Jingkai Qin, Yuxiao Cao et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.21983v1" target="_blank">2602.21983v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>摘要</h4>
                    <p><strong>1. 研究问题</strong><br>
这篇论文旨在解决人形机器人在非受限人机交互中难以产生自然且符合语境的注视转移问题。核心挑战在于如何将认知注意力机制与仿生运动生成有效耦合。</p>

<p><strong>2. 研究定位</strong><br>
现有文献往往将注视目标推理与运动生成割裂研究，缺乏统一的处理框架。这项工作填补了该空白，将认知层面的注意力定向与物理层面的拟人运动生成相结合，致力于提升人机交互的自然度。</p>

<p><strong>3. 核心贡献</strong><br>
论文提出了RGS框架，创新性地结合了视觉语言模型(VLM)进行符合人类认知规律的注视目标推理。同时引入条件VQ-VAE模型生成眼头协调的运动轨迹，实现了多样化且逼真的注视行为。</p>

<p><strong>4. 方法概述</strong><br>
RGS框架包含两个主要模块。首先利用基于VLM的注视推理管道，从多模态交互线索中推断符合语境的注视目标。随后通过条件VQ-VAE模型生成眼头协调的注视转移运动，确保动作的多样性与拟人性。</p>

<p><strong>5. 局限与不足</strong><br>
论文摘要未明确提及局限性，但基于方法推断，VLM推理可能带来较高的计算延迟，影响实时交互体验。此外运动生成的质量高度依赖于训练数据中眼动追踪数据的精度和多样性。</p>

<p><strong>6. 评价与思考</strong><br>
这项工作巧妙地将大模型技术引入机器人注视控制，为社交机器人领域提供了新的思路。其优点在于提升了注视行为的认知合理性和运动自然度，但实际部署中的实时性与计算成本仍需进一步考量。</p>
                </div>

                <hr>

                <h3 id="references">参考链接</h3>
                <ul>
                    <li><a href="https://arxiv.org/abs/2602.22774v1" target="_blank">Transformer Actor-Critic for Efficient Freshness-Aware Resource Allocation</a></li>
                    <li><a href="https://arxiv.org/abs/2602.22368v1" target="_blank">EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization</a></li>
                    <li><a href="https://arxiv.org/abs/2602.21983v1" target="_blank">Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots</a></li>
                </ul>

                <p><em>欢迎讨论和反馈。感谢阅读!</em></p>
            </div>
        </article>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2026 SY. All rights reserved. | Gaze/LLM Research Daily Review</p>
        </div>
    </footer>
</body>
</html>