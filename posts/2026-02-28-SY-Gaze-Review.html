<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SY&#39;s Gaze LLM Daily Review - 2026-02-28</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['\\(','\\)']], displayMath: [['\\[','\\]']]}
        });
    </script>
    <style>
        body {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            font-size: 18px;
        }
        header {
            margin-bottom: 2rem;
            border-bottom: 1px solid #eaecef;
        }
        .site-title {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2em;
            margin-bottom: 0.5em;
        }
        nav ul {
            display: flex;
            list-style: none;
            padding: 0;
            margin: 1rem 0;
        }
        nav ul li {
            margin-right: 1.5rem;
        }
        nav ul li a {
            text-decoration: none;
            color: #0366d6;
            font-weight: 600;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        .container {
            width: 100%;
            max-width: 800px;
        }
        h1 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2.4em;
            margin-bottom: 0.5em;
            line-height: 1.2;
        }
        h2 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.8em;
            margin-top: 1.5em;
            margin-bottom: 0.75em;
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }
        h3 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.5em;
            margin-top: 1.2em;
            margin-bottom: 0.6em;
        }
        h4 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.2em;
            margin-top: 1em;
            margin-bottom: 0.5em;
        }
        p {
            margin: 1em 0;
        }
        code {
            font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
            background: #f6f8fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 0.9em;
        }
        pre {
            background: #f6f8fa;
            padding: 16px;
            border-radius: 6px;
            overflow: auto;
            font-size: 0.9em;
        }
        blockquote {
            margin: 1em 0;
            padding: 0 1em;
            color: #6a737d;
            border-left: 0.25em solid #dfe2e5;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1.5em auto;
        }
        .meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
        }
        .post-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
        }
        /* SY Profile specific styling - green tint */
        .label {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: 600;
        }
        .figure-caption {
            text-align: center;
            color: #666;
            font-size: 0.9em;
            margin-top: -1em;
            margin-bottom: 2em;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1.5em 0;
        }
        th, td {
            border: 1px solid #dfe2e5;
            padding: 8px 12px;
            text-align: left;
        }
        th {
            background-color: #f6f8fa;
        }
        tr:nth-child(even) {
            background-color: #f6f8fa;
        }
        #table-of-contents {
            background-color: #f0f7f0;
            padding: 1.5rem;
            border-radius: 5px;
            margin: 2rem 0;
            border-left: 4px solid #137333;
        }
        #table-of-contents h3 {
            margin-top: 0;
            margin-bottom: 1rem;
            color: #137333;
        }
        #table-of-contents ol, #table-of-contents ul {
            margin-bottom: 0;
        }
        #table-of-contents a {
            text-decoration: none;
            color: #0366d6;
        }
        #table-of-contents a:hover {
            text-decoration: underline;
        }
        .post-content {
            margin-top: 2rem;
        }
        .paper-entry {
            margin-bottom: 2.5rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid #eaecef;
        }
        .paper-entry:last-child {
            border-bottom: none;
        }
        .paper-title {
            font-size: 1.3em;
            margin-bottom: 0.5em;
        }
        .paper-title a {
            color: #0366d6;
            text-decoration: none;
        }
        .paper-title a:hover {
            text-decoration: underline;
        }
        .paper-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 1em;
        }
        .paper-topics {
            display: inline-flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 0.5rem;
        }
        /* SY Profile topic tags - green tint */
        .topic-tag {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
        }
        footer {
            margin-top: 3rem;
            padding-top: 1rem;
            border-top: 1px solid #eaecef;
            font-size: 0.9em;
            color: #6a737d;
        }
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            h1 {
                font-size: 2em;
            }
            h2 {
                font-size: 1.6em;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1 class="site-title">Beren's Blog</h1>
            <nav>
                <ul>
                  <li><a href="../index.html">Home</a></li>
                  <li><a href="../blog.html">Blog</a></li>
                  <li><a href="../gallery.html">Gallery</a></li>
                  <li><a href="../about.html">About</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="container">
        <article class="blog-post">
            <h2 class="post-title">SY&#39;s Gaze LLM Daily Review</h2>
            <div class="post-meta">
                <span class="date">February 28, 2026</span>
                <span class="author">SY</span>
                <span class="label">Gaze LLM Research</span>
            </div>

            <div id="table-of-contents">
              <h3>Table of Contents</h3>
              <ol>
                <li><a href="#daily-concept">Daily Concept</a></li>
                <li><a href="#papers">Paper Summaries</a></li>
                <li><a href="#references">References</a></li>
              </ol>
            </div>

            <div class="post-content">
                <p>Here are selected research papers at the intersection of gaze/eye-tracking and LLMs for February 28, 2026.</p>

                <h2 id="daily-concept">Daily Concept: Eye tracking in multimodal AI</h2>
                <p><strong>1. What is it?</strong><br>
Eye tracking in multimodal AI involves integrating gaze data as a supplementary input modality alongside text and images to enhance model performance. It treats human visual attention as a supervisory signal that guides the model to focus on relevant regions within a scene or text. This integration bridges the gap between human cognitive processes and machine perception.</p>

<p><strong>2. How does it work?</strong><br>
The process typically encodes raw gaze coordinates into spatial features, such as heatmaps or fixation sequences, which are then fused with visual or textual embeddings using transformer architectures. Models often employ a gaze-guided attention mechanism where the model's attention weights are regularized to align with human fixation distributions. This alignment is frequently achieved by minimizing a divergence loss between the model's learned attention map \( A_{model} \) and the ground truth gaze map \( A_{gaze} \). For example, the optimization objective may include a term like \( \mathcal{L}_{gaze} = D_{KL}(A_{gaze} \parallel A_{model}) \) to force the model to attend to specific regions.</p>

<p><strong>3. Why does it matter?</strong><br>
Integrating gaze data provides a strong inductive bias that helps models distinguish signal from noise, leading to faster convergence and improved accuracy in tasks like Visual Question Answering. It enhances interpretability by forcing the model to align its internal attention mechanisms with human reasoning patterns. Furthermore, this approach enables the development of more responsive AI systems that can adapt to user intent in real time based on where they are looking.</p>

<p><strong>4. Key insight</strong><br>
Human gaze serves as a low-cost, implicit supervision signal that teaches AI systems not just what to process, but what to ignore. This biological prior allows large language models to mimic human-like efficiency in visual and textual processing tasks.</p>

                <h2 id="papers">Paper Summaries</h2>

                <div class="paper-entry" id="paper-1">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.16138v1" target="_blank">IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Parsa Madinei, Srijita Karmakar, Russell Cohen Hoffing et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.16138v1" target="_blank">2602.16138v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p><strong>1. Research Question</strong><br>
The paper addresses the challenge of ambiguity in open-ended Visual Question Answering (VQA). It investigates whether real-time eye-tracking data can resolve user intent and significantly improve the accuracy of Large Vision-Language Models (VLMs) on ambiguous queries without requiring model retraining.</p>

<p><strong>2. Positioning</strong><br>
This work positions itself at the intersection of cognitive science and multimodal AI, addressing the limitations of current VLMs that struggle with underspecified textual prompts. It fills a gap in the literature by moving beyond static image-text pairs, proposing a novel interactive protocol where implicit behavioral signals (gaze) augment the model's understanding during inference.</p>

<p><strong>3. Key Contribution</strong><br>
The primary contribution is IRIS, a training-free inference method that leverages human fixations to disambiguate visual questions. The authors demonstrate that fixations occurring closest to the start of a verbal question are the most informative, more than doubling accuracy on ambiguous questions (from 35.2% to 77.2%). They also release a new benchmark dataset and evaluation suite to standardize gaze-augmented VQA research.</p>

<p><strong>4. Methodology</strong><br>
IRIS operates by capturing user eye-tracking data in real-time as they view an image and ask a question. The method isolates specific gaze fixations synchronized with the onset of the verbal query to identify the user's region of interest. The VLM then uses this gaze-augmented input to focus its attention on relevant image regions, effectively filtering out irrelevant visual noise to answer the question.</p>

<p><strong>5. Limitations</strong><br>
A primary limitation is the dependency on specialized eye-tracking hardware, which restricts the method's accessibility compared to standard webcam-based interactions. The approach also relies on precise timing synchronization between the gaze data and speech onset, which may be difficult to achieve in noisy or uncontrolled real-world environments. Additionally, the user study size of 500 image-question pairs may not capture the full diversity of ambiguous scenarios encountered in broader applications.</p>

<p><strong>6. Critical Evaluation</strong><br>
This research offers a compelling proof-of-concept that implicit human signals can serve as a powerful interface for refining LLM outputs. The reported accuracy gains are substantial, and the training-free approach makes the method immediately applicable to state-of-the-art models. However, the practical deployment is currently hindered by hardware requirements, suggesting that future work must focus on making the gaze-input mechanism more accessible or software-based.</p>
                </div>
                <div class="paper-entry" id="paper-2">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.05132v1" target="_blank">ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Jia Li, Wenjie Zhao, Shijian Deng et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.05132v1" target="_blank">2602.05132v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p><strong>1. Research Question</strong><br>
The paper addresses the challenge of online egocentric gaze estimation, aiming to predict where a camera wearer is looking using only current and past video frames. The core question is how to effectively model gaze in real-time scenarios where explicit eye or head signals are unavailable and future context is inaccessible.</p>

<p><strong>2. Positioning</strong><br>
This work distinguishes itself from third-person gaze estimation methods that rely on clear images of the eyes or head pose. It targets the "online" setting required for augmented reality applications, filling a gap in utilizing temporal dependencies for gaze prediction without access to future frames.</p>

<p><strong>3. Key Contribution</strong><br>
The main contribution is ARGaze, a framework that reformulates gaze estimation as an autoregressive sequence prediction task similar to decoding in large language models. The authors introduce a Gaze Context Window mechanism that conditions current predictions on a fixed history of recent gaze estimates, demonstrating that this temporal prior significantly boosts performance.</p>

<p><strong>4. Methodology</strong><br>
The proposed method uses a transformer decoder that functions autoregressively, taking two main inputs at each timestep. These inputs are the current visual features extracted from the video frame and a fixed-length buffer of past gaze predictions. This architecture enforces causality, ensuring the model relies strictly on available history to predict the current gaze target.</p>

<p><strong>5. Limitations</strong><br>
A potential limitation inherent in the autoregressive design is the risk of error propagation, where incorrect predictions from previous timesteps could negatively influence future estimates. Additionally, the reliance on a fixed-length context window may restrict the model's ability to capture longer-term behavioral patterns that extend beyond the defined history limit.</p>

<p><strong>6. Critical Evaluation</strong><br>
This work provides a strong conceptual bridge between natural language processing techniques and computer vision by successfully applying autoregressive decoding to gaze estimation. The approach is highly practical for real-world deployment due to its bounded resource usage and state-of-the-art performance, offering a robust solution for streaming applications in augmented reality.</p>
                </div>
                <div class="paper-entry" id="paper-3">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.17848v1" target="_blank">On the scaling relationship between cloze probabilities and language model next-token prediction</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Cassandra L. Jacobs, Morgan Grobol<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.17848v1" target="_blank">2602.17848v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p><strong>1. Research Question</strong><br>
The paper investigates how the scale of Large Language Models (LLMs) influences their ability to predict human reading behavior. It specifically asks why larger models align better with human cloze data and what mechanistic changes occur as model size increases.</p>

<p><strong>2. Positioning</strong><br>
This research builds upon previous studies linking LLM performance to eye-tracking and reading time data. It fills a gap in the literature by explaining the specific mechanisms that drive improved prediction in larger models, distinguishing between semantic alignment and low-level statistical sensitivity.</p>

<p><strong>3. Key Contribution</strong><br>
The authors demonstrate that larger models generate superior next-token estimates in cloze tasks because they prioritize semantic appropriateness over raw lexical co-occurrence statistics. They argue that the increased memorization capacity of these models helps identify contextually correct words, though this simultaneously reduces sensitivity to low-level cues used in human word recognition.</p>

<p><strong>4. Methodology</strong><br>
The study compares human cloze responses with next-token predictions from language models of varying sizes. It evaluates how model scale affects the balance between semantic alignment with human responses and reliance on lexical co-occurrence statistics.</p>

<p><strong>5. Limitations</strong><br>
A primary limitation acknowledged is that even the largest models still under-allocate probability mass to human responses compared to actual human data. Furthermore, the reduced sensitivity to low-level lexical information in larger models may limit their ability to fully simulate the nuances of human word recognition processes.</p>

<p><strong>6. Critical Evaluation</strong><br>
This paper provides valuable insight into the trade-offs inherent in scaling language models for psycholinguistic modeling. It successfully identifies that better semantic guessing does not necessarily equate to a faithful simulation of human cognitive processing during reading, highlighting a critical divergence between model size and cognitive plausibility.</p>
                </div>

                <hr>

                <h3 id="references">References</h3>
                <ul>
                    <li><a href="https://arxiv.org/abs/2602.16138v1" target="_blank">IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models</a></li>
                    <li><a href="https://arxiv.org/abs/2602.05132v1" target="_blank">ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation</a></li>
                    <li><a href="https://arxiv.org/abs/2602.17848v1" target="_blank">On the scaling relationship between cloze probabilities and language model next-token prediction</a></li>
                </ul>

                <p><em>Feel free to reach out for discussion. Thanks for reading!</em></p>
            </div>
        </article>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2026 SY. All rights reserved. | Gaze/LLM Research Daily Review</p>
        </div>
    </footer>
</body>
</html>