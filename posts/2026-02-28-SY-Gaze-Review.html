<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SY&#39;s Gaze LLM Daily Review - 2026-02-28</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['\\(','\\)']], displayMath: [['\\[','\\]']]}
        });
    </script>
    <style>
        body {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            font-size: 18px;
        }
        header {
            margin-bottom: 2rem;
            border-bottom: 1px solid #eaecef;
        }
        .site-title {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2em;
            margin-bottom: 0.5em;
        }
        nav ul {
            display: flex;
            list-style: none;
            padding: 0;
            margin: 1rem 0;
        }
        nav ul li {
            margin-right: 1.5rem;
        }
        nav ul li a {
            text-decoration: none;
            color: #0366d6;
            font-weight: 600;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        .container {
            width: 100%;
            max-width: 800px;
        }
        h1 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2.4em;
            margin-bottom: 0.5em;
            line-height: 1.2;
        }
        h2 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.8em;
            margin-top: 1.5em;
            margin-bottom: 0.75em;
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }
        h3 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.5em;
            margin-top: 1.2em;
            margin-bottom: 0.6em;
        }
        h4 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.2em;
            margin-top: 1em;
            margin-bottom: 0.5em;
        }
        p {
            margin: 1em 0;
        }
        code {
            font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
            background: #f6f8fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 0.9em;
        }
        pre {
            background: #f6f8fa;
            padding: 16px;
            border-radius: 6px;
            overflow: auto;
            font-size: 0.9em;
        }
        blockquote {
            margin: 1em 0;
            padding: 0 1em;
            color: #6a737d;
            border-left: 0.25em solid #dfe2e5;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1.5em auto;
        }
        .meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
        }
        .post-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
        }
        /* SY Profile specific styling - green tint */
        .label {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: 600;
        }
        .figure-caption {
            text-align: center;
            color: #666;
            font-size: 0.9em;
            margin-top: -1em;
            margin-bottom: 2em;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1.5em 0;
        }
        th, td {
            border: 1px solid #dfe2e5;
            padding: 8px 12px;
            text-align: left;
        }
        th {
            background-color: #f6f8fa;
        }
        tr:nth-child(even) {
            background-color: #f6f8fa;
        }
        #table-of-contents {
            background-color: #f0f7f0;
            padding: 1.5rem;
            border-radius: 5px;
            margin: 2rem 0;
            border-left: 4px solid #137333;
        }
        #table-of-contents h3 {
            margin-top: 0;
            margin-bottom: 1rem;
            color: #137333;
        }
        #table-of-contents ol, #table-of-contents ul {
            margin-bottom: 0;
        }
        #table-of-contents a {
            text-decoration: none;
            color: #0366d6;
        }
        #table-of-contents a:hover {
            text-decoration: underline;
        }
        .post-content {
            margin-top: 2rem;
        }
        .paper-entry {
            margin-bottom: 2.5rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid #eaecef;
        }
        .paper-entry:last-child {
            border-bottom: none;
        }
        .paper-title {
            font-size: 1.3em;
            margin-bottom: 0.5em;
        }
        .paper-title a {
            color: #0366d6;
            text-decoration: none;
        }
        .paper-title a:hover {
            text-decoration: underline;
        }
        .paper-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 1em;
        }
        .paper-topics {
            display: inline-flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 0.5rem;
        }
        /* SY Profile topic tags - green tint */
        .topic-tag {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
        }
        footer {
            margin-top: 3rem;
            padding-top: 1rem;
            border-top: 1px solid #eaecef;
            font-size: 0.9em;
            color: #6a737d;
        }
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            h1 {
                font-size: 2em;
            }
            h2 {
                font-size: 1.6em;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1 class="site-title">Beren's Blog</h1>
            <nav>
                <ul>
                  <li><a href="../index.html">Home</a></li>
                  <li><a href="../blog.html">Blog</a></li>
                  <li><a href="../gallery.html">Gallery</a></li>
                  <li><a href="../about.html">About</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="container">
        <article class="blog-post">
            <h2 class="post-title">SY&#39;s Gaze LLM Daily Review</h2>
            <div class="post-meta">
                <span class="date">February 28, 2026</span>
                <span class="author">SY</span>
                <span class="label">Gaze LLM Research</span>
            </div>

            <div id="table-of-contents">
              <h3>Table of Contents</h3>
              <ol>
                <li><a href="#daily-concept">Daily Concept</a></li>
                <li><a href="#papers">Paper Summaries</a></li>
                <li><a href="#references">References</a></li>
              </ol>
            </div>

            <div class="post-content">
                <p>Here are selected research papers at the intersection of gaze/eye-tracking and LLMs for February 28, 2026.</p>

                <h2 id="daily-concept">Daily Concept: Eye tracking in multimodal AI</h2>
                <p><strong>1. What is it?</strong><br>
Eye tracking in multimodal AI refers to the integration of gaze data as a distinct input signal within models that process text and images. By capturing where a user looks, these systems leverage human visual attention to guide model processing or to align visual and linguistic representations. This approach treats gaze not just as a biological metric but as an informative prior for artificial systems.</p>

<p><strong>2. How does it work?</strong><br>
The process typically involves encoding gaze coordinates into feature vectors that align with the embedding space of the multimodal model. A gaze encoder transforms raw fixation points into attention maps or token embeddings, which are then fused with visual features extracted by a vision encoder. Mathematically, the model optimizes a conditional generation objective where the probability of an output token depends on text, image, and gaze inputs, expressed as \( P(y_t | y_{<t}, I, G) \). This allows the model to prioritize specific image regions indicated by high gaze density during the inference process.</p>

<p><strong>3. Why does it matter?</strong><br>
Integrating gaze data provides a strong inductive bias that helps models resolve ambiguity in visual scenes by focusing on relevant regions, similar to human cognitive processing. This significantly improves performance on tasks requiring fine-grained visual reasoning, such as visual question answering or medical image analysis. Furthermore, it opens avenues for developing collaborative AI agents that understand user intent through implicit behavioral signals.</p>

<p><strong>4. Key insight</strong><br>
Gaze data acts as a natural attention mechanism that bridges the gap between high-level language semantics and low-level visual features. It effectively guides the model to prioritize relevant visual information, reducing the computational search space for complex reasoning tasks.</p>

                <h2 id="papers">Paper Summaries</h2>

                <div class="paper-entry" id="paper-1">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.20408v1" target="_blank">Examining and Addressing Barriers to Diversity in LLM-Generated Ideas</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Yuting Deng, Melanie Brucks, Olivier Toubia<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.20408v1" target="_blank">2602.20408v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p>1. <strong>Research Question</strong><br>
 The paper investigates why ideas generated by Large Language Models (LLMs) lack diversity compared to those generated by independent groups of humans. It asks how cognitive mechanisms like fixation and knowledge partitioning contribute to this homogenization and whether prompting strategies can mitigate these issues.</p>

<p>2. <strong>Positioning</strong><br>
 This work situates itself at the intersection of cognitive psychology and generative AI, applying human cognitive theory to explain LLM behavioral patterns. It addresses a critical gap in the literature regarding the societal risks of AI reliance, specifically moving beyond output quality to examine the structural diversity of the idea space.</p>

<p>3. <strong>Key Contribution</strong><br>
 The primary contribution is the identification of two specific barriers to diversity, individual fixation and collective knowledge aggregation, alongside a practical framework to resolve them. The authors demonstrate that combining Chain-of-Thought prompting with ordinary personas allows LLMs to generate ideas with greater diversity than human groups.</p>

<p>4. <strong>Methodology</strong><br>
 The researchers conducted four studies involving LLMs and human participants to test targeted interventions for idea generation. They employed Chain-of-Thought prompting to reduce individual fixation and assigned distinct ordinary personas to simulate the knowledge partitioning observed in human populations.</p>

<p>5. <strong>Limitations</strong><br>
 While the interventions proved effective, the study acknowledges that Chain-of-Thought prompting improves diversity only in LLMs and does not translate to human cognition. Additionally, the reliance on specific prompting strategies suggests that diversity gains may be sensitive to prompt engineering and might not generalize across all model architectures or task types.</p>

<p>6. <strong>Critical Evaluation</strong><br>
 This paper offers a compelling theoretical framework by successfully mapping concepts like cognitive fixation onto LLM behaviors, providing a deeper understanding of model limitations. However, the reliance on prompt engineering to solve diversity issues implies a need for continuous manual intervention, which may not scale effortlessly in fully automated systems.</p>
                </div>
                <div class="paper-entry" id="paper-2">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.23335v1" target="_blank">Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Dany Haddad, Dan Bareket, Joseph Chee Chang et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.23335v1" target="_blank">2602.23335v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p><strong>1. Research Question</strong><br>
The paper addresses the lack of insight into how researchers utilize AI-powered scientific tools in real-world settings. It aims to characterize user interaction patterns, engagement behaviors, and how these behaviors evolve as users gain experience with LLM-based retrieval systems.</p>

<p><strong>2. Positioning</strong><br>
This work positions itself at the intersection of scientific information retrieval and human-computer interaction. It fills a critical gap by moving beyond controlled lab studies or traditional search log analysis to provide a large-scale, ecologically valid view of user behavior in deployed LLM-powered research platforms.</p>

<p><strong>3. Key Contribution</strong><br>
The primary contribution is the release of the Asta Interaction Dataset, a large-scale resource containing over 200,000 anonymized queries and interaction logs. The paper also provides a comprehensive analysis of user behavior and a novel query intent taxonomy to guide the future design of AI research assistants.</p>

<p><strong>4. Methodology</strong><br>
The authors collected and analyzed interaction logs from two deployed tools, a literature discovery interface and a scientific question-answering interface, within an LLM-powered retrieval-augmented generation platform. They performed quantitative and qualitative analyses to identify query patterns, navigation habits, and the evolution of user strategies over time.</p>

<p><strong>5. Limitations</strong><br>
The study is limited by its focus on a specific set of tools and a particular user demographic, which may restrict the generalizability of the findings to other domains or casual users. Additionally, the analysis relies solely on interaction logs, which lack explicit feedback mechanisms and cannot capture nuanced cognitive processes or visual attention that eye-tracking methods might reveal.</p>

<p><strong>6. Critical Evaluation</strong><br>
This work provides a valuable resource for the community by offering a realistic benchmark for evaluating scientific AI assistants. While the dataset is robust, the exclusive reliance on log-based data misses the deeper cognitive context of user engagement. Future work could benefit significantly from integrating gaze-tracking to understand how users visually validate generated evidence and citations.</p>
                </div>
                <div class="paper-entry" id="paper-3">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.23193v1" target="_blank">ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Elzo Brito dos Santos Filho<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.23193v1" target="_blank">2602.23193v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p>Here is a structured summary of the research paper.</p>

<p><strong>1. Research Question</strong><br>
This paper addresses the structural instability of Large Language Model (LLM) autonomous agents in software engineering. It investigates how to bridge the gap between probabilistic generation and deterministic execution while preventing context degradation over long horizons.</p>

<p><strong>2. Positioning</strong><br>
The work positions itself at the intersection of software engineering architecture and autonomous AI systems. It addresses the limitations of current reactive agent models by introducing patterns from distributed systems, specifically aiming to solve the lack of native state management and vulnerability to context loss in existing literature.</p>

<p><strong>3. Key Contribution</strong><br>
The primary contribution is the ESAA architecture, which adapts the Event Sourcing pattern for AI agents. This approach separates the agent's cognitive intention from the actual state mutation of the project, introducing an immutable event log that ensures forensic traceability and verifiable execution.</p>

<p><strong>4. Methodology</strong><br>
The proposed architecture requires agents to emit structured JSON intentions rather than direct code modifications. A deterministic orchestrator validates these intentions, persists them in an append-only log (<code>activity. jsonl</code>), applies file-writing effects, and generates a verifiable materialized view (<code>roadmap. json</code>). The system enforces boundary contracts and uses hashing to ensure the immutability of completed tasks.</p>

<p><strong>5. Limitations</strong><br>
The reliance on strict JSON output places a significant burden on the LLM's ability to generate syntactically correct code, which can be a point of failure for less capable models. Additionally, the complexity of maintaining an orchestrator and boundary contracts may introduce overhead that limits the architecture's applicability to smaller, simpler software tasks.</p>

<p><strong>6. Critical Evaluation</strong><br>
This work offers a rigorous engineering solution to the "black box" problem of autonomous agents, providing a clear mechanism for accountability through event logs. However, the specific model names cited in the case studies (e. g., GPT-5, Claude Sonnet 4.6) appear futuristic or hypothetical, which raises questions about the current empirical validity of the results.</p>
                </div>

                <hr>

                <h3 id="references">References</h3>
                <ul>
                    <li><a href="https://arxiv.org/abs/2602.20408v1" target="_blank">Examining and Addressing Barriers to Diversity in LLM-Generated Ideas</a></li>
                    <li><a href="https://arxiv.org/abs/2602.23335v1" target="_blank">Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset</a></li>
                    <li><a href="https://arxiv.org/abs/2602.23193v1" target="_blank">ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering</a></li>
                </ul>

                <p><em>Feel free to reach out for discussion. Thanks for reading!</em></p>
            </div>
        </article>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2026 SY. All rights reserved. | Gaze/LLM Research Daily Review</p>
        </div>
    </footer>
</body>
</html>