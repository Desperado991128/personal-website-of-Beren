<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SY&#39;s Gaze LLM Daily Review - 2026-02-28</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['\\(','\\)']], displayMath: [['\\[','\\]']]}
        });
    </script>
    <style>
        body {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            font-size: 18px;
        }
        header {
            margin-bottom: 2rem;
            border-bottom: 1px solid #eaecef;
        }
        .site-title {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2em;
            margin-bottom: 0.5em;
        }
        nav ul {
            display: flex;
            list-style: none;
            padding: 0;
            margin: 1rem 0;
        }
        nav ul li {
            margin-right: 1.5rem;
        }
        nav ul li a {
            text-decoration: none;
            color: #0366d6;
            font-weight: 600;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        .container {
            width: 100%;
            max-width: 800px;
        }
        h1 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2.4em;
            margin-bottom: 0.5em;
            line-height: 1.2;
        }
        h2 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.8em;
            margin-top: 1.5em;
            margin-bottom: 0.75em;
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }
        h3 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.5em;
            margin-top: 1.2em;
            margin-bottom: 0.6em;
        }
        h4 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.2em;
            margin-top: 1em;
            margin-bottom: 0.5em;
        }
        p {
            margin: 1em 0;
        }
        code {
            font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
            background: #f6f8fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 0.9em;
        }
        pre {
            background: #f6f8fa;
            padding: 16px;
            border-radius: 6px;
            overflow: auto;
            font-size: 0.9em;
        }
        blockquote {
            margin: 1em 0;
            padding: 0 1em;
            color: #6a737d;
            border-left: 0.25em solid #dfe2e5;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1.5em auto;
        }
        .meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
        }
        .post-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
        }
        /* SY Profile specific styling - green tint */
        .label {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: 600;
        }
        .figure-caption {
            text-align: center;
            color: #666;
            font-size: 0.9em;
            margin-top: -1em;
            margin-bottom: 2em;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1.5em 0;
        }
        th, td {
            border: 1px solid #dfe2e5;
            padding: 8px 12px;
            text-align: left;
        }
        th {
            background-color: #f6f8fa;
        }
        tr:nth-child(even) {
            background-color: #f6f8fa;
        }
        #table-of-contents {
            background-color: #f0f7f0;
            padding: 1.5rem;
            border-radius: 5px;
            margin: 2rem 0;
            border-left: 4px solid #137333;
        }
        #table-of-contents h3 {
            margin-top: 0;
            margin-bottom: 1rem;
            color: #137333;
        }
        #table-of-contents ol, #table-of-contents ul {
            margin-bottom: 0;
        }
        #table-of-contents a {
            text-decoration: none;
            color: #0366d6;
        }
        #table-of-contents a:hover {
            text-decoration: underline;
        }
        .post-content {
            margin-top: 2rem;
        }
        .paper-entry {
            margin-bottom: 2.5rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid #eaecef;
        }
        .paper-entry:last-child {
            border-bottom: none;
        }
        .paper-title {
            font-size: 1.3em;
            margin-bottom: 0.5em;
        }
        .paper-title a {
            color: #0366d6;
            text-decoration: none;
        }
        .paper-title a:hover {
            text-decoration: underline;
        }
        .paper-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 1em;
        }
        .paper-topics {
            display: inline-flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 0.5rem;
        }
        /* SY Profile topic tags - green tint */
        .topic-tag {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
        }
        footer {
            margin-top: 3rem;
            padding-top: 1rem;
            border-top: 1px solid #eaecef;
            font-size: 0.9em;
            color: #6a737d;
        }
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            h1 {
                font-size: 2em;
            }
            h2 {
                font-size: 1.6em;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1 class="site-title">Beren's Blog</h1>
            <nav>
                <ul>
                  <li><a href="../index.html">Home</a></li>
                  <li><a href="../blog.html">Blog</a></li>
                  <li><a href="../gallery.html">Gallery</a></li>
                  <li><a href="../about.html">About</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="container">
        <article class="blog-post">
            <h2 class="post-title">SY&#39;s Gaze LLM Daily Review</h2>
            <div class="post-meta">
                <span class="date">February 28, 2026</span>
                <span class="author">SY</span>
                <span class="label">Gaze LLM Research</span>
            </div>

            <div id="table-of-contents">
              <h3>Table of Contents</h3>
              <ol>
                <li><a href="#daily-concept">Daily Concept</a></li>
                <li><a href="#papers">Paper Summaries</a></li>
                <li><a href="#references">References</a></li>
              </ol>
            </div>

            <div class="post-content">
                <p>Here are selected research papers at the intersection of gaze/eye-tracking and LLMs for February 28, 2026.</p>

                <h2 id="daily-concept">Daily Concept: Eye tracking in multimodal AI</h2>
                <p><strong>1. What is it?</strong><br>
Eye tracking in multimodal AI involves integrating gaze data as a distinct input modality alongside text and images within large language models. This approach allows models to leverage human attention mechanisms, such as fixation points and scanpaths, to ground visual processing in user intent. It transforms the eye tracker from a passive recording device into an active signal for model training or inference.</p>

<p><strong>2. How does it work?</strong><br>
The process typically involves encoding raw gaze coordinates into feature embeddings compatible with the LLM's vector space. Models often project gaze points onto visual feature maps to create attention masks or heatmaps that highlight salient image regions. Mathematically, if \( V \) represents visual features and \( G \) represents the gaze density map, the attended feature representation \( V' \) can be formulated as an element-wise multiplication \( V' = V \odot G \). These gaze-weighted features are then concatenated with text embeddings and processed by the transformer architecture to predict the next token.</p>

<p><strong>3. Why does it matter?</strong><br>
This integration is significant because gaze provides a strong implicit signal for resolving ambiguity in visual question answering and referential communication. It enables models to align visual inputs with linguistic intent more efficiently by focusing computational resources on task-relevant regions. Furthermore, it opens pathways for accessibility tools and adaptive interfaces that respond to user attention in real time.</p>

<p><strong>4. Key insight</strong><br>
Gaze data acts as a natural bridge between vision and language, providing a direct mechanism for models to prioritize visual information exactly where humans focus their attention.</p>

                <h2 id="papers">Paper Summaries</h2>

                <div class="paper-entry" id="paper-1">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.22774v1" target="_blank">Transformer Actor-Critic for Efficient Freshness-Aware Resource Allocation</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Maryam Ansarifard, Mohit K. Sharma, Kishor C. Joshi et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.22774v1" target="_blank">2602.22774v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p>Here is a structured summary of the research paper.</p>

<p><strong>1. Research Question</strong><br>
The paper investigates how to minimize the Age of Information (AoI) in multi-user wireless networks that use non-orthogonal multiple access (NOMA). It specifically asks how a learning agent can effectively schedule users with heterogeneous needs and strict latency constraints to maintain data freshness.</p>

<p><strong>2. Positioning</strong><br>
This work positions itself at the intersection of deep reinforcement learning and next-generation wireless networking. It addresses the limitations of traditional DRL methods which often struggle with scalability and interpreting inter-user dependencies in complex NOMA environments. By adopting architectures common in Natural Language Processing (LLMs), the authors fill a gap in creating intelligent, priority-aware resource allocation mechanisms.</p>

<p><strong>3. Key Contribution</strong><br>
The primary contribution is a Proximal Policy Optimization (PPO) framework enhanced with a Transformer encoder for the actor-critic network. This approach uniquely leverages the self-attention mechanism to capture inter-user dependencies and focus on critical states, resulting in reduced average AoI compared to baseline methods.</p>

<p><strong>4. Methodology</strong><br>
The authors model the uplink scheduling problem as a Markov Decision Process (MDP) and solve it using a Deep Reinforcement Learning agent. They utilize a Transformer encoder within the neural network architecture, allowing the model to weigh the importance of different user states dynamically rather than treating all inputs uniformly.</p>

<p><strong>5. Limitations</strong><br>
The abstract does not explicitly discuss the computational overhead or inference latency introduced by the Transformer architecture, which is a critical factor for ultra-reliable low-latency communication (URLLC) systems. Furthermore, the evaluation relies on simulations, and the transition to real-world hardware implementation may face challenges regarding the processing speed of attention mechanisms.</p>

<p><strong>6. Critical Evaluation</strong><br>
This paper provides a strong demonstration of how attention mechanisms, the building blocks of Large Language Models, can be repurposed for wireless resource allocation to improve interpretability. The visualization of attention weights offers a "gaze" into the model's decision-making process, showing it learns to prioritize high-importance users much like human visual attention. However, the practical deployment of Transformer-based models in energy-constrained or ultra-low-latency edge devices remains a significant challenge that requires further investigation.</p>
                </div>
                <div class="paper-entry" id="paper-2">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.22368v1" target="_blank">EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Jiahao Zhang, Yifan Zhang, Kevin Leach et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.22368v1" target="_blank">2602.22368v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p><strong>1. Research Question</strong><br>
The paper investigates whether human eye-gaze patterns can serve as a proxy for expertise to guide and improve the performance of Large Language Models (LLMs) on the task of code summarization. It asks if integrating human attention signals into LLMs can enhance their semantic focus compared to standard fine-tuning approaches.</p>

<p><strong>2. Positioning</strong><br>
While LLMs have advanced significantly in code summarization, they typically rely on data-driven attention mechanisms that may not align with how human experts actually read and comprehend code. This work positions itself at the intersection of software engineering and cognitive modeling, filling the gap by using biological attention data to explicitly guide model attention rather than relying solely on learned statistical patterns.</p>

<p><strong>3. Key Contribution</strong><br>
The primary contribution is EyeLayer, a lightweight, plug-and-play module that integrates human gaze data into various LLM architectures without disrupting their existing representations. The paper demonstrates that this gaze-augmented approach consistently outperforms strong baselines across multiple model families, achieving up to a 13.17% improvement in BLEU-4 scores.</p>

<p><strong>4. Methodology</strong><br>
EyeLayer models human attention using a Multimodal Gaussian Mixture, learning parameters \((\mu_i, \sigma_i^2)\) to capture the location and intensity of developer focus. This mechanism redistributes token embeddings based on these learned gaze priors, effectively forcing the model to attend to specific code tokens similar to a human reader.</p>

<p><strong>5. Limitations</strong><br>
The approach relies on the availability and quality of eye-tracking data, which is often scarce and expensive to collect compared to standard code corpora. Furthermore, the method assumes that the gaze patterns of the specific participants in the dataset represent a universal "expert" standard for reading code, which may not hold true across different developer demographics or coding styles.</p>

<p><strong>6. Critical Evaluation</strong><br>
This work presents a strong interdisciplinary approach by successfully bridging cognitive science insights with modern LLM techniques. The architecture-agnostic design of EyeLayer is a significant strength, though the practical dependency on gaze data availability remains a hurdle for widespread adoption in resource-constrained environments.</p>
                </div>
                <div class="paper-entry" id="paper-3">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.21983v1" target="_blank">Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Jingchao Wei, Jingkai Qin, Yuxiao Cao et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.21983v1" target="_blank">2602.21983v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p><strong>1. Research Question</strong><br>
The paper addresses the challenge of enabling humanoid robots to perform natural and context-appropriate gaze shifts during unconstrained human-robot interaction. It specifically asks how to unify cognitive attention mechanisms, which decide where to look, with biomimetic motion generation, which decides how to move.</p>

<p><strong>2. Positioning</strong><br>
Existing literature often treats gaze target selection and motion generation as separate problems, leading to robotic behaviors that lack fluidity or social awareness. This work positions itself at the intersection of social robotics and generative AI, filling the gap between high-level semantic understanding and low-level physical actuation.</p>

<p><strong>3. Key Contribution</strong><br>
The primary contribution is the Robot Gaze-Shift (RGS) framework, a unified pipeline that bridges the gap between reasoning and acting. It introduces a novel combination of a Vision-Language Model (VLM) for semantic gaze reasoning and a conditional VQ-VAE for generating diverse, human-like eye and head movements.</p>

<p><strong>4. Methodology</strong><br>
The methodology consists of two distinct stages. First, the system uses a VLM to process multimodal inputs, such as audio and vision, to infer context-appropriate gaze targets. Second, it employs a conditional Vector Quantized-Variational Autoencoder (VQ-VAE) to generate coordinated eye and head movements that shift the robot's gaze toward the selected target.</p>

<p><strong>5. Limitations</strong><br>
While the abstract validates the framework's effectiveness, potential limitations include the computational latency inherent in running VLMs, which could delay reactions in real-time interactions. Additionally, the quality and diversity of the generated motion are strictly bounded by the datasets used to train the VQ-VAE, potentially limiting generalization to highly novel scenarios.</p>

<p><strong>6. Critical Evaluation</strong><br>
This work represents a significant advancement in humanizing robot behavior by successfully leveraging modern generative AI techniques for social interaction. The integration of VLMs for gaze reasoning is a particular strength, offering a scalable way to handle complex social cues, though the system's real-world utility will ultimately depend on its ability to operate with low latency on physical hardware.</p>
                </div>

                <hr>

                <h3 id="references">References</h3>
                <ul>
                    <li><a href="https://arxiv.org/abs/2602.22774v1" target="_blank">Transformer Actor-Critic for Efficient Freshness-Aware Resource Allocation</a></li>
                    <li><a href="https://arxiv.org/abs/2602.22368v1" target="_blank">EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization</a></li>
                    <li><a href="https://arxiv.org/abs/2602.21983v1" target="_blank">Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots</a></li>
                </ul>

                <p><em>Feel free to reach out for discussion. Thanks for reading!</em></p>
            </div>
        </article>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2026 SY. All rights reserved. | Gaze/LLM Research Daily Review</p>
        </div>
    </footer>
</body>
</html>