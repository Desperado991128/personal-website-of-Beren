<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SY&#39;s Gaze LLM Daily Review - 2026-02-28</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['\\(','\\)']], displayMath: [['\\[','\\]']]}
        });
    </script>
    <style>
        body {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            font-size: 18px;
        }
        header {
            margin-bottom: 2rem;
            border-bottom: 1px solid #eaecef;
        }
        .site-title {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2em;
            margin-bottom: 0.5em;
        }
        nav ul {
            display: flex;
            list-style: none;
            padding: 0;
            margin: 1rem 0;
        }
        nav ul li {
            margin-right: 1.5rem;
        }
        nav ul li a {
            text-decoration: none;
            color: #0366d6;
            font-weight: 600;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        .container {
            width: 100%;
            max-width: 800px;
        }
        h1 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2.4em;
            margin-bottom: 0.5em;
            line-height: 1.2;
        }
        h2 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.8em;
            margin-top: 1.5em;
            margin-bottom: 0.75em;
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }
        h3 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.5em;
            margin-top: 1.2em;
            margin-bottom: 0.6em;
        }
        h4 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.2em;
            margin-top: 1em;
            margin-bottom: 0.5em;
        }
        p {
            margin: 1em 0;
        }
        code {
            font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
            background: #f6f8fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 0.9em;
        }
        pre {
            background: #f6f8fa;
            padding: 16px;
            border-radius: 6px;
            overflow: auto;
            font-size: 0.9em;
        }
        blockquote {
            margin: 1em 0;
            padding: 0 1em;
            color: #6a737d;
            border-left: 0.25em solid #dfe2e5;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1.5em auto;
        }
        .meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
        }
        .post-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
        }
        /* SY Profile specific styling - green tint */
        .label {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: 600;
        }
        .figure-caption {
            text-align: center;
            color: #666;
            font-size: 0.9em;
            margin-top: -1em;
            margin-bottom: 2em;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1.5em 0;
        }
        th, td {
            border: 1px solid #dfe2e5;
            padding: 8px 12px;
            text-align: left;
        }
        th {
            background-color: #f6f8fa;
        }
        tr:nth-child(even) {
            background-color: #f6f8fa;
        }
        #table-of-contents {
            background-color: #f0f7f0;
            padding: 1.5rem;
            border-radius: 5px;
            margin: 2rem 0;
            border-left: 4px solid #137333;
        }
        #table-of-contents h3 {
            margin-top: 0;
            margin-bottom: 1rem;
            color: #137333;
        }
        #table-of-contents ol, #table-of-contents ul {
            margin-bottom: 0;
        }
        #table-of-contents a {
            text-decoration: none;
            color: #0366d6;
        }
        #table-of-contents a:hover {
            text-decoration: underline;
        }
        .post-content {
            margin-top: 2rem;
        }
        .paper-entry {
            margin-bottom: 2.5rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid #eaecef;
        }
        .paper-entry:last-child {
            border-bottom: none;
        }
        .paper-title {
            font-size: 1.3em;
            margin-bottom: 0.5em;
        }
        .paper-title a {
            color: #0366d6;
            text-decoration: none;
        }
        .paper-title a:hover {
            text-decoration: underline;
        }
        .paper-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 1em;
        }
        .paper-topics {
            display: inline-flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 0.5rem;
        }
        /* SY Profile topic tags - green tint */
        .topic-tag {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
        }
        footer {
            margin-top: 3rem;
            padding-top: 1rem;
            border-top: 1px solid #eaecef;
            font-size: 0.9em;
            color: #6a737d;
        }
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            h1 {
                font-size: 2em;
            }
            h2 {
                font-size: 1.6em;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1 class="site-title">Beren's Blog</h1>
            <nav>
                <ul>
                  <li><a href="../index.html">Home</a></li>
                  <li><a href="../blog.html">Blog</a></li>
                  <li><a href="../gallery.html">Gallery</a></li>
                  <li><a href="../about.html">About</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="container">
        <article class="blog-post">
            <h2 class="post-title">SY&#39;s Gaze LLM Daily Review</h2>
            <div class="post-meta">
                <span class="date">February 28, 2026</span>
                <span class="author">SY</span>
                <span class="label">Gaze LLM Research</span>
            </div>

            <div id="table-of-contents">
              <h3>Table of Contents</h3>
              <ol>
                <li><a href="#daily-concept">Daily Concept</a></li>
                <li><a href="#papers">Paper Summaries</a></li>
                <li><a href="#references">References</a></li>
              </ol>
            </div>

            <div class="post-content">
                <p>Here are selected research papers at the intersection of gaze/eye-tracking and LLMs for February 28, 2026.</p>

                <h2 id="daily-concept">Daily Concept: Eye tracking in multimodal AI</h2>
                <p>### 1. What is it?<br>
Eye tracking in multimodal AI involves integrating human gaze data as an explicit input signal or supervisory signal within models that process visual and textual information. It treats gaze patterns as a proxy for human attention, guiding the model to focus on specific regions of an image that are semantically relevant to the accompanying text. This approach creates a bridge between human cognitive processes and machine attention mechanisms.</p>

<p>### 2. How does it work?<br>
The core mechanism involves encoding gaze coordinates into spatial heatmaps or feature maps that align with the visual input dimensions. These gaze maps are fused with visual embeddings, often by modulating the attention weights in a Transformer architecture to prioritize attended regions. Mathematically, the standard attention score \( e_{ij} \) is often biased by a gaze prior \( g_{ij} \), resulting in a modulated weight:<br>
\[ \alpha_{ij} = \frac{\exp(e_{ij} + \lambda g_{ij})}{\sum_{k=1}^{N} \exp(e_{ik} + \lambda g_{ik})} \]<br>
Here, \( \lambda \) controls the influence of the gaze signal on the model's learned attention distribution.</p>

<p>### 3. Why does it matter?<br>
This integration matters because human gaze provides a strong prior for visual saliency and relevance, which helps models distinguish signal from noise in complex visual scenes. It significantly improves performance on tasks requiring fine grained visual understanding, such as Visual Question Answering (VQA) and image captioning, by reducing hallucination. Furthermore, it enhances model interpretability by aligning machine attention maps with human cognitive patterns.</p>

<p>### 4. Key insight. Gaze data acts as a cognitive shortcut that distills human intent directly into the model's attention mechanism. By learning where humans look, the model implicitly learns what is semantically meaningful.</p>

                <h2 id="papers">Paper Summaries</h2>

                <div class="paper-entry" id="paper-1">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.11669v1" target="_blank">Egocentric Gaze Estimation via Neck-Mounted Camera</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Haoyu Huang, Yoichi Sato<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.11669v1" target="_blank">2602.11669v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p>Here is a structured summary of the research paper.</p>

<p><strong>1. Research Question</strong><br>
The paper investigates whether it is possible to accurately estimate a device wearer's gaze using a camera mounted on the neck rather than on the head. It explores how this alternative, under-explored viewpoint compares to traditional head-mounted setups and what modeling techniques are effective for this new context.</p>

<p><strong>2. Positioning</strong><br>
Prior research in egocentric gaze estimation has focused almost exclusively on head-mounted cameras, leaving the potential of other body placements largely unexplored. This work positions itself as the first major exploration into neck-mounted gaze estimation. It fills a significant gap by providing the necessary data resources and baseline evaluations to legitimize this new viewpoint in the wearable technology space.</p>

<p><strong>3. Key Contribution</strong><br>
The authors introduce the first public dataset for neck-mounted gaze estimation, comprising roughly 4 hours of video from 8 participants performing daily activities. They also adapt a transformer-based model (GLC) for this task and propose two specific extensions. These extensions include an auxiliary task for classifying out-of-bound gaze and a geometry-aware multi-view co-learning strategy.</p>

<p><strong>4. Methodology</strong><br>
The researchers utilize a transformer-based architecture known as GLC to process the video data and predict gaze coordinates. They augment this baseline with an auxiliary classification head to determine if the user is looking outside the camera's field of view. Additionally, they experiment with a multi-view co-learning approach that attempts to leverage head-mounted camera data during training using a geometry-aware loss function.</p>

<p><strong>5. Limitations</strong><br>
The study acknowledges that the proposed multi-view co-learning approach failed to improve performance, suggesting that transferring knowledge between head and neck views is non-trivial. The dataset is also relatively small, consisting of only 8 participants, which may limit the generalizability of the findings across diverse populations and lighting conditions.</p>

<p><strong>6. Critical Evaluation</strong><br>
This work is a bold initial step into a novel area of wearable gaze estimation that could eventually enable less obtrusive devices than bulky head-mounted rigs. However, the failure of the co-learning approach indicates a significant domain shift between head and neck views that current geometric methods struggle to bridge. The relatively small dataset size is a weakness, but the successful application of out-of-bound classification offers a practical and immediate technical improvement for future research.</p>
                </div>
                <div class="paper-entry" id="paper-2">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.11648v1" target="_blank">Human-Like Gaze Behavior in Social Robots: A Deep Learning Approach Integrating Human and Non-Human Stimuli</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Faezeh Vahedi, Morteza Memari, Ramtin Tabatabaei et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.11648v1" target="_blank">2602.11648v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p><strong>1. Research Question</strong><br>
The paper addresses the challenge of creating naturalistic gaze behavior in social robots. It specifically asks how robots can dynamically adapt their gaze to respond to both human social cues and non-human environmental events, such as object drops or door openings.</p>

<p><strong>2. Positioning</strong><br>
Current literature on robotic gaze focuses heavily on human-to-human interaction cues, often neglecting environmental distractions. This work fills that gap by explicitly modeling gaze responses to non-human stimuli, which is a critical yet underexplored aspect of seamless communication. By integrating these external factors, the authors position their approach as a significant advancement over existing models that struggle with complex, dynamic contexts.</p>

<p><strong>3. Key Contribution</strong><br>
The primary contribution is a deep learning framework that predicts gaze behavior by integrating both social and non-social stimuli. The study demonstrates that Transformer and LSTM models can effectively learn these patterns from human data, achieving superior accuracy compared to previous methods. Furthermore, the authors validate their approach by deploying the model on a NAO robot, showing high user satisfaction in a large-scale evaluation.</p>

<p><strong>4. Methodology</strong><br>
The researchers collected gaze data from 41 participants using VR headsets while they viewed 3D animations and 360-degree real-world videos. This data trained two neural network architectures, specifically LSTM and Transformer models, to predict gaze direction. The trained models were then deployed on a NAO robot to evaluate real-time interaction performance with human participants.</p>

<p><strong>5. Limitations</strong><br>
While the abstract does not explicitly list limitations, the reported prediction accuracies, ranging from roughly 67% to 72%, suggest there is room for improvement in modeling complex human attention. Additionally, training on VR-simulated environments may introduce a "reality gap," where models struggle to generalize to the full complexity of physical world interactions not captured in the dataset.</p>

<p><strong>6. Critical Evaluation</strong><br>
This work provides a valuable contribution by expanding the scope of robotic gaze research to include environmental stimuli, making the behavior feel more authentic. The combination of technical prediction benchmarks with a large user study involving 275 participants offers a robust validation of the system's efficacy. However, the reliance on VR data collection is a potential weakness, as it may not fully replicate the visual and auditory dynamics of real-world scenarios.</p>
                </div>
                <div class="paper-entry" id="paper-3">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.10946v1" target="_blank">Developing Neural Network-Based Gaze Control Systems for Social Robots</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Ramtin Tabatabaei, Alireza Taheri<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.10946v1" target="_blank">2602.10946v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p><strong>1. Research Question</strong><br>
This paper aims to solve the challenge of generating natural and contextually appropriate gaze behavior for social robots during multi-party interactions. The core research question asks whether deep neural networks can effectively model and predict human gaze patterns in specific social scenarios to control a robot's attention.</p>

<p><strong>2. Positioning</strong><br>
The work positions itself at the intersection of social robotics and deep learning, addressing the need for robots to understand social context beyond simple object tracking. It fills a gap in the literature by moving from theoretical gaze models to an empirical, data-driven approach that maps specific social actions like waving or talking to precise motion-time patterns.</p>

<p><strong>3. Key Contribution</strong><br>
The primary contribution is the development of a complete pipeline that uses deep learning to translate human gaze data into executable robot control commands. The study provides a comparative analysis of data collected from both standard screen-based eye-trackers and Virtual Reality (VR) headsets, successfully deploying the resulting model on a physical Nao robot.</p>

<p><strong>4. Methodology</strong><br>
The researchers collected gaze data from 30 participants who watched video clips of various social scenarios on 2D screens and VR headsets. They employed Long Short-Term Memory (LSTM) networks and Transformer architectures to learn the temporal dynamics of gaze behavior. The best performing model was then integrated into a Nao robot to actuate the predicted gaze patterns.</p>

<p><strong>5. Limitations</strong><br>
The prediction accuracy reached only 60% for 2D animations and 65% for 3D animations, indicating significant room for improvement in modeling social intent. The study also relied on a limited set of pre-defined social scenarios, which may not fully capture the complexity and unpredictability of natural, real-world human interactions.</p>

<p><strong>6. Critical Evaluation</strong><br>
The application of Transformer architectures to model the sequential dependencies of social gaze is a logical and promising approach, leveraging the strengths of attention mechanisms for temporal data. While the moderate accuracy highlights the difficulty of predicting human intent, the successful user evaluation on the physical robot demonstrates that the model captures enough nuance to be perceived as socially competent.</p>
                </div>
                <div class="paper-entry" id="paper-4">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.04584v1" target="_blank">SalFormer360: a transformer-based saliency estimation model for 360-degree videos</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Mahmoud Z. A. Wahba, Francesco Barbato, Sara Baldoni et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.04584v1" target="_blank">2602.04584v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p>Here is a structured summary of the research paper.</p>

<p><strong>1. Research Question</strong><br>
The paper aims to solve the problem of accurately predicting visual saliency in 360-degree videos. It addresses the need for better models to optimize immersive content and predict user viewports in virtual reality environments.</p>

<p><strong>2. Positioning</strong><br>
This work positions itself as an evolution of saliency estimation techniques, moving beyond traditional methods to leverage transformer architectures. It fills a gap by adapting powerful 2D segmentation models, specifically SegFormer, to the unique spatial requirements of 360-degree video content.</p>

<p><strong>3. Key Contribution</strong><br>
The primary contribution is SalFormer360, a novel architecture that combines a fine-tuned SegFormer encoder with a custom decoder. The authors also introduce the integration of Viewing Center Bias to specifically address user attention patterns inherent to 360-degree environments.</p>

<p><strong>4. Methodology</strong><br>
The proposed method utilizes a transformer-based encoder-decoder structure where the encoder is adapted from SegFormer for spherical content. The authors explicitly incorporate Viewing Center Bias to account for how users distribute their attention in a virtual reality setting. They validate the model by comparing its performance against state-of-the-art methods on three major benchmark datasets.</p>

<p><strong>5. Limitations</strong><br>
The provided abstract does not explicitly list limitations. However, potential limitations common to this approach include the computational overhead associated with transformer models and the challenges inherent in adapting 2D architectures to spherical projections without losing spatial fidelity.</p>

<p><strong>6. Critical Evaluation</strong><br>
The work demonstrates significant performance improvements, most notably an \(18.6\%\) increase in the Pearson Correlation Coefficient on the VR-EyeTracking dataset. The approach is a strong technical contribution that effectively bridges the gap between 2D vision transformers and 360-degree multimedia analysis. While the results are compelling, the reliance on adapting 2D models suggests there is still room for developing native spherical processing techniques.</p>
                </div>
                <div class="paper-entry" id="paper-5">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.04489v1" target="_blank">Deconstructing sentence disambiguation by joint latent modeling of reading paradigms: LLM surprisal is not enough</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Dario Paape, Tal Linzen, Shravan Vasishth<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.04489v1" target="_blank">2602.04489v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p>### 1. Research Question. The paper investigates whether LLM-derived surprisal values alone are sufficient to explain human sentence processing difficulties, specifically focusing on "garden-path" sentences where temporary ambiguity occurs. It asks if a more nuanced model is required to separate distinct cognitive costs, such as the probability of misinterpretation versus the effort of reanalysis, while accounting for inattentive reading.</p>

<p>### 2. Positioning. This work positions itself against the prevailing trend in computational psycholinguistics that relies heavily on language model surprisal as a proxy for human processing effort. It fills a critical gap by arguing that surprisal is a monolithic metric that fails to capture the underlying latent processes of reading, such as the distinction between initial parsing errors and subsequent recovery. By integrating data from four different reading paradigms, the authors challenge the sufficiency of standard predictive metrics derived from models like GPT-2.</p>

<p>### 3. Key Contribution. The primary contribution is a latent-process mixture model that successfully disentangles three specific components of reading difficulty, namely garden-path probability, garden-path cost, and reanalysis cost. The model also accounts for inattentive reading trials, which refines the accuracy of processing cost estimates. The authors demonstrate that this approach outperforms a baseline model based on GPT-2 surprisal in predicting human reading patterns and task performance.</p>

<p>### 4. Methodology. The researchers utilized temporarily ambiguous garden-path sentences and collected human behavioral data across four paradigms, including eye tracking, uni- and bidirectional self-paced reading, and the Maze task. They developed a mixture model that treats the underlying reading strategy as a latent variable to distinguish between attentive processing and inattentive "skimming" states. This model was then evaluated through cross-validation against empirical data and compared against predictions generated from GPT-2 surprisal values.</p>

<p>### 5. Limitations. The study focuses specifically on garden-path sentences, which may limit the generalizability of the findings to the broader spectrum of syntactic processing and unambiguous texts. Additionally, the comparison relies on GPT-2, an older language model architecture, leaving open the question of whether newer, more powerful LLMs might capture these latent processes more effectively without the need for complex mixture modeling. The complexity of the proposed model could also pose challenges for widespread adoption as a standard evaluation metric.</p>

<p>### 6. Critical Evaluation. This paper offers a robust critique of the "surprisal is everything" hypothesis by providing a mechanistic explanation for *why* and *how* reading breaks down during ambiguity. The inclusion of eye-tracking data alongside other paradigms provides a high-fidelity validation of their latent variable approach. However, the strong claim that "LLM surprisal is not enough" rests on a comparison with GPT-2, and the field would benefit significantly from future work testing this framework against state-of-the-art models like GPT-4 or Llama.</p>
                </div>

                <hr>

                <h3 id="references">References</h3>
                <ul>
                    <li><a href="https://arxiv.org/abs/2602.11669v1" target="_blank">Egocentric Gaze Estimation via Neck-Mounted Camera</a></li>
                    <li><a href="https://arxiv.org/abs/2602.11648v1" target="_blank">Human-Like Gaze Behavior in Social Robots: A Deep Learning Approach Integrating Human and Non-Human Stimuli</a></li>
                    <li><a href="https://arxiv.org/abs/2602.10946v1" target="_blank">Developing Neural Network-Based Gaze Control Systems for Social Robots</a></li>
                    <li><a href="https://arxiv.org/abs/2602.04584v1" target="_blank">SalFormer360: a transformer-based saliency estimation model for 360-degree videos</a></li>
                    <li><a href="https://arxiv.org/abs/2602.04489v1" target="_blank">Deconstructing sentence disambiguation by joint latent modeling of reading paradigms: LLM surprisal is not enough</a></li>
                </ul>

                <p><em>Feel free to reach out for discussion. Thanks for reading!</em></p>
            </div>
        </article>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2026 SY. All rights reserved. | Gaze/LLM Research Daily Review</p>
        </div>
    </footer>
</body>
</html>