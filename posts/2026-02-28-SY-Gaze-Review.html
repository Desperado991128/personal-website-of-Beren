<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SY&#39;s Gaze LLM Daily Review - 2026-02-28</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['\\(','\\)']], displayMath: [['\\[','\\]']]}
        });
    </script>
    <style>
        body {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            font-size: 18px;
        }
        header {
            margin-bottom: 2rem;
            border-bottom: 1px solid #eaecef;
        }
        .site-title {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2em;
            margin-bottom: 0.5em;
        }
        nav ul {
            display: flex;
            list-style: none;
            padding: 0;
            margin: 1rem 0;
        }
        nav ul li {
            margin-right: 1.5rem;
        }
        nav ul li a {
            text-decoration: none;
            color: #0366d6;
            font-weight: 600;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        .container {
            width: 100%;
            max-width: 800px;
        }
        h1 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2.4em;
            margin-bottom: 0.5em;
            line-height: 1.2;
        }
        h2 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.8em;
            margin-top: 1.5em;
            margin-bottom: 0.75em;
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }
        h3 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.5em;
            margin-top: 1.2em;
            margin-bottom: 0.6em;
        }
        h4 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.2em;
            margin-top: 1em;
            margin-bottom: 0.5em;
        }
        p {
            margin: 1em 0;
        }
        code {
            font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
            background: #f6f8fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 0.9em;
        }
        pre {
            background: #f6f8fa;
            padding: 16px;
            border-radius: 6px;
            overflow: auto;
            font-size: 0.9em;
        }
        blockquote {
            margin: 1em 0;
            padding: 0 1em;
            color: #6a737d;
            border-left: 0.25em solid #dfe2e5;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1.5em auto;
        }
        .meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
        }
        .post-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
        }
        /* SY Profile specific styling - green tint */
        .label {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: 600;
        }
        .figure-caption {
            text-align: center;
            color: #666;
            font-size: 0.9em;
            margin-top: -1em;
            margin-bottom: 2em;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1.5em 0;
        }
        th, td {
            border: 1px solid #dfe2e5;
            padding: 8px 12px;
            text-align: left;
        }
        th {
            background-color: #f6f8fa;
        }
        tr:nth-child(even) {
            background-color: #f6f8fa;
        }
        #table-of-contents {
            background-color: #f0f7f0;
            padding: 1.5rem;
            border-radius: 5px;
            margin: 2rem 0;
            border-left: 4px solid #137333;
        }
        #table-of-contents h3 {
            margin-top: 0;
            margin-bottom: 1rem;
            color: #137333;
        }
        #table-of-contents ol, #table-of-contents ul {
            margin-bottom: 0;
        }
        #table-of-contents a {
            text-decoration: none;
            color: #0366d6;
        }
        #table-of-contents a:hover {
            text-decoration: underline;
        }
        .post-content {
            margin-top: 2rem;
        }
        .paper-entry {
            margin-bottom: 2.5rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid #eaecef;
        }
        .paper-entry:last-child {
            border-bottom: none;
        }
        .paper-title {
            font-size: 1.3em;
            margin-bottom: 0.5em;
        }
        .paper-title a {
            color: #0366d6;
            text-decoration: none;
        }
        .paper-title a:hover {
            text-decoration: underline;
        }
        .paper-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 1em;
        }
        .paper-topics {
            display: inline-flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 0.5rem;
        }
        /* SY Profile topic tags - green tint */
        .topic-tag {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
        }
        footer {
            margin-top: 3rem;
            padding-top: 1rem;
            border-top: 1px solid #eaecef;
            font-size: 0.9em;
            color: #6a737d;
        }
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            h1 {
                font-size: 2em;
            }
            h2 {
                font-size: 1.6em;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1 class="site-title">Beren's Blog</h1>
            <nav>
                <ul>
                  <li><a href="../index.html">Home</a></li>
                  <li><a href="../blog.html">Blog</a></li>
                  <li><a href="../gallery.html">Gallery</a></li>
                  <li><a href="../about.html">About</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="container">
        <article class="blog-post">
            <h2 class="post-title">SY&#39;s Gaze LLM Daily Review</h2>
            <div class="post-meta">
                <span class="date">February 28, 2026</span>
                <span class="author">SY</span>
                <span class="label">Gaze LLM Research</span>
            </div>

            <div id="table-of-contents">
              <h3>Table of Contents</h3>
              <ol>
                <li><a href="#daily-concept">Daily Concept</a></li>
                <li><a href="#papers">Paper Summaries</a></li>
                <li><a href="#references">References</a></li>
              </ol>
            </div>

            <div class="post-content">
                <p>Here are selected research papers at the intersection of gaze/eye-tracking and LLMs for February 28, 2026.</p>

                <h2 id="daily-concept">Daily Concept: Eye tracking in multimodal AI</h2>
                <p><strong>1. What is it?</strong><br>
Eye tracking in multimodal AI involves integrating human gaze data, such as fixation locations and scanpaths, as a supplementary input signal alongside traditional modalities like text and images. This technique treats visual attention as a distinct data modality that guides the model toward semantically relevant regions. It effectively bridges human cognitive processes with machine learning architectures.</p>

<p><strong>2. How does it work?</strong><br>
The process begins by converting raw gaze coordinates into structured inputs like spatial heatmaps or sequential token embeddings. These representations are projected into a shared latent space and fused with visual and textual features, often through cross-attention mechanisms or concatenation. For instance, the combined input representation \( H \) for a multimodal transformer might be formulated as \( H = f(E_{text}, E_{image}, E_{gaze}) \), where \( E_{gaze} \) represents the encoded gaze embeddings. The model then learns to weigh specific visual features based on where humans focused during data collection.</p>

<p><strong>3. Why does it matter?</strong><br>
Incorporating gaze provides a strong inductive bias that mimics human selectivity, allowing models to ignore irrelevant noise and focus on salient features. This leads to improved performance and faster convergence in tasks requiring fine-grained visual understanding, such as visual question answering or medical image analysis. It also enhances model interpretability by offering a human-grounded reference for evaluating machine attention mechanisms.</p>

<p><strong>4. Key insight</strong><br>
Gaze data acts as a supervisory signal that teaches models not just what to process, but where to look, effectively distilling human cognitive priorities into machine learning pipelines.</p>

                <h2 id="papers">Paper Summaries</h2>

                <div class="paper-entry" id="paper-1">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.22774v1" target="_blank">Transformer Actor-Critic for Efficient Freshness-Aware Resource Allocation</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Maryam Ansarifard, Mohit K. Sharma, Kishor C. Joshi et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.22774v1" target="_blank">2602.22774v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p>Here is a structured summary of the research paper.</p>

<p>### 1. Research Question. The paper aims to solve the problem of minimizing the Age of Information (AoI) in multi-user uplink wireless networks that use non-orthogonal multiple access (NOMA). The core research question asks how an intelligent agent can effectively schedule heterogeneous users with different task sizes and urgency levels to ensure information freshness while adhering to strict wireless transmission constraints.</p>

<p>### 2. Positioning. This work positions itself at the intersection of deep reinforcement learning (DRL) and wireless communications, specifically addressing the limitations of traditional DRL methods in handling complex, large-scale state spaces. It fills a gap in the literature by moving beyond standard neural networks, which often struggle with user heterogeneity and inter-dependencies, by adopting architectures typically used in sequence modeling. The paper argues that existing methods lack the scalability and focus required for dynamic priority-aware resource allocation.</p>

<p>### 3. Key Contribution. The main contribution is a Proximal Policy Optimization (PPO) framework enhanced with a Transformer encoder, designed to optimize resource allocation for AoI minimization. The novel integration of the attention mechanism allows the model to dynamically weigh the importance of different users, effectively capturing inter-user dependencies that simpler architectures miss. Furthermore, the paper provides an interpretability analysis, demonstrating that the model learns to "gaze" at high-priority users, evolving from uniform attention to structured, priority-based focus.</p>

<p>### 4. Methodology. The authors model the wireless scheduling problem as a Markov Decision Process (MDP) and solve it using a Transformer-based Actor-Critic network. The architecture leverages self-attention mechanisms to process the system state, allowing the agent to identify critical users and schedule transmissions under NOMA constraints. This approach treats the set of user states as an input sequence, where the attention weights act similarly to a visual gaze, filtering out less relevant information to focus on users with high penalty sensitivity or aging data.</p>

<p>### 5. Limitations. While the abstract highlights successful simulations, the approach likely incurs higher computational overhead compared to simpler baseline models due to the complexity of the Transformer architecture. The evaluation is conducted in a simulated environment, which may not fully capture the unpredictable noise and hardware constraints of real-world wireless channels. Additionally, the reliance on specific NOMA constraints might limit the direct applicability of the model to other multiple access technologies without significant retraining or structural changes.</p>

<p>### 6. Critical Evaluation. This work offers a compelling demonstration of how attention mechanisms, foundational to Large Language Models (LLMs), can be repurposed for decision-making in wireless networks. The primary strength lies in the interpretability of the results, as the visualization of attention weights provides clear insight into the model's scheduling logic, analogous to analyzing eye-tracking data to understand human focus. However, the practical deployment of such models in real-time systems remains a challenge, as the latency introduced by Transformer inference could potentially counteract the gains achieved in information freshness.</p>
                </div>
                <div class="paper-entry" id="paper-2">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.22368v1" target="_blank">EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Jiahao Zhang, Yifan Zhang, Kevin Leach et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.22368v1" target="_blank">2602.22368v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p>### 1. Research Question. The paper investigates whether human eye-gaze patterns can be integrated into Large Language Models (LLMs) to improve the quality of automated code summarization. It specifically asks if human attention priors can serve as a proxy for expertise to enhance the semantic focus of these models.</p>

<p>### 2. Positioning. While LLMs have achieved high performance in code understanding, they typically rely solely on data-driven attention mechanisms without explicit guidance from human cognitive processes. This work positions itself at the intersection of software engineering and cognitive computing, filling the gap by treating eye-tracking data as a source of human expertise that can complement standard model training.</p>

<p>### 3. Key Contribution. The primary contribution is EyeLayer, a lightweight module designed to inject human gaze information into various model architectures without disrupting their pre-trained representations. It demonstrates that human attention signals are highly transferable, successfully improving performance across distinct model families including LLaMA-3.2, Qwen3, and CodeBERT.</p>

<p>### 4. Methodology. EyeLayer models human attention using a Multimodal Gaussian Mixture, parameterized by learned means \(\mu_i\) and variances \(\sigma_i^2\) to capture the location and intensity of developer focus. This mechanism redistributes token embeddings based on these gaze parameters, effectively creating an attention prior that guides the model to focus on code elements humans find most important.</p>

<p>### 5. Limitations. The approach relies on the availability of high-quality eye-tracking data, which is expensive to collect and much scarcer than standard code corpora. Additionally, the effectiveness of the learned gaze priors may vary depending on how well the eye-tracking participants' coding expertise aligns with the specific code summarization tasks.</p>

<p>### 6. Critical Evaluation. This work offers a compelling fusion of cognitive science and deep learning, providing a novel pathway to improve model interpretability and performance through human-centric signals. The reported BLEU-4 gains of up to 13.17% are significant, though the practical scalability of the method depends on overcoming the logistical hurdles associated with large-scale gaze data collection.</p>
                </div>
                <div class="paper-entry" id="paper-3">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.21983v1" target="_blank">Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Jingchao Wei, Jingkai Qin, Yuxiao Cao et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.21983v1" target="_blank">2602.21983v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p><strong>1. Research Question</strong><br>
The paper addresses the challenge of enabling humanoid robots to perform natural and context-appropriate gaze shifts during unconstrained human-robot interaction. It asks how to effectively unify cognitive attention mechanisms, which determine where to look, with biomimetic motion generation, which determines how to move the eyes and head.</p>

<p><strong>2. Positioning</strong><br>
Existing research often treats gaze target selection and motion generation as separate problems, resulting in mechanical or contextually disjointed behaviors in robots. This work positions itself at the intersection of cognitive robotics and generative motion synthesis, filling a critical gap by integrating semantic understanding with physical motor control.</p>

<p><strong>3. Key Contribution</strong><br>
The primary contribution is the Robot Gaze-Shift (RGS) framework, a unified pipeline that bridges the divide between gaze intent and physical action. The paper introduces two novel components. First, it uses a Vision-Language Model (VLM) for semantic gaze reasoning. Second, it employs a conditional VQ-VAE for generating diverse, human-like eye-head coordination.</p>

<p><strong>4. Methodology</strong><br>
The RGS framework operates in two distinct stages. First, a VLM-based pipeline processes multimodal inputs to infer gaze targets that align with human social norms. Second, a conditional Vector Quantized-Variational Autoencoder (VQ-VAE) generates the actual motion trajectories, coordinating the robot's eyes and head to produce realistic movements.</p>

<p><strong>5. Limitations</strong><br>
While the abstract validates the framework's effectiveness, potential limitations include the computational latency of VLMs, which could affect real-time responsiveness during fast-paced interactions. Additionally, the diversity of the generated motions is inherently constrained by the data used to train the VQ-VAE, potentially limiting generalization to highly novel social contexts.</p>

<p><strong>6. Critical Evaluation</strong><br>
This work offers a compelling integration of high-level semantic reasoning and low-level motion generation, marking a significant advance in social robotics. The use of generative models like VQ-VAE to introduce motion diversity is a particular strength, as it avoids the repetitive stiffness often seen in traditional robotic control systems. However, the practical success of this system will ultimately depend on its ability to execute these complex models with low latency in noisy, real-world environments.</p>
                </div>
                <div class="paper-entry" id="paper-4">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.20408v1" target="_blank">Examining and Addressing Barriers to Diversity in LLM-Generated Ideas</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Yuting Deng, Melanie Brucks, Olivier Toubia<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.20408v1" target="_blank">2602.20408v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p>1. <strong>Research Question</strong><br>
 The paper investigates why Large Language Models (LLMs) generate less diverse ideas compared to independent groups of humans. It asks whether cognitive mechanisms like fixation and knowledge aggregation limit LLM diversity and if targeted prompting interventions can mitigate these issues.</p>

<p>2. <strong>Positioning</strong><br>
 This research positions itself at the intersection of cognitive psychology and generative AI. It addresses a gap in the literature regarding the specific mechanisms causing homogenization in LLM outputs. By applying theories of human cognition to artificial systems, the authors move beyond observing low diversity to explaining its theoretical roots.</p>

<p>3. <strong>Key Contribution</strong><br>
 The main contribution is the identification of two specific barriers to diversity, fixation and knowledge aggregation, alongside a practical prompting framework to solve them. The authors demonstrate that combining Chain-of-Thought prompting with "ordinary" personas significantly increases idea diversity. This combined approach allows LLMs to outperform human baselines in generating diverse concepts.</p>

<p>4. <strong>Methodology</strong><br>
 The researchers conducted four studies to empirically test the effects of different prompting strategies on idea generation. They used Chain-of-Thought prompting to reduce individual fixation and assigned distinct ordinary personas to simulate the knowledge partitioning found in human populations. They then measured the semantic distance between generated ideas to quantify diversity.</p>

<p>5. <strong>Limitations</strong><br>
 The study relies on semantic similarity metrics to measure diversity, which may not fully capture the practical utility or quality of the generated ideas. The effectiveness of persona-based prompting depends heavily on the specific training data and biases inherent in the model used. Additionally, the findings are specific to text-based ideation tasks and may not generalize to other creative domains.</p>

<p>6. <strong>Critical Evaluation</strong><br>
 The strength of this work lies in its theoretical grounding, offering a robust explanation for LLM homogenization rather than treating the model as a black box. The finding that "ordinary" personas outperform famous "creative" archetypes is a counter-intuitive and valuable insight for prompt engineers. However, the approach relies on simulating diversity through statistical cues rather than achieving genuine cognitive independence, which may still lead to systemic biases over time.</p>
                </div>

                <hr>

                <h3 id="references">References</h3>
                <ul>
                    <li><a href="https://arxiv.org/abs/2602.22774v1" target="_blank">Transformer Actor-Critic for Efficient Freshness-Aware Resource Allocation</a></li>
                    <li><a href="https://arxiv.org/abs/2602.22368v1" target="_blank">EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization</a></li>
                    <li><a href="https://arxiv.org/abs/2602.21983v1" target="_blank">Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots</a></li>
                    <li><a href="https://arxiv.org/abs/2602.20408v1" target="_blank">Examining and Addressing Barriers to Diversity in LLM-Generated Ideas</a></li>
                </ul>

                <p><em>Feel free to reach out for discussion. Thanks for reading!</em></p>
            </div>
        </article>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2026 SY. All rights reserved. | Gaze/LLM Research Daily Review</p>
        </div>
    </footer>
</body>
</html>