<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SY&#39;s Gaze LLM Daily Review - 2026-02-28</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['\\(','\\)']], displayMath: [['\\[','\\]']]}
        });
    </script>
    <style>
        body {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            font-size: 18px;
        }
        header {
            margin-bottom: 2rem;
            border-bottom: 1px solid #eaecef;
        }
        .site-title {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2em;
            margin-bottom: 0.5em;
        }
        nav ul {
            display: flex;
            list-style: none;
            padding: 0;
            margin: 1rem 0;
        }
        nav ul li {
            margin-right: 1.5rem;
        }
        nav ul li a {
            text-decoration: none;
            color: #0366d6;
            font-weight: 600;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        .container {
            width: 100%;
            max-width: 800px;
        }
        h1 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2.4em;
            margin-bottom: 0.5em;
            line-height: 1.2;
        }
        h2 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.8em;
            margin-top: 1.5em;
            margin-bottom: 0.75em;
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }
        h3 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.5em;
            margin-top: 1.2em;
            margin-bottom: 0.6em;
        }
        h4 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.2em;
            margin-top: 1em;
            margin-bottom: 0.5em;
        }
        p {
            margin: 1em 0;
        }
        code {
            font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
            background: #f6f8fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 0.9em;
        }
        pre {
            background: #f6f8fa;
            padding: 16px;
            border-radius: 6px;
            overflow: auto;
            font-size: 0.9em;
        }
        blockquote {
            margin: 1em 0;
            padding: 0 1em;
            color: #6a737d;
            border-left: 0.25em solid #dfe2e5;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1.5em auto;
        }
        .meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
        }
        .post-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
        }
        /* SY Profile specific styling - green tint */
        .label {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: 600;
        }
        .figure-caption {
            text-align: center;
            color: #666;
            font-size: 0.9em;
            margin-top: -1em;
            margin-bottom: 2em;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1.5em 0;
        }
        th, td {
            border: 1px solid #dfe2e5;
            padding: 8px 12px;
            text-align: left;
        }
        th {
            background-color: #f6f8fa;
        }
        tr:nth-child(even) {
            background-color: #f6f8fa;
        }
        #table-of-contents {
            background-color: #f0f7f0;
            padding: 1.5rem;
            border-radius: 5px;
            margin: 2rem 0;
            border-left: 4px solid #137333;
        }
        #table-of-contents h3 {
            margin-top: 0;
            margin-bottom: 1rem;
            color: #137333;
        }
        #table-of-contents ol, #table-of-contents ul {
            margin-bottom: 0;
        }
        #table-of-contents a {
            text-decoration: none;
            color: #0366d6;
        }
        #table-of-contents a:hover {
            text-decoration: underline;
        }
        .post-content {
            margin-top: 2rem;
        }
        .paper-entry {
            margin-bottom: 2.5rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid #eaecef;
        }
        .paper-entry:last-child {
            border-bottom: none;
        }
        .paper-title {
            font-size: 1.3em;
            margin-bottom: 0.5em;
        }
        .paper-title a {
            color: #0366d6;
            text-decoration: none;
        }
        .paper-title a:hover {
            text-decoration: underline;
        }
        .paper-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 1em;
        }
        .paper-topics {
            display: inline-flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 0.5rem;
        }
        /* SY Profile topic tags - green tint */
        .topic-tag {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
        }
        footer {
            margin-top: 3rem;
            padding-top: 1rem;
            border-top: 1px solid #eaecef;
            font-size: 0.9em;
            color: #6a737d;
        }
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            h1 {
                font-size: 2em;
            }
            h2 {
                font-size: 1.6em;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1 class="site-title">Beren's Blog</h1>
            <nav>
                <ul>
                  <li><a href="../index.html">Home</a></li>
                  <li><a href="../blog.html">Blog</a></li>
                  <li><a href="../gallery.html">Gallery</a></li>
                  <li><a href="../about.html">About</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="container">
        <article class="blog-post">
            <h2 class="post-title">SY&#39;s Gaze LLM Daily Review</h2>
            <div class="post-meta">
                <span class="date">February 28, 2026</span>
                <span class="author">SY</span>
                <span class="label">Gaze LLM Research</span>
            </div>

            <div id="table-of-contents">
              <h3>Table of Contents</h3>
              <ol>
                <li><a href="#daily-concept">Daily Concept</a></li>
                <li><a href="#papers">Paper Summaries</a></li>
                <li><a href="#references">References</a></li>
              </ol>
            </div>

            <div class="post-content">
                <p>Here are selected research papers at the intersection of gaze/eye-tracking and LLMs for February 28, 2026.</p>

                <h2 id="daily-concept">Daily Concept: Eye tracking in multimodal AI</h2>
                <p><strong>1. What is it?</strong><br>
Eye tracking in multimodal AI involves integrating human gaze data as an explicit input modality alongside text and images within models like Vision-Language Models (VLMs). This approach treats eye movements not just as a behavioral metric, but as a supervisory signal to guide model attention or as a feature to predict user intent. It effectively bridges the gap between human visual cognition and artificial attention mechanisms.</p>

<p><strong>2. How does it work?</strong><br>
The process typically begins by encoding raw gaze coordinates into a spatial heatmap or a sequence of fixation embeddings. These gaze features are fused with visual features extracted by a vision encoder, effectively weighting regions of interest that humans find salient. Mathematically, the visual feature map \( V \) is often modulated by a gaze density map \( G \) to produce a gaze-weighted representation \( V' = V \odot G \), which is then processed by the language model. Alternatively, models may predict gaze patterns as an auxiliary task to force the artificial attention heads to align with human attention.</p>

<p><strong>3. Why does it matter?</strong><br>
Integrating gaze signals allows models to prioritize relevant visual information, mimicking the efficiency of human visual processing. This leads to improved performance on tasks requiring detailed visual reasoning, such as visual question answering, by reducing the search space for the model. Furthermore, it enhances interpretability by aligning the model focus with human attention, making AI decision processes more transparent.</p>

<p><strong>4. Key insight</strong><br>
Eye tracking provides a unique form of implicit supervision that teaches AI systems not just what to look at, but how to look, effectively aligning artificial attention mechanisms with biological visual cognition.</p>

                <h2 id="papers">Paper Summaries</h2>

                <div class="paper-entry" id="paper-1">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.22774v1" target="_blank">Transformer Actor-Critic for Efficient Freshness-Aware Resource Allocation</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Maryam Ansarifard, Mohit K. Sharma, Kishor C. Joshi et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.22774v1" target="_blank">2602.22774v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p>Here is a structured summary of the research paper.</p>

<p><strong>1. Research Question</strong><br>
The paper aims to solve the problem of minimizing the Age of Information (AoI) in multi-user wireless networks that use non-orthogonal multiple access (NOMA). It specifically asks how a learning agent can effectively schedule users with diverse data requirements and constraints to ensure information freshness.</p>

<p><strong>2. Positioning</strong><br>
This work positions itself at the intersection of wireless resource allocation and deep reinforcement learning. It addresses the limitations of traditional methods that struggle with the high complexity and dynamic nature of user heterogeneity in next-generation networks. By integrating advanced neural architectures, the authors fill a gap in applying attention mechanisms to handle inter-user dependencies in communication scheduling.</p>

<p><strong>3. Key Contribution</strong><br>
The primary contribution is a novel Proximal Policy Optimization (PPO) framework that incorporates a Transformer encoder to manage scheduling decisions. This approach leverages the attention mechanism to dynamically prioritize users based on their urgency and constraint sensitivity. The paper also provides an interpretability analysis, demonstrating that the model learns to focus its attention on high-priority users much like a human focuses on relevant visual stimuli.</p>

<p><strong>4. Methodology</strong><br>
The authors model the uplink network environment as a Markov Decision Process (MDP) where the agent observes user states and allocates power resources. They replace standard neural network layers in the actor-critic model with a Transformer encoder, allowing the system to weigh the importance of different user inputs simultaneously. This architecture captures the complex relationships between users while adhering to the specific power constraints of NOMA systems.</p>

<p><strong>5. Limitations</strong><br>
While the simulations demonstrate performance gains, the study relies on modeled environments which may not capture all the unpredictability of physical wireless channels. The use of Transformer architectures introduces higher computational overhead and parameter counts compared to simpler baseline models, potentially challenging real-time deployment on edge hardware. Additionally, the analysis focuses primarily on average AoI reduction, potentially overlooking worst-case latency scenarios.</p>

<p><strong>6. Critical Evaluation</strong><br>
This research offers a compelling translation of Transformer architecture from Natural Language Processing (NLP) to wireless optimization. The use of attention weights provides a form of "digital gaze," allowing researchers to visually interpret the model's decision-making process and verify that it aligns with logical scheduling priorities. While the approach effectively handles scalability, the practical adoption of such computationally intensive models in ultra-low-latency systems remains an open challenge.</p>
                </div>
                <div class="paper-entry" id="paper-2">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.22368v1" target="_blank">EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Jiahao Zhang, Yifan Zhang, Kevin Leach et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.22368v1" target="_blank">2602.22368v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p><strong>1. Research Question</strong><br>
The paper investigates whether human eye-gaze patterns can serve as a proxy for developer expertise to improve the performance of Large Language Models (LLMs) on code summarization tasks. It specifically asks if integrating biological attention signals into LLMs can enhance their semantic focus and generate more accurate natural language descriptions of source code.</p>

<p><strong>2. Positioning</strong><br>
While LLMs have achieved state-of-the-art results in code summarization, current methods rely solely on data-driven attention mechanisms that may miss the intuitive focus points used by human developers. This work positions itself at the intersection of software engineering and cognitive computing, filling a gap by incorporating human cognitive signals as explicit attention priors to guide model reasoning.</p>

<p><strong>3. Key Contribution</strong><br>
The primary contribution is EyeLayer, a lightweight module that seamlessly integrates human gaze data into diverse LLM architectures without disrupting their pre-trained representations. The authors demonstrate that this biologically-inspired approach consistently outperforms standard fine-tuning baselines, achieving up to a 13.17% improvement in BLEU-4 scores across multiple model families.</p>

<p><strong>4. Methodology</strong><br>
EyeLayer models human attention using a Multimodal Gaussian Mixture to capture where and how intensively developers focus on code tokens. This mechanism redistributes token embeddings based on learned parameters \((\mu_i, \sigma_i^2)\), effectively creating an attention prior that augments the standard self-attention mechanism in models like LLaMA-3.2 and CodeBERT.</p>

<p><strong>5. Limitations</strong><br>
Although the abstract highlights strong performance, a potential limitation is the dependency on high-quality eye-tracking datasets, which are often scarce and expensive to collect compared to standard code corpora. Additionally, the approach assumes that the gaze patterns of the study participants generalize effectively to the broader population of developers and diverse coding styles.</p>

<p><strong>6. Critical Evaluation</strong><br>
This work offers a novel perspective by treating eye-tracking data as a learnable mathematical prior rather than just an external feature, suggesting that human attention encodes structural semantics that LLMs alone may overlook. The reported gains across different architectures are compelling, though the practical adoption of this method will depend on the cost and availability of gaze data for new programming languages or domains.</p>
                </div>
                <div class="paper-entry" id="paper-3">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.21983v1" target="_blank">Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Jingchao Wei, Jingkai Qin, Yuxiao Cao et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.21983v1" target="_blank">2602.21983v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p>### 1. Research Question. The paper addresses the challenge of generating natural gaze shifts in humanoid robots during unconstrained human robot interaction. It asks how to unify cognitive attention mechanisms for target selection with biomimetic motion generation for realistic movement.</p>

<p>### 2. Positioning. This work positions itself at the intersection of cognitive robotics and motion generation. It addresses a gap in existing literature where gaze targets are often predetermined or motion generation lacks contextual awareness. The authors argue that current methods fail to fully integrate multimodal attention cues with the biological mechanics of eye head coordination in open ended interactions.</p>

<p>### 3. Key Contribution. The primary contribution is the Robot Gaze Shift (RGS) framework which unifies gaze reasoning and motion generation into a single pipeline. The authors introduce a Vision Language Model (VLM) approach to determine socially appropriate gaze targets. They also contribute a conditional Vector Quantized Variational Autoencoder (VQ VAE) model that produces diverse and human like eye head coordinated movements.</p>

<p>### 4. Methodology. The methodology consists of two distinct stages. First, a VLM analyzes multimodal inputs to infer gaze targets that align with human social norms. Second, a conditional VQ VAE model generates the actual motion trajectories, ensuring that the robot's eye and head movements are coordinated and natural rather than robotic or rigid.</p>

<p>### 5. Limitations. The abstract does not explicitly list limitations, but potential constraints are inherent to the chosen architectures. The reliance on VLMs may introduce latency issues in real time interaction loops or reasoning errors in complex scenes. Additionally, the diversity of the generated motion is bounded by the training data of the VQ VAE, which might not cover all possible nuances of human idiosyncratic gaze behavior.</p>

<p>### 6. Critical Evaluation. This work offers a compelling integration of modern generative AI techniques with classical robotics problems. The application of VLMs to gaze reasoning is a logical and powerful step forward for social robotics. However, the real world viability depends heavily on the inference speed of the VLM and the smoothness of the VQ VAE outputs on physical hardware.</p>
                </div>

                <hr>

                <h3 id="references">References</h3>
                <ul>
                    <li><a href="https://arxiv.org/abs/2602.22774v1" target="_blank">Transformer Actor-Critic for Efficient Freshness-Aware Resource Allocation</a></li>
                    <li><a href="https://arxiv.org/abs/2602.22368v1" target="_blank">EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization</a></li>
                    <li><a href="https://arxiv.org/abs/2602.21983v1" target="_blank">Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots</a></li>
                </ul>

                <p><em>Feel free to reach out for discussion. Thanks for reading!</em></p>
            </div>
        </article>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2026 SY. All rights reserved. | Gaze/LLM Research Daily Review</p>
        </div>
    </footer>
</body>
</html>