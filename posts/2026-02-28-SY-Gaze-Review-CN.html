<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SY眼动LLM研究日报 - 2026-02-28</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['\\(','\\)']], displayMath: [['\\[','\\]']]}
        });
    </script>
    <style>
        body {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            font-size: 18px;
        }
        header {
            margin-bottom: 2rem;
            border-bottom: 1px solid #eaecef;
        }
        .site-title {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2em;
            margin-bottom: 0.5em;
        }
        nav ul {
            display: flex;
            list-style: none;
            padding: 0;
            margin: 1rem 0;
        }
        nav ul li {
            margin-right: 1.5rem;
        }
        nav ul li a {
            text-decoration: none;
            color: #0366d6;
            font-weight: 600;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        .container {
            width: 100%;
            max-width: 800px;
        }
        h1 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2.4em;
            margin-bottom: 0.5em;
            line-height: 1.2;
        }
        h2 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.8em;
            margin-top: 1.5em;
            margin-bottom: 0.75em;
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }
        h3 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.5em;
            margin-top: 1.2em;
            margin-bottom: 0.6em;
        }
        h4 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.2em;
            margin-top: 1em;
            margin-bottom: 0.5em;
        }
        p {
            margin: 1em 0;
        }
        code {
            font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
            background: #f6f8fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 0.9em;
        }
        pre {
            background: #f6f8fa;
            padding: 16px;
            border-radius: 6px;
            overflow: auto;
            font-size: 0.9em;
        }
        blockquote {
            margin: 1em 0;
            padding: 0 1em;
            color: #6a737d;
            border-left: 0.25em solid #dfe2e5;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1.5em auto;
        }
        .meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
        }
        .post-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
        }
        /* SY Profile specific styling - green tint */
        .label {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: 600;
        }
        .figure-caption {
            text-align: center;
            color: #666;
            font-size: 0.9em;
            margin-top: -1em;
            margin-bottom: 2em;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1.5em 0;
        }
        th, td {
            border: 1px solid #dfe2e5;
            padding: 8px 12px;
            text-align: left;
        }
        th {
            background-color: #f6f8fa;
        }
        tr:nth-child(even) {
            background-color: #f6f8fa;
        }
        #table-of-contents {
            background-color: #f0f7f0;
            padding: 1.5rem;
            border-radius: 5px;
            margin: 2rem 0;
            border-left: 4px solid #137333;
        }
        #table-of-contents h3 {
            margin-top: 0;
            margin-bottom: 1rem;
            color: #137333;
        }
        #table-of-contents ol, #table-of-contents ul {
            margin-bottom: 0;
        }
        #table-of-contents a {
            text-decoration: none;
            color: #0366d6;
        }
        #table-of-contents a:hover {
            text-decoration: underline;
        }
        .post-content {
            margin-top: 2rem;
        }
        .paper-entry {
            margin-bottom: 2.5rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid #eaecef;
        }
        .paper-entry:last-child {
            border-bottom: none;
        }
        .paper-title {
            font-size: 1.3em;
            margin-bottom: 0.5em;
        }
        .paper-title a {
            color: #0366d6;
            text-decoration: none;
        }
        .paper-title a:hover {
            text-decoration: underline;
        }
        .paper-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 1em;
        }
        .paper-topics {
            display: inline-flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 0.5rem;
        }
        /* SY Profile topic tags - green tint */
        .topic-tag {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
        }
        footer {
            margin-top: 3rem;
            padding-top: 1rem;
            border-top: 1px solid #eaecef;
            font-size: 0.9em;
            color: #6a737d;
        }
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            h1 {
                font-size: 2em;
            }
            h2 {
                font-size: 1.6em;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1 class="site-title">Beren's Blog</h1>
            <nav>
                <ul>
                  <li><a href="../index.html">Home</a></li>
                  <li><a href="../blog.html">Blog</a></li>
                  <li><a href="../gallery.html">Gallery</a></li>
                  <li><a href="../about.html">About</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="container">
        <article class="blog-post">
            <h2 class="post-title">SY眼动LLM研究日报</h2>
            <div class="post-meta">
                <span class="date">February 28, 2026</span>
                <span class="author">SY</span>
                <span class="label">眼动LLM研究</span>
            </div>

            <div id="table-of-contents">
              <h3>目录</h3>
              <ol>
                <li><a href="#daily-concept">每日概念</a></li>
                <li><a href="#papers">论文摘要</a></li>
                <li><a href="#references">参考链接</a></li>
              </ol>
            </div>

            <div class="post-content">
                <p>以下是 2026年02月28日 精选的眼动追踪与LLM交叉研究论文摘要。</p>

                <h2 id="daily-concept">每日概念: Eye tracking in multimodal AI</h2>
                <p><strong>1. 概念定义</strong><br>
多模态AI中的眼动追踪是指将眼动数据作为辅助模态或监督信号整合到深度学习模型中。这些数据通常包含注视点、瞳孔直径或扫视路径等信息，用于引导模型关注图像或文本的关键区域。通过模拟人类的视觉注意力机制，AI系统能够更有效地建立视觉输入与语言描述之间的关联。</p>

<p><strong>2. 核心原理</strong><br>
核心机制是将眼动信号转化为模型可学习的注意力先验或特征向量。在视觉语言模型中，人类注视图通常被用来指导Transformer的注意力权重分布。这往往通过在损失函数中增加正则化项来实现，例如最小化模型注意力图与人类注视分布之间的KL散度，公式为 \( L_{gaze} = D_{KL}(G_{human} || A_{model}) \)。这种方法强制模型关注人类认为重要的语义区域，从而优化特征对齐过程。</p>

<p><strong>3. 研究意义</strong><br>
这一研究方向对于提升多模态大模型的可解释性和减少幻觉现象至关重要。它提供了一种稀疏且高效的监督信号，帮助模型更准确地学习视觉与语言之间的细粒度对齐关系。通过引入人类的认知策略，模型能够模拟更自然的阅读和理解过程，从而在视觉问答和图像描述生成任务中表现更优。</p>

<p><strong>4. 关键洞见</strong><br>
眼动数据不仅是生理信号，更是连接人类认知过程与AI模型推理能力的桥梁。它将“人类关注什么”这一显性知识转化为模型“应该学习什么”的隐性指导。</p>

                <h2 id="papers">论文摘要</h2>

                <div class="paper-entry" id="paper-1">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.22774v1" target="_blank">Transformer Actor-Critic for Efficient Freshness-Aware Resource Allocation</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>作者:</strong> Maryam Ansarifard, Mohit K. Sharma, Kishor C. Joshi et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.22774v1" target="_blank">2602.22774v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>摘要</h4>
                    <p><strong>1. 研究问题</strong><br>
这篇论文旨在解决多用户上行非正交多址接入网络中的信息年龄最小化问题。核心挑战在于如何在满足用户异构性和NOMA约束的前提下，保障自动驾驶等应用所需的超可靠低延迟通信。</p>

<p><strong>2. 研究定位</strong><br>
该工作将大语言模型中的Transformer架构引入深度强化学习资源分配领域，填补了传统方法难以有效捕获用户间长距离依赖关系的空白。它通过注意力机制提升了策略的可扩展性，为下一代无线网络的智能化管理提供了新的技术路径。</p>

<p><strong>3. 核心贡献</strong><br>
论文提出了基于Transformer编码器的近端策略优化框架，利用注意力机制动态聚焦关键用户状态。研究创新性地可视化了注意力权重的演变过程，展示了模型如何学习模拟类似人类视觉关注的优先级策略，从而显著降低了平均信息年龄。</p>

<p><strong>4. 方法概述</strong><br>
该方法在Actor-Critic架构中引入Transformer编码器，通过自注意力机制处理用户状态序列。模型能够自动学习用户间的关联特征，并在复杂的NOMA调度约束下生成最优的资源分配策略。</p>

<p><strong>5. 局限与不足</strong><br>
Transformer结构通常伴随着较高的计算开销，这在毫秒级决策的通信场景中可能成为瓶颈。同时，实验主要基于仿真数据，缺乏真实无线环境下的验证，且注意力机制的可解释性分析仍停留在定性层面。</p>

<p><strong>6. 评价与思考</strong><br>
这项工作是LLM技术在通信资源管理领域的成功应用范例，证明了注意力机制不仅能处理自然语言，还能像眼动追踪筛选视觉信息一样筛选关键网络状态。这种将AI认知机制应用于物理层调度的尝试，为跨学科研究提供了极具价值的参考。</p>
                </div>
                <div class="paper-entry" id="paper-2">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.22368v1" target="_blank">EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>作者:</strong> Jiahao Zhang, Yifan Zhang, Kevin Leach et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.22368v1" target="_blank">2602.22368v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>摘要</h4>
                    <p><strong>1. 研究问题</strong><br>
这篇论文探讨如何利用人类在代码阅读过程中的眼动注视模式来增强大型语言模型（LLM）的代码摘要能力。核心研究问题是，人类的认知专业知识能否作为一种注意力先验，有效地弥补纯数据驱动模型在理解代码语义焦点时的不足。</p>

<p><strong>2. 研究定位</strong><br>
该工作位于软件工程、认知科学与大模型微调的交叉领域，旨在填补人类认知注意力机制与LLM内部自注意力机制之间的空白。现有研究多关注模型架构或纯文本训练数据，而本文创新性地将眼动追踪数据作为人类专业知识的代理引入模型，探索了认知信号增强LLM的新路径。</p>

<p><strong>3. 核心贡献</strong><br>
论文提出了EyeLayer，一个轻量级的注意力增强模块，能够无缝集成到不同架构的LLM中而不破坏原有表征。该方法在LLaMA-3.2、Qwen3和CodeBERT等多种模型上均取得了优于强基线的表现，其中BLEU-4指标最高提升了13.17%，证明了人类注视信号对模型语义聚焦能力的有效补充。</p>

<p><strong>4. 方法概述</strong><br>
EyeLayer利用多模态高斯混合模型对人类阅读代码时的注意力分布进行建模。该方法通过学习参数 \( (\mu_i, \sigma_i^2) \) 来捕捉开发者的注视位置和强度，并据此重新分配Token嵌入的权重。这种设计将眼动数据转化为可学习的注意力先验，以模块化的方式直接融入LLM的嵌入层或注意力计算过程中。</p>

<p><strong>5. 局限与不足</strong><br>
眼动追踪数据的获取通常需要昂贵的专用设备且数据规模相对有限，这可能限制了该方法在大规模数据集上的泛化能力。此外，高斯混合模型虽然能模拟注意力分布，但对于极其复杂的代码逻辑结构，简单的参数化建模可能无法完全捕捉人类认知的动态变化。不同开发者之间的眼动习惯差异也可能给模型的通用性带来挑战。</p>

<p><strong>6. 评价与思考</strong><br>
这项工作展示了将认知科学成果融入AI系统的巨大潜力，特别是在需要深度语义理解的代码任务中。其优点在于模块设计轻量且跨架构通用性强，为“以人为中心”的AI研究提供了有力证据。然而，未来的研究需要解决眼动数据稀缺的问题，例如探索合成眼动数据或无监督的注意力对齐方法，以降低实际应用门槛。</p>
                </div>
                <div class="paper-entry" id="paper-3">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.21983v1" target="_blank">Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>作者:</strong> Jingchao Wei, Jingkai Qin, Yuxiao Cao et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.21983v1" target="_blank">2602.21983v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>摘要</h4>
                    <p><strong>1. 研究问题</strong><br>
这篇论文旨在解决人形机器人在非结构化的人机交互中难以产生自然且符合情境的视线转移问题。核心挑战在于如何将认知层面的注意力机制与物理层面的仿生运动生成有效耦合。</p>

<p><strong>2. 研究定位</strong><br>
现有研究往往将视线目标选择与运动生成割裂处理，难以实现流畅自然的社交互动。该工作填补了这一空白，尝试构建一个统一的框架，使机器人能够像人类一样根据多模态线索自然地转移视线。</p>

<p><strong>3. 核心贡献</strong><br>
论文提出了RGS框架，主要创新点在于结合了视觉语言模型(VLM)进行符合人类认知的视线推理，以及利用条件VQ-VAE生成多样化的眼头协调运动。这种结合确保了视线转移在目标选择上的合理性和动作表现上的真实感。</p>

<p><strong>4. 方法概述</strong><br>
方法分为两个阶段。首先利用VLM处理多模态输入以推理出符合语境的注视目标。随后通过条件VQ-VAE模型生成眼动与头部协调的运动轨迹，从而实现类人的视线转移行为。</p>

<p><strong>5. 局限与不足</strong><br>
尽管摘要未详述局限，但此类方法通常面临VLM推理延迟可能影响实时交互体验的问题。同时，生成模型的多样性依赖于训练数据的覆盖范围，在极端未见过的交互场景中表现可能受限。</p>

<p><strong>6. 评价与思考</strong><br>
该研究展示了LLM技术在眼动追踪与机器人控制领域的巨大潜力，成功将语义理解转化为物理动作。它不仅解决了看哪里的问题，还深入探索了如何像人一样看，对提升社交机器人的自然度具有重要意义。</p>
                </div>

                <hr>

                <h3 id="references">参考链接</h3>
                <ul>
                    <li><a href="https://arxiv.org/abs/2602.22774v1" target="_blank">Transformer Actor-Critic for Efficient Freshness-Aware Resource Allocation</a></li>
                    <li><a href="https://arxiv.org/abs/2602.22368v1" target="_blank">EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization</a></li>
                    <li><a href="https://arxiv.org/abs/2602.21983v1" target="_blank">Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots</a></li>
                </ul>

                <p><em>欢迎讨论和反馈。感谢阅读!</em></p>
            </div>
        </article>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2026 SY. All rights reserved. | Gaze/LLM Research Daily Review</p>
        </div>
    </footer>
</body>
</html>