<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SY眼动LLM研究日报 - 2026-02-28</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['\\(','\\)']], displayMath: [['\\[','\\]']]}
        });
    </script>
    <style>
        body {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            font-size: 18px;
        }
        header {
            margin-bottom: 2rem;
            border-bottom: 1px solid #eaecef;
        }
        .site-title {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2em;
            margin-bottom: 0.5em;
        }
        nav ul {
            display: flex;
            list-style: none;
            padding: 0;
            margin: 1rem 0;
        }
        nav ul li {
            margin-right: 1.5rem;
        }
        nav ul li a {
            text-decoration: none;
            color: #0366d6;
            font-weight: 600;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        .container {
            width: 100%;
            max-width: 800px;
        }
        h1 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2.4em;
            margin-bottom: 0.5em;
            line-height: 1.2;
        }
        h2 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.8em;
            margin-top: 1.5em;
            margin-bottom: 0.75em;
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }
        h3 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.5em;
            margin-top: 1.2em;
            margin-bottom: 0.6em;
        }
        h4 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.2em;
            margin-top: 1em;
            margin-bottom: 0.5em;
        }
        p {
            margin: 1em 0;
        }
        code {
            font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
            background: #f6f8fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 0.9em;
        }
        pre {
            background: #f6f8fa;
            padding: 16px;
            border-radius: 6px;
            overflow: auto;
            font-size: 0.9em;
        }
        blockquote {
            margin: 1em 0;
            padding: 0 1em;
            color: #6a737d;
            border-left: 0.25em solid #dfe2e5;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1.5em auto;
        }
        .meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
        }
        .post-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
        }
        /* SY Profile specific styling - green tint */
        .label {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: 600;
        }
        .figure-caption {
            text-align: center;
            color: #666;
            font-size: 0.9em;
            margin-top: -1em;
            margin-bottom: 2em;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1.5em 0;
        }
        th, td {
            border: 1px solid #dfe2e5;
            padding: 8px 12px;
            text-align: left;
        }
        th {
            background-color: #f6f8fa;
        }
        tr:nth-child(even) {
            background-color: #f6f8fa;
        }
        #table-of-contents {
            background-color: #f0f7f0;
            padding: 1.5rem;
            border-radius: 5px;
            margin: 2rem 0;
            border-left: 4px solid #137333;
        }
        #table-of-contents h3 {
            margin-top: 0;
            margin-bottom: 1rem;
            color: #137333;
        }
        #table-of-contents ol, #table-of-contents ul {
            margin-bottom: 0;
        }
        #table-of-contents a {
            text-decoration: none;
            color: #0366d6;
        }
        #table-of-contents a:hover {
            text-decoration: underline;
        }
        .post-content {
            margin-top: 2rem;
        }
        .paper-entry {
            margin-bottom: 2.5rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid #eaecef;
        }
        .paper-entry:last-child {
            border-bottom: none;
        }
        .paper-title {
            font-size: 1.3em;
            margin-bottom: 0.5em;
        }
        .paper-title a {
            color: #0366d6;
            text-decoration: none;
        }
        .paper-title a:hover {
            text-decoration: underline;
        }
        .paper-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 1em;
        }
        .paper-topics {
            display: inline-flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 0.5rem;
        }
        /* SY Profile topic tags - green tint */
        .topic-tag {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
        }
        footer {
            margin-top: 3rem;
            padding-top: 1rem;
            border-top: 1px solid #eaecef;
            font-size: 0.9em;
            color: #6a737d;
        }
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            h1 {
                font-size: 2em;
            }
            h2 {
                font-size: 1.6em;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1 class="site-title">Beren's Blog</h1>
            <nav>
                <ul>
                  <li><a href="../index.html">Home</a></li>
                  <li><a href="../blog.html">Blog</a></li>
                  <li><a href="../gallery.html">Gallery</a></li>
                  <li><a href="../about.html">About</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="container">
        <article class="blog-post">
            <h2 class="post-title">SY眼动LLM研究日报</h2>
            <div class="post-meta">
                <span class="date">February 28, 2026</span>
                <span class="author">SY</span>
                <span class="label">眼动LLM研究</span>
            </div>

            <div id="table-of-contents">
              <h3>目录</h3>
              <ol>
                <li><a href="#daily-concept">每日概念</a></li>
                <li><a href="#papers">论文摘要</a></li>
                <li><a href="#references">参考链接</a></li>
              </ol>
            </div>

            <div class="post-content">
                <p>以下是 2026年02月28日 精选的眼动追踪与LLM交叉研究论文摘要。</p>

                <h2 id="daily-concept">每日概念: Eye tracking in multimodal AI</h2>
                <p><strong>概念定义</strong><br>
多模态AI中的眼动追踪是指将人类眼球运动数据作为一种模态输入或监督信号整合到深度学习模型中。这种方法利用注视点和眼跳轨迹等生理信号来指导模型关注关键信息区域，从而模拟人类的视觉注意力机制。</p>

<p><strong>核心原理</strong><br>
核心机制是将离散的眼动注视点转化为连续的注意力权重图，并将其与视觉或文本特征进行融合。具体而言，离散的注视点坐标通常通过高斯核函数转化为连续的显著图 \( S \)，公式为 \( S(x,y) = \sum_{i=1}^{N} \exp\left(-\frac{(x-x_i)^2 + (y-y_i)^2}{2\sigma^2}\right) \)。在训练阶段，模型通过最小化预测注意力分布与真实眼动数据之间的KL散度来对齐机器与人类的注意力机制。</p>

<p><strong>研究意义</strong><br>
在眼动追踪与LLM的交叉研究中，眼动数据提供了稀缺且高质量的人类先验知识，能够有效引导大模型对齐人类的认知过程。这种跨模态融合不仅提升了模型在视觉问答和阅读理解任务中的性能，还为解决大模型幻觉问题提供了基于人类注意力的纠偏机制。</p>

<p><strong>关键洞见</strong><br>
眼动追踪将隐性的认知过程显性化，为AI模型提供了超越传统文本标签的细粒度监督信号。</p>

                <h2 id="papers">论文摘要</h2>

                <div class="paper-entry" id="paper-1">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.16138v1" target="_blank">IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>作者:</strong> Parsa Madinei, Srijita Karmakar, Russell Cohen Hoffing et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.16138v1" target="_blank">2602.16138v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>摘要</h4>
                    <p><strong>1. 研究问题</strong><br>
这篇论文旨在解决大型视觉语言模型在开放式视觉问答中因问题歧义而导致的回答不准确问题。核心研究问题是如何利用人类实时的眼动注视数据来消除用户意图的不确定性，从而提升模型对模糊问题的理解能力。</p>

<p><strong>2. 研究定位</strong><br>
这项工作填补了人机交互中实时眼动信号与多模态大模型推理相结合的空白。它不同于以往依赖静态图像特征或需额外训练的方法，提出了一种无需训练的推理时干预策略。该研究将认知科学中的眼动机制引入VLM推理流程，拓展了多模态交互的研究边界。</p>

<p><strong>3. 核心贡献</strong><br>
论文提出了IRIS框架，这是一种无需训练的方法，通过实时眼动追踪显著提升了VLM处理歧义问题的能力。研究发现提问开始时刻附近的注视点最具信息量，能将歧义问题的准确率从35.2%大幅提升至77.2%。此外，团队发布了包含眼动数据的消歧VQA基准数据集及实时交互协议。</p>

<p><strong>4. 方法概述</strong><br>
IRIS采用一种推理时扫视机制，在模型推理阶段实时捕获用户的眼动数据。系统筛选出用户开始提问时刻附近的注视点，将其作为视觉提示输入到VLM中。这种方法无需修改模型参数，直接利用眼动信号引导模型关注图像中的相关区域，从而解析用户意图。</p>

<p><strong>5. 局限与不足</strong><br>
该方法高度依赖专业的眼动追踪硬件，可能限制了其在普通消费级设备上的普及应用。用户研究虽然有效，但在更复杂的动态视频场景下的泛化能力尚待验证。实时眼动数据的质量受个体差异和环境因素影响，可能引入额外的噪声干扰模型判断。</p>

<p><strong>6. 评价与思考</strong><br>
这是一项极具创新性的跨学科研究，巧妙地利用人类认知过程中的眼动信号来弥补模型在意图理解上的缺陷。其免训练的特性降低了应用门槛，展示了推理时干预策略的巨大潜力。未来的关键挑战在于如何将这种依赖专业硬件的方案转化为基于普通摄像头的低成本应用。</p>
                </div>
                <div class="paper-entry" id="paper-2">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.05132v1" target="_blank">ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>作者:</strong> Jia Li, Wenjie Zhao, Shijian Deng et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.05132v1" target="_blank">2602.05132v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>摘要</h4>
                    <p>### 1. 研究问题<br>
这篇论文旨在解决在线自我中心眼动追踪问题，即仅利用过去和当前的帧来预测相机佩戴者的注视点。核心挑战在于缺乏显式的头部或眼部信号，模型必须从稀疏线索中推断当前的视觉注意力。</p>

<p>### 2. 研究定位<br>
该研究定位于在线自我中心眼动追踪领域，区别于可使用未来帧的离线方法，它更符合增强现实(AR)和辅助技术的实时应用需求。它填补了利用时序依赖性进行在线预测的空白，将眼动追踪从单帧预测扩展到了序列预测问题。</p>

<p>### 3. 核心贡献<br>
论文提出了ARGaze模型，将眼动追踪重构为序列预测任务，借鉴了视觉语言模型中的自回归解码思想。主要创新在于引入了注视上下文窗口机制，利用历史注视信息作为先验，显著提升了在线场景下的预测精度并达到了最先进水平。</p>

<p>### 4. 方法概述<br>
ARGaze采用Transformer解码器架构，在每个时间步联合处理当前视觉特征与固定长度的历史注视上下文窗口。这种自回归设计强制了因果性约束，确保模型仅依赖过去信息进行预测，适合资源受限的流式推理。</p>

<p>### 5. 局限与不足<br>
由于采用自回归机制，模型可能存在误差累积的问题，即早期的错误预测可能影响后续结果。此外，固定长度的注视上下文窗口可能无法适应所有动态变化的任务场景，且对初始帧的预测可能因缺乏历史信息而精度较低。</p>

<p>### 6. 评价与思考<br>
这项工作巧妙地将大语言模型中的自回归生成范式迁移至眼动追踪领域，利用了注视行为的时序连续性先验，具有很强的借鉴意义。虽然方法有效，但如何平衡上下文窗口长度与计算效率，以及如何缓解自回归带来的误差传播，值得进一步探讨。</p>
                </div>
                <div class="paper-entry" id="paper-3">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.17848v1" target="_blank">On the scaling relationship between cloze probabilities and language model next-token prediction</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>作者:</strong> Cassandra L. Jacobs, Morgan Grobol<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.17848v1" target="_blank">2602.17848v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>摘要</h4>
                    <p><strong>1. 研究问题</strong><br>
这篇论文旨在探究语言模型规模增大如何影响其对人类阅读行为(如眼动追踪和阅读时间)的预测能力。核心问题在于揭示大模型在填空任务中表现更优的具体机制及其与人类认知处理的异同。</p>

<p><strong>2. 研究定位</strong><br>
这项工作位于心理语言学与大语言模型研究的交叉领域，延续了利用计算模型模拟人类阅读行为的传统。它填补了关于模型规模如何具体影响其对人类填空概率预测质量的认知空白，特别是区分了统计共现与语义理解的作用。</p>

<p><strong>3. 核心贡献</strong><br>
论文发现大模型虽然对人类反应的概率质量分配仍不足，但能提供更高质量的下一词元预测。核心贡献在于揭示了大模型通过更强的记忆能力提升语义匹配度，从而在填空任务中优于小模型，但同时也降低了对低级词汇统计信息的敏感度。</p>

<p><strong>4. 方法概述</strong><br>
研究通过对比不同规模语言模型的下一词元预测概率与人类填空概率，分析了二者的相关性。研究重点考察了模型预测与人类反应在语义一致性及词汇共现统计特征上的差异，以此解释模型缩放带来的性能变化。</p>

<p><strong>5. 局限与不足</strong><br>
研究指出即使是最优模型也未能给人类反应分配足够的概率质量，表明模型与人类认知过程仍存在显著差距。此外，大模型对低级词汇识别信息的敏感度降低，可能限制了其在某些精细阅读任务中的模拟精度。</p>

<p><strong>6. 评价与思考</strong><br>
这项工作为理解大语言模型的认知机制提供了重要视角，特别是区分了记忆容量与统计敏感度对阅读预测的影响。它提示我们在利用LLM模拟人类眼动追踪时，需要权衡模型规模带来的语义收益与低级特征丢失之间的矛盾。</p>
                </div>

                <hr>

                <h3 id="references">参考链接</h3>
                <ul>
                    <li><a href="https://arxiv.org/abs/2602.16138v1" target="_blank">IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models</a></li>
                    <li><a href="https://arxiv.org/abs/2602.05132v1" target="_blank">ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation</a></li>
                    <li><a href="https://arxiv.org/abs/2602.17848v1" target="_blank">On the scaling relationship between cloze probabilities and language model next-token prediction</a></li>
                </ul>

                <p><em>欢迎讨论和反馈。感谢阅读!</em></p>
            </div>
        </article>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2026 SY. All rights reserved. | Gaze/LLM Research Daily Review</p>
        </div>
    </footer>
</body>
</html>