<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SY眼动LLM研究日报 - 2026-02-28</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['\\(','\\)']], displayMath: [['\\[','\\]']]}
        });
    </script>
    <style>
        body {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            font-size: 18px;
        }
        header {
            margin-bottom: 2rem;
            border-bottom: 1px solid #eaecef;
        }
        .site-title {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2em;
            margin-bottom: 0.5em;
        }
        nav ul {
            display: flex;
            list-style: none;
            padding: 0;
            margin: 1rem 0;
        }
        nav ul li {
            margin-right: 1.5rem;
        }
        nav ul li a {
            text-decoration: none;
            color: #0366d6;
            font-weight: 600;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        .container {
            width: 100%;
            max-width: 800px;
        }
        h1 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2.4em;
            margin-bottom: 0.5em;
            line-height: 1.2;
        }
        h2 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.8em;
            margin-top: 1.5em;
            margin-bottom: 0.75em;
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }
        h3 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.5em;
            margin-top: 1.2em;
            margin-bottom: 0.6em;
        }
        h4 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.2em;
            margin-top: 1em;
            margin-bottom: 0.5em;
        }
        p {
            margin: 1em 0;
        }
        code {
            font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
            background: #f6f8fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 0.9em;
        }
        pre {
            background: #f6f8fa;
            padding: 16px;
            border-radius: 6px;
            overflow: auto;
            font-size: 0.9em;
        }
        blockquote {
            margin: 1em 0;
            padding: 0 1em;
            color: #6a737d;
            border-left: 0.25em solid #dfe2e5;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1.5em auto;
        }
        .meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
        }
        .post-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
        }
        /* SY Profile specific styling - green tint */
        .label {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: 600;
        }
        .figure-caption {
            text-align: center;
            color: #666;
            font-size: 0.9em;
            margin-top: -1em;
            margin-bottom: 2em;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1.5em 0;
        }
        th, td {
            border: 1px solid #dfe2e5;
            padding: 8px 12px;
            text-align: left;
        }
        th {
            background-color: #f6f8fa;
        }
        tr:nth-child(even) {
            background-color: #f6f8fa;
        }
        #table-of-contents {
            background-color: #f0f7f0;
            padding: 1.5rem;
            border-radius: 5px;
            margin: 2rem 0;
            border-left: 4px solid #137333;
        }
        #table-of-contents h3 {
            margin-top: 0;
            margin-bottom: 1rem;
            color: #137333;
        }
        #table-of-contents ol, #table-of-contents ul {
            margin-bottom: 0;
        }
        #table-of-contents a {
            text-decoration: none;
            color: #0366d6;
        }
        #table-of-contents a:hover {
            text-decoration: underline;
        }
        .post-content {
            margin-top: 2rem;
        }
        .paper-entry {
            margin-bottom: 2.5rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid #eaecef;
        }
        .paper-entry:last-child {
            border-bottom: none;
        }
        .paper-title {
            font-size: 1.3em;
            margin-bottom: 0.5em;
        }
        .paper-title a {
            color: #0366d6;
            text-decoration: none;
        }
        .paper-title a:hover {
            text-decoration: underline;
        }
        .paper-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 1em;
        }
        .paper-topics {
            display: inline-flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 0.5rem;
        }
        /* SY Profile topic tags - green tint */
        .topic-tag {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
        }
        footer {
            margin-top: 3rem;
            padding-top: 1rem;
            border-top: 1px solid #eaecef;
            font-size: 0.9em;
            color: #6a737d;
        }
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            h1 {
                font-size: 2em;
            }
            h2 {
                font-size: 1.6em;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1 class="site-title">Beren's Blog</h1>
            <nav>
                <ul>
                  <li><a href="../index.html">Home</a></li>
                  <li><a href="../blog.html">Blog</a></li>
                  <li><a href="../gallery.html">Gallery</a></li>
                  <li><a href="../about.html">About</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="container">
        <article class="blog-post">
            <h2 class="post-title">SY眼动LLM研究日报</h2>
            <div class="post-meta">
                <span class="date">February 28, 2026</span>
                <span class="author">SY</span>
                <span class="label">眼动LLM研究</span>
            </div>

            <div id="table-of-contents">
              <h3>目录</h3>
              <ol>
                <li><a href="#daily-concept">每日概念</a></li>
                <li><a href="#papers">论文摘要</a></li>
                <li><a href="#references">参考链接</a></li>
              </ol>
            </div>

            <div class="post-content">
                <p>以下是 2026年02月28日 精选的眼动追踪与LLM交叉研究论文摘要。</p>

                <h2 id="daily-concept">每日概念: Eye tracking in multimodal AI</h2>
                <p><strong>1. 概念定义</strong><br>
这是指将眼动数据作为一种生物信号模态集成到多模态人工智能系统中，以模拟或增强模型对视觉信息的处理能力。通过引入注视点、眼跳轨迹和瞳孔直径等数据，AI系统能够获得类似人类的视觉注意力机制，从而优化对图像和文本信息的理解与交互。</p>

<p><strong>2. 核心原理</strong><br>
核心机制在于将眼动信号转化为注意力先验，并以此指导模型对视觉或文本特征的加权处理。在典型的多模态模型中，眼动数据通常被编码为空间权重图，用于调制视觉特征提取过程。数学上，这可以表示为特征图 \( V \) 与注视概率图 \( G \) 的加权融合，即 \( V' = V \odot G \)，其中 \( \odot \) 代表逐元素乘法。这种方法利用人类的认知显著性来抑制背景噪声，使模型聚焦于关键信息区域。</p>

<p><strong>3. 研究意义</strong><br>
在眼动与LLM的交叉研究中，这一技术为解决长上下文注意力分散问题提供了生物学启发，能够显著降低模型的计算开销并提升推理性能。它不仅让LLM具备了模拟人类阅读模式的能力，还能通过眼动数据作为隐式反馈信号来优化文本生成质量。此外，这种融合有助于构建更符合人类认知习惯的可解释AI系统。</p>

<p><strong>4. 关键洞见</strong><br>
眼动数据提供了人类认知过程的“地面实况”，它将生物注意力机制直接转化为模型可学习的先验知识，从而在机器智能与人类认知之间架起了一座桥梁。</p>

                <h2 id="papers">论文摘要</h2>

                <div class="paper-entry" id="paper-1">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.22774v1" target="_blank">Transformer Actor-Critic for Efficient Freshness-Aware Resource Allocation</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>作者:</strong> Maryam Ansarifard, Mohit K. Sharma, Kishor C. Joshi et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.22774v1" target="_blank">2602.22774v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>摘要</h4>
                    <p><strong>1. 研究问题</strong><br>
这篇论文旨在解决多用户上行非正交多址接入网络中的信息年龄最小化问题。核心挑战在于如何处理用户异构性，包括任务大小、AoI阈值和惩罚敏感度的差异，同时满足严格的NOMA调度约束。</p>

<p><strong>2. 研究定位</strong><br>
这项工作位于深度强化学习与下一代无线通信资源管理的交叉领域。它填补了现有研究在处理复杂用户异构性和系统可扩展性方面的空白，特别是引入了源自大语言模型领域的Transformer架构来解决无线资源分配问题。</p>

<p><strong>3. 核心贡献</strong><br>
论文提出了一种基于近端策略优化的深度强化学习框架，并创新性地集成了Transformer编码器。主要贡献在于利用注意力机制捕捉用户间的依赖关系，并通过注意力图的演变分析，证明了模型能够自主学习并优先关注高重要性的用户。</p>

<p><strong>4. 方法概述</strong><br>
该方法构建了一个基于Transformer编码器的Actor-Critic网络架构。通过自注意力机制，智能体能够动态聚焦于关键用户状态并捕捉用户间的隐式关联，从而在复杂的NOMA约束下输出更优的资源分配策略。</p>

<p><strong>5. 局限与不足</strong><br>
尽管仿真结果积极，但在实际部署中，Transformer架构较高的计算复杂度可能对实时性要求极高的URLLC系统构成挑战。此外，研究主要基于仿真环境，模型在真实物理信道衰落和突发干扰下的鲁棒性仍需进一步验证。</p>

<p><strong>6. 评价与思考</strong><br>
这项工作展示了大语言模型中的Transformer技术在通信资源分配领域的巨大潜力，证明了注意力机制在捕捉系统状态依赖性方面的有效性。文中对注意力权重演变的分析类似于眼动追踪研究中的注视点分析，早期均匀分布到后期聚焦关键目标的模式，为理解AI决策逻辑提供了类似视觉认知的直观解释。</p>
                </div>
                <div class="paper-entry" id="paper-2">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.22368v1" target="_blank">EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>作者:</strong> Jiahao Zhang, Yifan Zhang, Kevin Leach et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.22368v1" target="_blank">2602.22368v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>摘要</h4>
                    <p><strong>1. 研究问题</strong><br>
这篇论文探讨了如何将人类在代码理解过程中的专业知识引入大语言模型。核心研究问题是，人类的眼动注意力模式能否作为先验知识，有效增强LLM在代码摘要任务中的表现。</p>

<p><strong>2. 研究定位</strong><br>
该工作位于代码摘要与人机交互研究的交叉领域。现有研究主要关注模型架构优化或大规模预训练，较少探索人类认知信号对模型注意力的指导作用。本文填补了这一空白，提出了一种将人类视觉注意力无缝集成到LLM推理过程中的轻量级方案。</p>

<p><strong>3. 核心贡献</strong><br>
论文提出了EyeLayer模块，这是一种轻量级的注意力增强机制，能够将人类眼动数据转化为可学习的注意力先验。该工作的创新在于利用多模态高斯混合模型重新分配Token嵌入，在不破坏原有模型结构的前提下实现了显著性能提升，BLEU-4指标最高提升了13.17%。</p>

<p><strong>4. 方法概述</strong><br>
EyeLayer通过多模态高斯混合模型对人类阅读代码时的眼动轨迹进行建模，捕捉开发者关注的焦点位置和强度。该方法利用学习到的参数 \((\mu_i, \sigma_i^2)\) 对Token嵌入进行重新分配和加权，从而将人类注意力模式无缝注入到LLM的表示层中。</p>

<p><strong>5. 局限与不足</strong><br>
该研究依赖于高质量的眼动追踪数据，这可能限制了其在缺乏此类数据的新编程语言或特定场景中的应用。此外，虽然方法具有通用性，但实验主要集中在代码摘要任务，其在代码生成或错误检测等其他下游任务中的效果尚需验证。</p>

<p><strong>6. 评价与思考</strong><br>
这是一项极具启发性的工作，成功搭建了认知科学与深度学习之间的桥梁。其优点在于方法设计轻量且通用，能够跨不同架构的模型(如LLaMA和CodeBERT)生效。然而，眼动数据的采集成本较高，未来研究可能需要探索如何利用合成数据来降低对真实眼动数据的依赖。</p>
                </div>
                <div class="paper-entry" id="paper-3">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.21983v1" target="_blank">Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>作者:</strong> Jingchao Wei, Jingkai Qin, Yuxiao Cao et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.21983v1" target="_blank">2602.21983v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>摘要</h4>
                    <p><strong>1. 研究问题</strong><br>
这篇论文旨在解决仿人机器人在非受限人机交互中难以产生自然且符合情境的视线转移问题。核心挑战在于如何将认知注意力机制与仿生运动生成进行有效耦合。</p>

<p><strong>2. 研究定位</strong><br>
该工作定位于填补认知建模与运动生成之间脱节的空白。现有研究往往孤立处理这两个环节，而本文通过统一框架实现了从多模态感知到类人动作输出的闭环。</p>

<p><strong>3. 核心贡献</strong><br>
论文主要贡献在于提出了RGS框架，利用视觉语言模型(VLM)实现符合人类规律的目标选择。同时引入条件VQ-VAE模型生成多样化的眼头协调动作，显著提升了交互的自然度。</p>

<p><strong>4. 方法概述</strong><br>
方法分为两个阶段。首先通过VLM处理视听线索推理出合理的注视目标。接着利用条件VQ-VAE模型生成眼头协调的运动轨迹，模拟人类多样的视线转移行为。</p>

<p><strong>5. 局限与不足</strong><br>
潜在局限可能包括VLM推理带来的计算延迟，影响实时交互体验。此外，运动生成模型的效果可能受限于训练数据的覆盖范围，难以泛化至所有极端交互场景。</p>

<p><strong>6. 评价与思考</strong><br>
这是一项将大模型语义理解能力与眼动追踪技术深度融合的优秀工作。它突破了传统规则驱动方法的局限，为具身智能中的自然人机交互提供了极具启发性的解决方案。</p>
                </div>
                <div class="paper-entry" id="paper-4">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.20408v1" target="_blank">Examining and Addressing Barriers to Diversity in LLM-Generated Ideas</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>作者:</strong> Yuting Deng, Melanie Brucks, Olivier Toubia<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.20408v1" target="_blank">2602.20408v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>摘要</h4>
                    <p>以下是针对论文《Examining and Addressing Barriers to Diversity in LLM-Generated Ideas》的结构化中文总结。</p>

<p>1. <strong>研究问题</strong><br>
这篇论文旨在解决大语言模型(LLM)生成的想法多样性低于人类独立样本的问题，探讨这种同质化趋势是否会损害社会层面的创新能力。核心研究问题是识别导致LLM想法多样性受限的认知机制，并寻找能够有效提升多样性的干预方法。</p>

<p>2. <strong>研究定位</strong><br>
这项工作位于认知心理学与大语言模型研究的交叉领域，填补了关于LLM想法生成机制的理论空白。它将人类的认知固着和知识分区概念引入AI研究，解释了为何LLM在群体层面倾向于产生同质化内容，从而扩展了对AI创新能力的理解。</p>

<p>3. <strong>核心贡献</strong><br>
论文的主要贡献在于从理论和实证角度识别了阻碍LLM多样性的两个机制，即个体层面的认知固着和集体层面的知识聚合。创新之处在于提出了针对性的提示工程解决方案，即利用思维链缓解固着，利用普通人设优化知识分区，成功使LLM生成的想法多样性超越了人类基准。</p>

<p>4. <strong>方法概述</strong><br>
研究通过四个实验研究，分别测试了思维链提示和普通人设提示对想法多样性的影响。思维链提示通过结构化推理减少早期输出对后续生成的约束，而普通人设则作为多样的采样线索，引导模型锚定语义空间的不同区域。</p>

<p>5. <strong>局限与不足</strong><br>
虽然摘要未明确列出局限，但潜在的不足可能包括思维链提示带来的计算成本增加，以及普通人设选择对生成结果的敏感性。此外，该方法在不同领域或任务类型中的泛化能力仍需进一步验证。</p>

<p>6. <strong>评价与思考</strong><br>
这项工作极具价值，因为它将认知心理学中的“固着”概念（这一概念常通过眼动追踪研究视觉注意力锁定）应用于LLM的生成过程分析。优点在于理论框架扎实且解决方案实用，为构建健康的AI辅助创新生态系统提供了重要指导，避免了单纯追求效率而牺牲创新所需的多样性。</p>
                </div>

                <hr>

                <h3 id="references">参考链接</h3>
                <ul>
                    <li><a href="https://arxiv.org/abs/2602.22774v1" target="_blank">Transformer Actor-Critic for Efficient Freshness-Aware Resource Allocation</a></li>
                    <li><a href="https://arxiv.org/abs/2602.22368v1" target="_blank">EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization</a></li>
                    <li><a href="https://arxiv.org/abs/2602.21983v1" target="_blank">Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots</a></li>
                    <li><a href="https://arxiv.org/abs/2602.20408v1" target="_blank">Examining and Addressing Barriers to Diversity in LLM-Generated Ideas</a></li>
                </ul>

                <p><em>欢迎讨论和反馈。感谢阅读!</em></p>
            </div>
        </article>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2026 SY. All rights reserved. | Gaze/LLM Research Daily Review</p>
        </div>
    </footer>
</body>
</html>