<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SY&#39;s Gaze LLM Daily Review - 2026-02-27</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['\\(','\\)']], displayMath: [['\\[','\\]']]}
        });
    </script>
    <style>
        body {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            font-size: 18px;
        }
        header {
            margin-bottom: 2rem;
            border-bottom: 1px solid #eaecef;
        }
        .site-title {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2em;
            margin-bottom: 0.5em;
        }
        nav ul {
            display: flex;
            list-style: none;
            padding: 0;
            margin: 1rem 0;
        }
        nav ul li {
            margin-right: 1.5rem;
        }
        nav ul li a {
            text-decoration: none;
            color: #0366d6;
            font-weight: 600;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        .container {
            width: 100%;
            max-width: 800px;
        }
        h1 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2.4em;
            margin-bottom: 0.5em;
            line-height: 1.2;
        }
        h2 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.8em;
            margin-top: 1.5em;
            margin-bottom: 0.75em;
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }
        h3 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.5em;
            margin-top: 1.2em;
            margin-bottom: 0.6em;
        }
        h4 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.2em;
            margin-top: 1em;
            margin-bottom: 0.5em;
        }
        p {
            margin: 1em 0;
        }
        code {
            font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
            background: #f6f8fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 0.9em;
        }
        pre {
            background: #f6f8fa;
            padding: 16px;
            border-radius: 6px;
            overflow: auto;
            font-size: 0.9em;
        }
        blockquote {
            margin: 1em 0;
            padding: 0 1em;
            color: #6a737d;
            border-left: 0.25em solid #dfe2e5;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1.5em auto;
        }
        .meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
        }
        .post-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
        }
        /* SY Profile specific styling - green tint */
        .label {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: 600;
        }
        .figure-caption {
            text-align: center;
            color: #666;
            font-size: 0.9em;
            margin-top: -1em;
            margin-bottom: 2em;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1.5em 0;
        }
        th, td {
            border: 1px solid #dfe2e5;
            padding: 8px 12px;
            text-align: left;
        }
        th {
            background-color: #f6f8fa;
        }
        tr:nth-child(even) {
            background-color: #f6f8fa;
        }
        #table-of-contents {
            background-color: #f0f7f0;
            padding: 1.5rem;
            border-radius: 5px;
            margin: 2rem 0;
            border-left: 4px solid #137333;
        }
        #table-of-contents h3 {
            margin-top: 0;
            margin-bottom: 1rem;
            color: #137333;
        }
        #table-of-contents ol, #table-of-contents ul {
            margin-bottom: 0;
        }
        #table-of-contents a {
            text-decoration: none;
            color: #0366d6;
        }
        #table-of-contents a:hover {
            text-decoration: underline;
        }
        .post-content {
            margin-top: 2rem;
        }
        .paper-entry {
            margin-bottom: 2.5rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid #eaecef;
        }
        .paper-entry:last-child {
            border-bottom: none;
        }
        .paper-title {
            font-size: 1.3em;
            margin-bottom: 0.5em;
        }
        .paper-title a {
            color: #0366d6;
            text-decoration: none;
        }
        .paper-title a:hover {
            text-decoration: underline;
        }
        .paper-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 1em;
        }
        .paper-topics {
            display: inline-flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 0.5rem;
        }
        /* SY Profile topic tags - green tint */
        .topic-tag {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
        }
        footer {
            margin-top: 3rem;
            padding-top: 1rem;
            border-top: 1px solid #eaecef;
            font-size: 0.9em;
            color: #6a737d;
        }
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            h1 {
                font-size: 2em;
            }
            h2 {
                font-size: 1.6em;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1 class="site-title">Beren's Blog</h1>
            <nav>
                <ul>
                  <li><a href="../index.html">Home</a></li>
                  <li><a href="../blog.html">Blog</a></li>
                  <li><a href="../gallery.html">Gallery</a></li>
                  <li><a href="../about.html">About</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="container">
        <article class="blog-post">
            <h2 class="post-title">SY&#39;s Gaze LLM Daily Review</h2>
            <div class="post-meta">
                <span class="date">February 27, 2026</span>
                <span class="author">SY</span>
                <span class="label">Gaze LLM Research</span>
            </div>

            <div id="table-of-contents">
              <h3>Table of Contents</h3>
              <ol>
                <li><a href="#daily-concept">Daily Concept</a></li>
                <li><a href="#papers">Paper Summaries</a></li>
                <li><a href="#references">References</a></li>
              </ol>
            </div>

            <div class="post-content">
                <p>Here are selected research papers at the intersection of gaze/eye-tracking and LLMs for February 27, 2026.</p>

                <h2 id="daily-concept">Daily Concept: Cognitive load measurement via eye tracking</h2>
                <p><strong>1. What is it?</strong><br>
Cognitive load measurement via eye tracking is the process of quantifying the mental effort a user exerts while processing information by analyzing their eye movements and physiological signals. In the context of AI, it serves as an implicit feedback mechanism to evaluate how difficult it is for a human to read or comprehend text generated by Large Language Models (LLMs). This allows researchers to assess usability and difficulty objectively without relying on subjective user surveys.</p>

<p><strong>2. How does it work?</strong><br>
The method operates on the psychophysiological principle that increased mental effort correlates with specific ocular behaviors. Common indicators include pupillary dilation, longer fixation durations, and a higher frequency of regression saccades, which occur when the eye moves back to re-read previous text. A machine learning model often aggregates these temporal and spatial features into a feature vector \( X \) to predict a load score \( y \), typically formulated as \( y = f(X; \theta) \). By monitoring these signals in real time, the system infers the user's instantaneous cognitive state.</p>

<p><strong>3. Why does it matter?</strong><br>
This measurement is significant for developing human-centered AI systems that prioritize user experience alongside output accuracy. It enables LLMs to dynamically adapt their output complexity or provide assistance when a user shows signs of being overwhelmed. Consequently, this facilitates the creation of adaptive interfaces that optimize communication strategies to minimize cognitive friction.</p>

<p><strong>4. Key insight</strong><br>
Eye tracking acts as a passive, continuous sensor for user confusion, turning the human reader into an implicit annotator for LLM quality. This allows models to optimize for human comprehension, not just semantic correctness.</p>

                <h2 id="papers">Paper Summaries</h2>

                <div class="paper-entry" id="paper-1">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.22774v1" target="_blank">Transformer Actor-Critic for Efficient Freshness-Aware Resource Allocation</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Maryam Ansarifard, Mohit K. Sharma, Kishor C. Joshi et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.22774v1" target="_blank">2602.22774v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p>Here is a structured summary of the research paper.</p>

<p><strong>1. Research Question</strong><br>
How can we minimize the Age of Information (AoI) in multi-user uplink wireless networks that use non-orthogonal multiple access (NOMA)? The paper specifically investigates how to handle user heterogeneity and strict latency constraints while optimizing scheduling decisions.</p>

<p><strong>2. Positioning</strong><br>
This work positions itself at the intersection of deep reinforcement learning (DRL) and next-generation wireless networking. It addresses the limitations of traditional DRL methods in handling complex, scalable environments with inter-user dependencies. The authors fill a gap by adapting the Transformer architecture, typically used in Natural Language Processing (NLP), to solve resource allocation problems in ultra-reliable low-latency communication (URLLC) scenarios.</p>

<p><strong>3. Key Contribution</strong><br>
The primary contribution is a novel Proximal Policy Optimization (PPO) framework enhanced with a Transformer encoder. This approach leverages the attention mechanism to dynamically focus on critical user states and capture dependencies between users. The paper also provides an interpretability analysis, demonstrating how the model learns to prioritize high-importance users through the evolution of attention weights.</p>

<p><strong>4. Methodology</strong><br>
The authors model the wireless scheduling problem as a Markov Decision Process and solve it using an Actor-Critic architecture. They replace traditional neural network layers with a Transformer encoder, allowing the agent to process user states as a sequence. This enables the self-attention mechanism to weigh the importance of different users, effectively learning optimal scheduling policies that adhere to NOMA constraints.</p>

<p><strong>5. Limitations</strong><br>
The evaluation relies entirely on simulations, which may not fully capture the stochastic nature and hardware constraints of real-world wireless environments. While the Transformer improves performance, it introduces higher computational overhead compared to simpler neural architectures, potentially complicating deployment on edge devices. The study also assumes a specific NOMA setting, which may limit generalizability to other multiple access schemes.</p>

<p><strong>6. Critical Evaluation</strong><br>
This paper offers a compelling example of how techniques from LLM research, specifically the attention mechanism, can be successfully transferred to wireless resource allocation. The visualization of attention maps provides a level of interpretability often missing in deep learning, functioning similarly to a "gaze" analysis where the model reveals which inputs drive its decisions. While the results are impressive, the practical value would be strengthened by a discussion on the inference latency of the Transformer model in real-time systems.</p>
                </div>
                <div class="paper-entry" id="paper-2">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.22368v1" target="_blank">EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Jiahao Zhang, Yifan Zhang, Kevin Leach et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.22368v1" target="_blank">2602.22368v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p><strong>1. Research Question</strong><br>
The paper investigates whether human cognitive signals, specifically eye-gaze patterns, can be integrated into Large Language Models (LLMs) to improve the quality of automated code summarization. It asks if human attention serves as an effective prior to guide model focus during code comprehension tasks.</p>

<p><strong>2. Positioning</strong><br>
While LLMs have achieved high performance in code summarization, current methods rely solely on data-driven attention mechanisms that may misalign with how human experts read and understand code. This work bridges the gap between cognitive science and AI by treating eye-tracking data as a proxy for human expertise, aiming to align model attention with developer attention.</p>

<p><strong>3. Key Contribution</strong><br>
The primary contribution is EyeLayer, a lightweight module that seamlessly injects human gaze information into various LLM architectures without disrupting their pre-trained representations. The authors demonstrate that this human-centric signal provides complementary information, resulting in performance gains of up to 13.17% BLEU-4 across diverse model families like LLaMA and Qwen.</p>

<p><strong>4. Methodology</strong><br>
EyeLayer models human visual attention using a Multimodal Gaussian Mixture, utilizing learned parameters \((\mu_i, \sigma_i^2)\) to represent the location and intensity of developer focus. This mathematical model redistributes token embeddings to create an attention prior, which is fused with the LLM's existing embeddings to emphasize semantically important code tokens.</p>

<p><strong>5. Limitations</strong><br>
The approach relies on the availability of high-quality eye-tracking datasets, which are resource-intensive to collect and may not exist for all programming languages or contexts. Additionally, the use of Gaussian distributions to model attention creates a probabilistic approximation that might not fully capture the complex, dynamic temporal aspects of human reading behavior.</p>

<p><strong>6. Critical Evaluation</strong><br>
This work presents a compelling proof-of-concept for "cognitively-augmented" AI, showing that biological signals can correct or enhance machine attention mechanisms. The primary strength lies in the module's architecture-agnostic design, though the dependency on external gaze data currently limits its applicability to domains where such specialized hardware and studies are available.</p>
                </div>
                <div class="paper-entry" id="paper-3">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.21983v1" target="_blank">Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Jingchao Wei, Jingkai Qin, Yuxiao Cao et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.21983v1" target="_blank">2602.21983v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p><strong>1. Research Question</strong><br>
How can humanoid robots perform natural and context appropriate gaze shifts during unconstrained human robot interaction? The paper addresses the specific challenge of coupling cognitive attention mechanisms for target selection with biomimetic motion generation for physical execution.</p>

<p><strong>2. Positioning</strong><br>
Existing literature often treats gaze target selection and motion generation as separate problems, leading to robotic behaviors that feel unnatural or lack context awareness. This work positions itself at the intersection of cognitive AI and robotics, aiming to bridge the gap between high level semantic reasoning and low level motor control.</p>

<p><strong>3. Key Contribution</strong><br>
The primary contribution is the Robot Gaze Shift (RGS) framework, which unifies gaze reasoning and motion generation into a single pipeline. The paper introduces a novel application of Vision Language Models (VLMs) to determine where to look, and a conditional VQ-VAE model to generate diverse, human like eye head coordination patterns.</p>

<p><strong>4. Methodology</strong><br>
The proposed framework operates in two stages. First, it utilizes a VLM to process multimodal inputs and infer context appropriate gaze targets that align with human social norms. Second, it employs a conditional Vector Quantized Variational Autoencoder (VQ-VAE) to synthesize the physical gaze shift trajectories, ensuring the resulting motion is realistic and varied.</p>

<p><strong>5. Limitations</strong><br>
While the abstract validates the framework, potential limitations include the computational latency of running VLMs in real time scenarios and the risk of the model hallucinating inappropriate gaze targets. Additionally, the diversity of the generated motion is inherently bounded by the specific human demonstration data used to train the VQ-VAE.</p>

<p><strong>6. Critical Evaluation</strong><br>
This work represents a significant advancement in applying Large Language Model technologies to nonverbal robot behavior, moving beyond simple pre-programmed gestures. The integration of generative models for motion synthesis is a particular strength, as it avoids the repetitive nature of traditional rule based systems, though the system's robustness in highly noisy or chaotic real world environments remains to be fully seen.</p>
                </div>

                <hr>

                <h3 id="references">References</h3>
                <ul>
                    <li><a href="https://arxiv.org/abs/2602.22774v1" target="_blank">Transformer Actor-Critic for Efficient Freshness-Aware Resource Allocation</a></li>
                    <li><a href="https://arxiv.org/abs/2602.22368v1" target="_blank">EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization</a></li>
                    <li><a href="https://arxiv.org/abs/2602.21983v1" target="_blank">Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots</a></li>
                </ul>

                <p><em>Feel free to reach out for discussion. Thanks for reading!</em></p>
            </div>
        </article>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2026 SY. All rights reserved. | Gaze/LLM Research Daily Review</p>
        </div>
    </footer>
</body>
</html>