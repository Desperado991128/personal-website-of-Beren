<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SY&#39;s Gaze LLM Daily Review - 2026-02-27</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['\\(','\\)']], displayMath: [['\\[','\\]']]}
        });
    </script>
    <style>
        body {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            font-size: 18px;
        }
        header {
            margin-bottom: 2rem;
            border-bottom: 1px solid #eaecef;
        }
        .site-title {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2em;
            margin-bottom: 0.5em;
        }
        nav ul {
            display: flex;
            list-style: none;
            padding: 0;
            margin: 1rem 0;
        }
        nav ul li {
            margin-right: 1.5rem;
        }
        nav ul li a {
            text-decoration: none;
            color: #0366d6;
            font-weight: 600;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        .container {
            width: 100%;
            max-width: 800px;
        }
        h1 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2.4em;
            margin-bottom: 0.5em;
            line-height: 1.2;
        }
        h2 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.8em;
            margin-top: 1.5em;
            margin-bottom: 0.75em;
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }
        h3 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.5em;
            margin-top: 1.2em;
            margin-bottom: 0.6em;
        }
        h4 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.2em;
            margin-top: 1em;
            margin-bottom: 0.5em;
        }
        p {
            margin: 1em 0;
        }
        code {
            font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
            background: #f6f8fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 0.9em;
        }
        pre {
            background: #f6f8fa;
            padding: 16px;
            border-radius: 6px;
            overflow: auto;
            font-size: 0.9em;
        }
        blockquote {
            margin: 1em 0;
            padding: 0 1em;
            color: #6a737d;
            border-left: 0.25em solid #dfe2e5;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1.5em auto;
        }
        .meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
        }
        .post-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
        }
        /* SY Profile specific styling - green tint */
        .label {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: 600;
        }
        .figure-caption {
            text-align: center;
            color: #666;
            font-size: 0.9em;
            margin-top: -1em;
            margin-bottom: 2em;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1.5em 0;
        }
        th, td {
            border: 1px solid #dfe2e5;
            padding: 8px 12px;
            text-align: left;
        }
        th {
            background-color: #f6f8fa;
        }
        tr:nth-child(even) {
            background-color: #f6f8fa;
        }
        #table-of-contents {
            background-color: #f0f7f0;
            padding: 1.5rem;
            border-radius: 5px;
            margin: 2rem 0;
            border-left: 4px solid #137333;
        }
        #table-of-contents h3 {
            margin-top: 0;
            margin-bottom: 1rem;
            color: #137333;
        }
        #table-of-contents ol, #table-of-contents ul {
            margin-bottom: 0;
        }
        #table-of-contents a {
            text-decoration: none;
            color: #0366d6;
        }
        #table-of-contents a:hover {
            text-decoration: underline;
        }
        .post-content {
            margin-top: 2rem;
        }
        .paper-entry {
            margin-bottom: 2.5rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid #eaecef;
        }
        .paper-entry:last-child {
            border-bottom: none;
        }
        .paper-title {
            font-size: 1.3em;
            margin-bottom: 0.5em;
        }
        .paper-title a {
            color: #0366d6;
            text-decoration: none;
        }
        .paper-title a:hover {
            text-decoration: underline;
        }
        .paper-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 1em;
        }
        .paper-topics {
            display: inline-flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 0.5rem;
        }
        /* SY Profile topic tags - green tint */
        .topic-tag {
            background-color: #e6f4ea;
            color: #137333;
            padding: 0.2em 0.6em;
            border-radius: 3px;
            font-size: 0.85em;
        }
        footer {
            margin-top: 3rem;
            padding-top: 1rem;
            border-top: 1px solid #eaecef;
            font-size: 0.9em;
            color: #6a737d;
        }
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            h1 {
                font-size: 2em;
            }
            h2 {
                font-size: 1.6em;
            }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1 class="site-title">SY's Blog</h1>
            <nav>
                <ul>
                  <li><a href="../index.html">Home</a></li>
                  <li><a href="../blog.html">Blog</a></li>
                  <li><a href="../gallery.html">Gallery</a></li>
                  <li><a href="../about.html">About</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="container">
        <article class="blog-post">
            <h2 class="post-title">SY&#39;s Gaze LLM Daily Review</h2>
            <div class="post-meta">
                <span class="date">February 27, 2026</span>
                <span class="author">SY</span>
                <span class="label">Gaze LLM Research</span>
            </div>

            <div id="table-of-contents">
              <h3>Table of Contents</h3>
              <ol>
                <li><a href="#daily-concept">Daily Concept</a></li>
                <li><a href="#papers">Paper Summaries</a></li>
                <li><a href="#references">References</a></li>
              </ol>
            </div>

            <div class="post-content">
                <p>Here are selected research papers at the intersection of gaze/eye-tracking and LLMs for February 27, 2026.</p>

                <h2 id="daily-concept">Daily Concept: Cognitive load measurement via eye tracking</h2>
                <p><strong>1. What is it?</strong><br>
Cognitive load measurement via eye tracking is the quantification of mental effort a user exerts while processing information, inferred through physiological eye metrics. In the context of LLMs, this involves analyzing gaze patterns to assess how difficult it is for a user to comprehend generated text or interact with an interface. This process transforms subjective mental strain into an objective, implicit feedback signal for model evaluation.</p>

<p><strong>2. How does it work?</strong><br>
The method relies on the premise that increased mental demand correlates with specific ocular behaviors, such as longer fixation durations and increased pupil dilation. Researchers extract features like average fixation duration \( \bar{F} \) and pupil diameter \( D \) from raw gaze data streams. These features serve as input vectors to machine learning models that predict a continuous load value or classify cognitive states. For example, a regression model might estimate load \( L \) as a function of these gaze features, such as \( L = \beta_0 + \beta_1 \bar{F} + \beta_2 D + \epsilon \).</p>

<p><strong>3. Why does it matter?</strong><br>
It provides an objective way to evaluate the quality of LLM outputs beyond traditional text similarity metrics like BLEU or ROUGE scores. By measuring cognitive load, researchers can fine-tune LLMs to generate text that is not only factually correct but also cognitively ergonomic for the human reader. This creates a pathway for user-centric optimization where models learn to minimize user confusion or mental fatigue during interaction.</p>

<p><strong>4. Key insight</strong><br>
Eye tracking acts as a passive, real-time sensor that bridges the gap between human mental states and machine learning optimization objectives.</p>

                <h2 id="papers">Paper Summaries</h2>

                <div class="paper-entry" id="paper-1">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.22774v1" target="_blank">Transformer Actor-Critic for Efficient Freshness-Aware Resource Allocation</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Maryam Ansarifard, Mohit K. Sharma, Kishor C. Joshi et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.22774v1" target="_blank">2602.22774v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p>Here is a structured summary of the research paper.</p>

<p><strong>1. Research Question</strong><br>
The paper aims to solve the problem of minimizing the Age of Information (AoI) in multi-user wireless networks that use non-orthogonal multiple access (NOMA). It specifically asks how a learning agent can efficiently schedule heterogeneous users with different urgency levels and task sizes while adhering to strict transmission constraints.</p>

<p><strong>2. Positioning</strong><br>
Existing wireless resource allocation methods often struggle to handle the complex interdependencies and scalability required for ultra-reliable low-latency communication (URLLC). This work positions itself at the intersection of deep reinforcement learning and wireless networking, filling a gap by introducing attention mechanisms to manage user heterogeneity and dynamic constraints more effectively than traditional heuristic or standard neural network approaches.</p>

<p><strong>3. Key Contribution</strong><br>
The primary contribution is a Proximal Policy Optimization (PPO) framework enhanced with a Transformer encoder for wireless scheduling. This approach is novel because it leverages the self-attention mechanism to dynamically weigh the importance of different users, resulting in lower average AoI and better scalability compared to baseline methods.</p>

<p><strong>4. Methodology</strong><br>
The authors propose a Transformer-based Actor-Critic network where the Transformer encoder processes the system state to capture inter-user dependencies. This allows the agent to generate attention weights that act as a form of "machine gaze," focusing computational priority on critical users to determine optimal scheduling actions within the NOMA constraints.</p>

<p><strong>5. Limitations</strong><br>
While the abstract demonstrates success in simulations, it does not address the computational overhead of training Transformers compared to simpler architectures. Potential limitations also include the dependency on simulated environments which may not fully capture the unpredictability of real-world physical channels or hardware constraints on edge devices.</p>

<p><strong>6. Critical Evaluation</strong><br>
This work offers a compelling demonstration of how architectures typically reserved for Natural Language Processing (NLP) can be repurposed for network optimization. The strength of the paper lies in its interpretability, as the evolution of attention weights provides a clear visualization of the model's decision-making process, similar to analyzing eye-tracking data to understand human focus. However, the practical deployment of such compute-intensive models in real-time wireless systems remains a significant challenge.</p>
                </div>
                <div class="paper-entry" id="paper-2">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.22368v1" target="_blank">EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Jiahao Zhang, Yifan Zhang, Kevin Leach et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.22368v1" target="_blank">2602.22368v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p><strong>1. Research Question</strong><br>
The paper investigates whether human expertise, proxied by eye-gaze patterns during code reading, can be integrated into Large Language Models (LLMs) to enhance code summarization tasks. It specifically asks if modeling human visual attention can provide complementary signals to improve the semantic focus of these models.</p>

<p><strong>2. Positioning</strong><br>
While LLMs have advanced the field of code summarization, current models do not explicitly model the cognitive processes developers use to understand code. This work positions itself at the intersection of software engineering and cognitive computing, filling the gap between data-driven model attention and biological human attention mechanisms.</p>

<p><strong>3. Key Contribution</strong><br>
The primary contribution is EyeLayer, a lightweight module that seamlessly incorporates human gaze data into various LLM architectures without disrupting their pre-trained representations. The authors demonstrate that this approach is highly effective and transferable, achieving up to 13.17% improvement in BLEU-4 scores across diverse model families like LLaMA-3.2, Qwen3, and CodeBERT.</p>

<p><strong>4. Methodology</strong><br>
EyeLayer models human attention using a Multimodal Gaussian Mixture, redistributing token embeddings based on learned parameters \((\mu_i, \sigma_i^2)\) that represent the center and intensity of developer focus. This mechanism transforms raw eye-tracking data into generalizable attention priors, which are then used to augment the internal embeddings of the language models.</p>

<p><strong>5. Limitations</strong><br>
The abstract does not explicitly list limitations, but potential challenges include the requirement for high-quality eye-tracking datasets, which are expensive and difficult to collect. Additionally, the approach assumes that gaze patterns are consistent across different developers and programming contexts, which may introduce noise or bias if the eye-tracking data does not perfectly align with the model's input.</p>

<p><strong>6. Critical Evaluation</strong><br>
This work offers a novel perspective by treating human gaze as a structured attention prior rather than just auxiliary input, bridging the gap between cognitive science and AI. The reported gains are significant, and the architecture's ability to function as a plug-and-play module across different model types suggests strong practical utility for human-centered AI design.</p>
                </div>
                <div class="paper-entry" id="paper-3">
                    <h3 class="paper-title">
                        <a href="https://arxiv.org/pdf/2602.21983v1" target="_blank">Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots</a>
                    </h3>
                    <div class="paper-meta">
                        <strong>Authors:</strong> Jingchao Wei, Jingkai Qin, Yuxiao Cao et al.<br>
                        <strong>arXiv:</strong> <a href="https://arxiv.org/abs/2602.21983v1" target="_blank">2602.21983v1</a>
                    </div>
                    <div class="paper-topics">
                    </div>

                    <h4>Summary</h4>
                    <p><strong>1. Research Question</strong><br>
The paper addresses the challenge of enabling humanoid robots to perform natural and context appropriate gaze shifts during unconstrained human robot interaction. It asks how to effectively unify cognitive attention mechanisms for target selection with biomimetic motion generation for physical execution.</p>

<p><strong>2. Positioning</strong><br>
This work positions itself at the intersection of cognitive robotics and motion generation. It fills a critical gap in the literature where previous approaches often treated gaze target reasoning and motor control separately, leading to robotic or unnatural behaviors in dynamic social settings.</p>

<p><strong>3. Key Contribution</strong><br>
The main contribution is the Robot Gaze Shift (RGS) framework. This pipeline uniquely combines a Vision Language Model (VLM) for semantic gaze reasoning with a conditional VQ-VAE for generating diverse, human-like eye-head coordinated motions.</p>

<p><strong>4. Methodology</strong><br>
The methodology operates in two stages. First, a VLM processes multimodal cues to infer where the robot should look, mimicking human attention patterns. Second, a conditional VQ-VAE model generates the specific head and eye motor commands required to execute the shift, ensuring the movement itself appears biological rather than mechanical.</p>

<p><strong>5. Limitations</strong><br>
While the abstract validates the framework, potential limitations include the computational latency of running VLMs in real-time interaction loops. Additionally, the diversity and naturalness of the generated motions are heavily dependent on the quality and variance of the training data used for the VQ-VAE.</p>

<p><strong>6. Critical Evaluation</strong><br>
The integration of VLMs into gaze control is a significant advancement for creating socially aware robots. However, the system's complexity could introduce synchronization delays between the auditory or visual stimulus and the physical response, which is a critical factor for maintaining the illusion of natural interaction.</p>
                </div>

                <hr>

                <h3 id="references">References</h3>
                <ul>
                    <li><a href="https://arxiv.org/abs/2602.22774v1" target="_blank">Transformer Actor-Critic for Efficient Freshness-Aware Resource Allocation</a></li>
                    <li><a href="https://arxiv.org/abs/2602.22368v1" target="_blank">EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization</a></li>
                    <li><a href="https://arxiv.org/abs/2602.21983v1" target="_blank">Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots</a></li>
                </ul>

                <p><em>Feel free to reach out for discussion. Thanks for reading!</em></p>
            </div>
        </article>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2026 SY. All rights reserved. | Gaze/LLM Research Daily Review</p>
        </div>
    </footer>
</body>
</html>