<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DeepSeek's OpenSourceWeek: A Deep Dive into Their AI Infrastructure Approach</title>
    <!-- Using system Palatino fonts instead of Google Fonts -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['\\(','\\)']], displayMath: [['\\[','\\]']]}
        });
    </script>
    <style>
        body {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            font-size: 18px;
        }
        header {
            margin-bottom: 2rem;
            border-bottom: 1px solid #eaecef;
        }
        .site-title {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2em;
            margin-bottom: 0.5em;
        }
        nav ul {
            display: flex;
            list-style: none;
            padding: 0;
            margin: 1rem 0;
        }
        nav ul li {
            margin-right: 1.5rem;
        }
        nav ul li a {
            text-decoration: none;
            color: #0366d6;
            font-weight: 600;
        }
        nav ul li a:hover {
            text-decoration: underline;
        }
        .container {
            width: 100%;
            max-width: 800px;
        }
        h1 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 2.4em;
            margin-bottom: 0.5em;
            line-height: 1.2;
        }
        h2 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.8em;
            margin-top: 1.5em;
            margin-bottom: 0.75em;
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }
        h3 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.5em;
            margin-top: 1.2em;
            margin-bottom: 0.6em;
        }
        h4 {
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 1.2em;
            margin-top: 1em;
            margin-bottom: 0.5em;
        }
        p {
            margin: 1em 0;
        }
        code {
            font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
            background: #f6f8fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 0.9em;
        }
        pre {
            background: #f6f8fa;
            padding: 16px;
            border-radius: 6px;
            overflow: auto;
            font-size: 0.9em;
        }
        blockquote {
            margin: 1em 0;
            padding: 0 1em;
            color: #6a737d;
            border-left: 0.25em solid #dfe2e5;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1.5em auto;
        }
        .meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
        }
        .post-meta {
            color: #6a737d;
            font-size: 0.9em;
            margin-bottom: 2em;
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
        }
        .figure-caption {
            text-align: center;
            color: #666;
            font-size: 0.9em;
            margin-top: -1em;
            margin-bottom: 2em;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1.5em 0;
        }
        th, td {
            border: 1px solid #dfe2e5;
            padding: 8px 12px;
            text-align: left;
        }
        th {
            background-color: #f6f8fa;
        }
        tr:nth-child(even) {
            background-color: #f6f8fa;
        }
        #table-of-contents {
            background-color: #f8f9fa;
            padding: 1.5rem;
            border-radius: 5px;
            margin: 2rem 0;
        }
        #table-of-contents h3 {
            margin-top: 0;
            margin-bottom: 1rem;
        }
        #table-of-contents ol {
            margin-bottom: 0;
        }
        #table-of-contents a {
            text-decoration: none;
            color: #0366d6;
        }
        #table-of-contents a:hover {
            text-decoration: underline;
        }
        .post-content {
            margin-top: 2rem;
        }
        footer {
            margin-top: 3rem;
            padding-top: 1rem;
            border-top: 1px solid #eaecef;
            font-size: 0.9em;
            color: #6a737d;
        }
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
            h1 {
                font-size: 2em;
            }
            h2 {
                font-size: 1.6em;
            }
        }
        /* Language switching styles */
        .language-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            z-index: 1000;
            padding: 8px 16px;
            background-color: #0366d6;
            color: white;
            border: none;
            border-radius: 4px;
            font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, serif;
            font-size: 16px;
            cursor: pointer;
            transition: background-color 0.3s;
        }

        .language-switch:hover {
            background-color: #0353b4;
        }

        /* Content language display control */
        .zh-content {
            display: none;
        }

        .en-content {
            display: block;
        }

        body.zh .zh-content {
            display: block;
        }

        body.zh .en-content {
            display: none;
        }
    </style>
</head>
<body>
    <button class="language-switch" id="languageToggle">Switch to 中文</button>

    <header>
        <div class="container">
            <h1 class="site-title">Beren's Blog</h1>
            <nav>
                <ul>
                  <li><a href="../index.html">Home</a></li>
                  <li><a href="../blog.html">Blog</a></li>
                  <li><a href="../gallery.html">Gallery</a></li>
                  <li><a href="../about.html">About</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="container">
        <article class="blog-post">
            <div class="en-content">
                <h2 class="post-title">DeepSeek's OpenSourceWeek: A Deep Dive into Their AI Infrastructure Approach</h2>
                <div class="post-meta">
                    <span class="date">March 2, 2025</span>
                    <span class="author">Beren Meng</span>
                    <span class="label">GenAI & LLMs</span>
                </div>

                <div id="table-of-contents">
                  <h3>Table of Contents</h3>
                  <ol>
                    <li><a href="#foundation">Compute Optimization Fundamentals</a></li>
                    <li><a href="#parallelism">Parallelism Strategies for Multi-Expert Models</a></li>
                    <li><a href="#data-backbone">Core Data Infrastructure for AI Workloads</a></li>
                    <li><a href="#performance">Performance Benchmarks</a></li>
                    <li><a href="#state-of-ai">Insights for AI Infrastructure Development</a></li>
                    <li><a href="#open-source">Why Open-Source Everything</a></li>
                    <li><a href="#inference-system">DeepSeek-V3/R1 Production Inference System</a></li>
                  </ol>
                </div>

                <div class="post-content">
                    <p>
                        Here we are again, folks, and we're back with another interesting post on DeepSeek's AI infra approach.
                    </p>
                    <p>
                        This week, DeepSeek AI took an extraordinary step by open-sourcing critical components of their AI infrastructure stack during their aptly named "OpenSourceWeek." I've been eagerly following these releases, since I'm always curious about architectural design principles, as well as AI systems design. I realized that this wasn't just another PR exercise (well it could be), but a technical showcase revealing the building blocks that power their latest models.
                    </p>
                    <img src="../images/DeepSeek2/1-DeepSeek-overview.png" alt="DeepSeek OpenSourceWeek" />
                    <div class="figure-caption">Figure 1: Overview of DeepSeek's OpenSourceWeek components</div>
                    <p>
                        Over six days, the team released eight repositories spanning compute optimization, parallelism techniques, and data infrastructure. What makes this particularly interesting is how these components work together, forming a comprehensive stack for training and deploying trillion-parameter language models.
                    </p>
                    <p>
                        Let me walk you through what they've shared, why it matters, and what it reveals about the directions in AI infrastructure.
                    </p>

                    <h3 id="foundation">Compute Optimization Fundamentals</h3>

                    <p>
                        The first and third days focused on bare-metal optimizations targeting the computational bottlenecks in large language models. FlashMLA and DeepGEMM address two critical operations in neural network computation: attention mechanisms and matrix multiplication.
                    </p>

                    <p>
                        <strong>FlashMLA</strong> launched on day one, delivering a highly optimized kernel for multi-head linear attention (MLA). To provide more contexts, attention mechanisms are the computational heart of modern language models, and linear attention variants are increasingly important for handling long sequences efficiently.
                    </p>

                    <img src="../images/DeepSeek2/2-DeepSeek-flashmla.png" alt="DeepSeek FlashMLA Architecture" />
                    <div class="figure-caption">Figure 2: FlashMLA architecture for efficient attention computation</div>

                    <p>
                        The repository reveals meticulous GPU-level optimizations for Hopper architecture, supporting both BF16 and FP16 precision with clever memory management through paged KV caching (with block size 64). The performance numbers are impressive: 3000 GB/s in memory-bound configurations and 580 TFLOPS in compute-bound scenarios on H800 GPUs.
                    </p>

                    <p>
                        Two days later, <strong>DeepGEMM</strong> arrived, tackling the general matrix multiplication (GEMM) operations that form the computational backbone of neural networks. DeepGEMM stands out with its focused design philosophy: clean, efficient FP8 GEMMs with fine-grained scaling and a remarkably lightweight implementation (~300 lines of core code).
                    </p>

                    <p>
                        Despite being lightweight, DeepGEMM incorporates advanced techniques like just-in-time (JIT) compilation, warp specialization, and clever optimizations for Hopper GPU architectures. The performance matches or exceeds expert-tuned libraries across various matrix shapes.
                    </p>

                    <p>
                        DeepGEMM shows up to 2.7x speedup compared to CUTLASS 3.6 for certain matrix shapes. Now it's not hard to get the point of how specialized implementations can dramatically outperform general-purpose solutions for specific workloads.
                    </p>
                    <img src="../images/DeepSeek2/4-DeepSeek-deepgeem.png" alt="DeepSeek Compute Optimization" />
                    <div class="figure-caption">Figure 3: DeepGEMM Architecture</div>

                    <h3 id="parallelism">Parallelism Strategies for Multi-Expert Models</h3>

                    <p>
                        The second and fourth days unveiled DeepSeek's approaches to training and deploying massive models across distributed hardware.
                    </p>

                    <p>
                        <strong>DeepEP</strong> (day two) tackles the communication bottlenecks in expert parallelism, a technique that's becoming increasingly central to state-of-the-art MoE models. Expert parallelism distributes different neural network "experts" across multiple GPUs, but this creates unique communication patterns that traditional frameworks handle suboptimally.
                    </p>

                    <p>
                        DeepEP offers both high-throughput kernels for training and prefilling, plus specialized low-latency kernels for inference decoding. They provided an innovative approach to computation-communication overlap, by developing a hook-based method that doesn't consume any GPU streaming multiprocessors (SMs), allowing computation to proceed unimpeded.
                    </p>

                    <p>
                        The performance data is compelling: intranode dispatch/combine operations reach 153-158 GB/s (NVLink bound), while internode operations achieve 43-47 GB/s (RDMA bound). For latency-sensitive inference, they've driven down communication overhead to just 163-194 microseconds for dispatch operations.
                    </p>

                    <img src="../images/DeepSeek2/3-DeepSeek-deepep.png" alt="DeepSeek Expert Parallelism" />
                    <div class="figure-caption">Figure 4: DeepEP architecture showing communication-computation overlap</div>

                    <p>
                        Day four expanded the parallelism toolkit with three related projects:
                    </p>

                    <ol>
                        <li><strong>DualPipe</strong> introduces a bidirectional pipeline parallelism algorithm that achieves full overlap of forward and backward computation-communication phases while reducing pipeline bubbles.</li>
                        <li><strong>EPLB</strong> (Expert Parallelism Load Balancer) intelligently distributes experts across GPUs to balance computational load, employing either hierarchical or global balancing strategies depending on the workload.</li>
                        <li><strong>Profile Data</strong> provides real-world profiling data demonstrating their computation-communication overlap strategies in action, revealing the careful orchestration happening under the hood.</li>
                    </ol>

                    <img src="../images/DeepSeek2/5-DeepSeek-dualpipe.png" alt="DeepSeek DualPipe Architecture" />
                    <div class="figure-caption">Figure 5: DualPipe bidirectional pipeline architecture</div>

                    <h3 id="data-backbone">Core Data Infrastructure for AI Workloads</h3>

                    <p>
                        The final day brought two complementary data infrastructure projects: 3FS and smallpond. These address the often-overlooked challenge of efficiently managing the massive datasets required for training and model serving.
                    </p>

                    <p>
                        <strong>3FS</strong> (Fire-Flyer File System) is a high-performance distributed file system designed specifically for AI workloads. It utilizes modern SSDs and RDMA networks to provide a shared storage layer with strong consistency guarantees. The system achieves remarkable performance: 6.6 TiB/s aggregate read throughput on a cluster of 180 storage nodes, each equipped with 2×200Gbps InfiniBand NICs and sixteen 14TiB NVMe SSDs.
                    </p>

                    <p>
                        3FS focus on diverse AI workloads, from data preparation and loading to checkpointing and KV caching for inference. The disaggregated architecture combines the throughput of thousands of SSDs with the network bandwidth of hundreds of nodes, enabling applications to access storage resources regardless of physical location.
                    </p>

                    <p>
                        Its companion, <strong>smallpond</strong>, is a lightweight data processing framework built on DuckDB and 3FS. It provides a Python API for high-performance data processing that can scale to petabyte-level datasets. In tests with the GraySort benchmark, smallpond sorted 110.5 TiB of data in just over 30 minutes, achieving a throughput of 3.66 TiB/min.
                    </p>

                    <p>
                        Together, these data infrastructure components complete DeepSeek's open source stack, a pretty organic and comprehensive combo of computation optmization, parallism, and data infrastructure.
                    </p>

                    <table>
                        <thead>
                            <tr>
                                <th>Layer</th>
                                <th>Components</th>
                                <th>Key Features</th>
                                <th>Impact</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Compute Optimization</strong></td>
                                <td>FlashMLA, DeepGEMM</td>
                                <td>Optimized CUDA kernels for attention and matrix multiplication; FP8/BF16/FP16 support</td>
                                <td>1.5-2.7x speedup on key operations</td>
                            </tr>
                            <tr>
                                <td><strong>Parallelism</strong></td>
                                <td>DeepEP, DualPipe, EPLB</td>
                                <td>Expert parallelism communication, pipeline parallelism, load balancing</td>
                                <td>Efficient scaling across hundreds of GPUs with minimal overhead</td>
                            </tr>
                            <tr>
                                <td><strong>Data Infrastructure</strong></td>
                                <td>3FS, smallpond</td>
                                <td>Distributed file system, data processing framework</td>
                                <td>6.6 TiB/s storage throughput, efficient data preparation</td>
                            </tr>
                        </tbody>
                    </table>

                    <img src="../images/DeepSeek2/6-DeepSeek-3fs.png" alt="3FS and smallpond" />
                    <div class="figure-caption">Figure 6: 3FS distributed storage architecture and performance</div>


                    <h3 id="performance">Performance Benchmarks</h3>

                    <p>
                        Beyond the architectural elegance, what ultimately matters is performance. The benchmarks across these projects reveal impressive capabilities:
                    </p>

                    <table>
                        <thead>
                            <tr>
                                <th>Component</th>
                                <th>Benchmark</th>
                                <th>Result</th>
                                <th>Comparison</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>FlashMLA</td>
                                <td>H800 GPU throughput</td>
                                <td>3000 GB/s (memory-bound), 580 TFLOPS (compute-bound)</td>
                                <td>State-of-the-art for MLA kernels</td>
                            </tr>
                            <tr>
                                <td>DeepGEMM</td>
                                <td>Matrix multiplication (128×2112×7168)</td>
                                <td>352 TFLOPS</td>
                                <td>2.4x vs. CUTLASS 3.6</td>
                            </tr>
                            <tr>
                                <td>DeepEP</td>
                                <td>Low-latency dispatch (8 experts)</td>
                                <td>163μs latency, 46 GB/s bandwidth</td>
                                <td>Optimized for inference</td>
                            </tr>
                            <tr>
                                <td>3FS</td>
                                <td>Read stress test</td>
                                <td>6.6 TiB/s aggregate throughput</td>
                                <td>180 storage nodes, 500+ clients</td>
                            </tr>
                            <tr>
                                <td>smallpond</td>
                                <td>GraySort benchmark</td>
                                <td>110.5 TiB sorted in 30m14s (3.66 TiB/min)</td>
                                <td>25 storage nodes, 50 compute nodes</td>
                            </tr>
                        </tbody>
                    </table>

                    <p>
                        These numbers represent substantial improvements over existing solutions, particularly in the specific scenarios DeepSeek has optimized for. But what's more interesting is how these optimizations compound when deployed together in a real-world AI pipeline. We have hundreds of reasons to believe that more open-source libraries built on these will accelarate the process of building AI workloads in prototyping and production.
                    </p>


                    <h3 id="state-of-ai">Insights for AI Infra Development</h3>

                    <p>
                        DeepSeek's OpenSourceWeek offers valuable insights into where AI infrastructure is heading:
                    </p>

                    <p><strong>Specialization is accelerating.</strong> General-purpose tools are giving way to highly specialized implementations optimized for specific AI workloads.</p>

                    <p><strong>Hardware-software co-design is essential.</strong> The most significant gains come from software that deeply understands the underlying hardware. Data management in these layers are equally critical.</p>

                    <p><strong>More and more Mixture-of-Experts designs.</strong></p>

                    <p><strong>System-level optimization matters more than algorithms.</strong> Many of these improvements come not from new algorithms but from better implementations and system-level optimizations.</p>

                    <p>
                        The era of "just throw more GPUs at it" is giving way to carefully engineered systems where efficiency is paramount. Collecting 300 thousand of GPUs at home is quite a privilage, thus it would be great if you get to know more tips on how to utilize every single one of them, when you only have a few available as an individual researcher or common scale enterprise.
                    </p>

                    <h3 id="open-source">Why Open-Source Everything</h3>

                    <p>
                        A question that naturally arises is why DeepSeek would open source these competitive advantages. While I can't speak to their specific motivations, several possibilities come to mind:
                    </p>

                    <ol>
                        <li><strong>Ecosystem development:</strong> By releasing these tools, DeepSeek can help grow the ecosystem around their approach to AI infrastructure.</li>
                        <li><strong>Talent attraction:</strong> Open sourcing sophisticated technology demonstrates technical prowess and helps attract engineering talent.</li>
                        <li><strong>Standardization:</strong> These implementations could become de facto standards, shaping how the industry builds similar systems.</li>
                        <li><strong>Community improvement:</strong> External contributors might identify optimizations or extensions that benefit DeepSeek's own systems.</li>
                    </ol>

                    <p>
                        Whatever the motivation, these releases provide valuable tools for anyone building large-scale AI systems and offer a fascinating glimpse into the engineering practices of a leading AI lab.
                    </p>

                    <h3 id="inference-system">DeepSeek-V3/R1 Production Inference System</h3>

                    <p>
                        Just when I thought OpenSourceWeek was complete, DeepSeek surprised us with a bonus "Day 6" release, which is a detailed overview of their actual production inference system for DeepSeek-V3/R1. This is a very interesting and informative release, because while training infrastructure gets considerable attention, inference systems often remain mysterious despite being the workhorses of real-world AI deployment, and they told us a lot about how they operate on this.
                    </p>

                    <p>
                        The system reveals a carefully engineered architecture optimized for two critical objectives: higher throughput and lower latency. To achieve these, DeepSeek employs cross-node Expert Parallelism (EP) at a scale that's frankly astonishing, up to EP144 for the decoding phase.
                    </p>

                    <p>
                        Why such extreme parallelism? The answer lies in the sparse activation pattern of their MoE models. With only 8 out of 256 experts activated per layer, achieving adequate batch sizes per expert requires distributing the work across many GPUs. This enables both higher throughput (through better GPU utilization) and lower latency (by reducing per-GPU memory access demands).
                    </p>

                    <p>
                        Their prefill-decode disaggregation architecture is also impressive, which employs different parallelism strategies based on the computational characteristics of each phase:
                    </p>

                    <ul>
                        <li><strong>Prefilling Phase</strong>: [Routed Expert EP32, MLA/Shared Expert DP32] spanning 4 nodes with 32 redundant routed experts</li>
                        <li><strong>Decoding Phase</strong>: [Routed Expert EP144, MLA/Shared Expert DP144] spanning 18 nodes with the same number of redundant experts</li>
                    </ul>

                    <p>
                        To mitigate the communication overhead inherent in such large-scale parallelism, they've implemented a dual-batch overlap strategy, cleverly hiding communication costs behind computation. During prefilling, two microbatches execute alternately, with the communication cost of one hidden behind the computation of the other. For decoding, they use a 5-stage pipeline with the attention layer subdivided to achieve seamless overlapping.
                    </p>

                    <p>
                        The system also incorporates sophisticated load balancing at multiple levels:
                    </p>

                    <ol>
                        <li><strong>Prefill Load Balancer</strong>: Equalizes input token counts per GPU and balances core-attention computation</li>
                        <li><strong>Decode Load Balancer</strong>: Balances KVCache usage and request counts per GPU</li>
                        <li><strong>Expert-Parallel Load Balancer</strong>: Minimizes the maximum dispatch receive load across all GPUs</li>
                    </ol>

                    <p>
                        Perhaps most interesting are the actual production statistics. Over a 24-hour period, DeepSeek's V3/R1 inference services processed a staggering 608 billion input tokens and generated 168 billion output tokens, with 56.3% of inputs hitting their on-disk KV cache. The system ran on up to 278 H800 nodes (with 8 GPUs each), dynamically scaling down during low-traffic nighttime hours to redirect resources to research and training.
                    </p>

                    <p>
                        Each H800 node delivers approximately 73.7k tokens/s input during prefilling or 14.8k tokens/s output during decoding. At an estimated GPU cost of $2 per hour, their daily infrastructure expense runs around $87,072, impressive economics for a system of this scale.
                    </p>

                    <img src="../images/DeepSeek2/7-DeepSeek-integration.png" alt="DeepSeek-V3/R1 Inference System" />
                    <div class="figure-caption">Figure 7: DeepSeek-V3/R1 inference system architecture showing prefill and decode phases</div>

                    <p>
                        For those interested in the technical details, I encourage you to explore the repositories themselves. There's much more depth than I could cover here, and the code itself tells a fascinating story of engineering excellence. As AI systems continue to grow in scale and complexity, infrastructure innovations like these will be increasingly crucial to continued progress.
                    </p>

                    <hr>
                    
                    <h3>Further Reading</h3>
                    <ul>
                        <li><a href="https://github.com/deepseek-ai/FlashMLA" target="_blank">FlashMLA GitHub Repository</a></li>
                        <li><a href="https://github.com/deepseek-ai/DeepEP" target="_blank">DeepEP GitHub Repository</a></li>
                        <li><a href="https://github.com/deepseek-ai/DeepGEMM" target="_blank">DeepGEMM GitHub Repository</a></li>
                        <li><a href="https://github.com/deepseek-ai/DualPipe" target="_blank">DualPipe GitHub Repository</a></li>
                        <li><a href="https://github.com/deepseek-ai/eplb" target="_blank">EPLB GitHub Repository</a></li>
                        <li><a href="https://github.com/deepseek-ai/profile-data" target="_blank">Profile Data GitHub Repository</a></li>
                        <li><a href="https://github.com/deepseek-ai/3FS" target="_blank">3FS GitHub Repository</a></li>
                        <li><a href="https://github.com/deepseek-ai/smallpond" target="_blank">Smallpond GitHub Repository</a></li>
                        <li><a href="https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md" target="_blank">DeepSeek-V3/R1 Inference System Overview</a></li>
                    </ul>

                    <hr>

                    <p><em>Found this interesting? Feel free to reach out to me to discuss further technical aspects in the field of GenAI, LLM, cycling, coffee, and many more. Cheers!</em></p>
                </div>
            </div>

            <!-- Chinese version of the content -->
            <div class="zh-content">
                <h2 class="post-title">DeepSeek开源周：总结与回顾</h2>
                <div class="post-meta">
                    <span class="date">2025年3月2日</span>
                    <span class="author">Beren Meng</span>
                    <span class="label">GenAI & LLMs</span>
                </div>

                <div id="table-of-contents">
                  <h3>目录</h3>
                  <ol>
                    <li><a href="#zh-foundation">底层计算优化</a></li>
                    <li><a href="#zh-parallelism">多专家模型并行策略</a></li>
                    <li><a href="#zh-data-backbone">AI数据基础设施核心支撑</a></li>
                    <li><a href="#zh-comprehensive-stack">完整AI基础设施技术栈</a></li>
                    <li><a href="#zh-performance">性能基准测试与实践验证</a></li>
                    <li><a href="#zh-philosophy">DeepSeek的工程理念</a></li>
                    <li><a href="#zh-open-source">开源决策与技术影响</a></li>
                    <li><a href="#zh-inference-system">DeepSeek-V3/R1生产级推理系统解析</a></li>
                  </ol>
                </div>

                <div class="post-content">
                    <p>
                        朋友们欢迎回来。这周的每个夜晚我都蹲守在DS的推特账号面前，仿佛是在等待一个个彩蛋。昨天终于跟完了DS的开源周所有repo，在这里做个简单的总结和回顾，也是敦促自己未来应在AI infra方面菜就多练，以成为一个更加全面的全栈。
                    </p>

                    <img src="../images/DeepSeek2/1-DeepSeek-overview-CN.png" alt="DeepSeek开源周" />
                    <div class="figure-caption">图1：DeepSeek开源周组件概览</div>
                    <p>
                        在五天时间内（加上day6的彩蛋），DeepSeek团队陆续开放了八个代码仓库，涵盖计算优化、并行策略和数据处理基础设施。这些组件协同工作的方式尤其值得关注，因为它们可以说是共同覆盖了训练和部署万亿参数大模型完整技术栈的核心方面。
                    </p>

                    <h3 id="zh-foundation">底层计算优化</h3>

                    <p>
                        首日与第三日的开源聚焦于针对大语言模型计算瓶颈的底层优化。FlashMLA和DeepGEMM这两个项目分别攻克了神经网络计算中的两大核心操作：注意力机制与矩阵乘法。
                    </p>

                    <img src="../images/DeepSeek2/2-DeepSeek-flashmla-CN.png" alt="DeepSeek FlashMLA架构" />
                    <div class="figure-caption">图2：FlashMLA核心机制</div>

                    <p>
                        在大语言模型中，注意力机制是计算核心，而线性注意力变体对长序列处理效率至关重要。<strong>FlashMLA</strong>作为首日开源的明星项目，提供了高度优化的多头线性注意力（MLA）计算核。
                    </p>

                    <img src="../images/DeepSeek2/2-DeepSeek-flashmla-CNv2.png" alt="DeepSeek FlashMLA架构v2" />
                    <div class="figure-caption">图3：FlashMLA高效注意力计算架构</div>

                    <p>
                        这个代码库中展现了针对Hopper架构的极致GPU优化：支持BF16/FP16精度，采用分页KV缓存（块大小64）的智能内存管理。H800 GPU上的性能数据令人瞩目：内存受限场景达3000 GB/s，计算受限场景达580 TFLOPS。
                    </p>

                    <p>
                        两天后亮相的<strong>DeepGEMM</strong>则直击神经网络计算基石——通用矩阵乘法（GEMM）。其设计哲学展现出独特魅力：专注于实现干净高效的FP8 GEMM，通过精细缩放策略和轻量级实现（核心代码约300行），在保持简洁的同时融入即时编译（JIT）、Warp专业化等先进技术，性能全面超越专家调优的行业标准库（如CUTLASS 3.6）。
                    </p>

                    <img src="../images/DeepSeek2/4-DeepSeek-deepgeem-CN.png" alt="DeepSeek计算优化" />
                    <div class="figure-caption">图4：DeepGEMM库核心机制</div>

                    <p>
                        这两个库的对比印证了系统工程的核心原则：针对特定工作负载的专用实现，其性能可碾压通用方案。例如DeepGEMM在某些矩阵形状下相较CUTLASS 3.6实现2.7倍加速。
                    </p>
                    <img src="../images/DeepSeek2/4-DeepSeek-deepgeem-CNv2.png" alt="DeepSeek计算优化v2" />
                    <div class="figure-caption">图5：DeepGEMM库架构</div>

                    <h3 id="zh-parallelism">多专家模型并行策略</h3>

                    <p>
                        第二日与第四日的开源揭示了DeepSeek在大模型分布式训练中的创新并行策略。
                    </p>

                    <p>
                        <strong>DeepEP</strong>（第二日）攻克专家并行（Expert Parallelism）中的通信瓶颈。该技术将不同神经网络"专家"分布到多GPU，但传统框架难以优化其独特通信模式。
                    </p>

                    <p>
                        DeepEP同时提供高吞吐训练核与低延迟推理核，其创新之处在于实现计算-通信无冲突重叠：采用hook机制避免占用GPU流式多处理器（SMs），确保计算持续进行。性能表现亮眼——节点内调度/组合操作达153-158 GB/s（受限于NVLink），跨节点操作达43-47 GB/s（受限于RDMA）。在延迟敏感的推理场景，调度操作通信开销仅163-194微秒。
                    </p>

                    <img src="../images/DeepSeek2/3-DeepSeek-deepep-CN.png" alt="DeepSeek专家并行" />
                    <div class="figure-caption">图6：DeepEP核心机制</div>

                    <img src="../images/DeepSeek2/3-DeepSeek-deepep-CNv2.png" alt="DeepSeek专家并行v2" />
                    <div class="figure-caption">图7：DeepEP架构</div>

                    <p>
                        第四日开源的三项技术进一步扩展并行工具集：
                    </p>

                    <ol>
                        <li><strong>DualPipe</strong>提出双向流水线并行算法，实现前向/反向计算-通信阶段完全重叠，同时减少流水线气泡</li>
                        <li><strong>EPLB</strong>（专家并行负载均衡器）智能分配专家到GPU，根据负载采用分层或全局均衡策略</li>
                        <li><strong>Profile Data</strong>提供真实场景的性能剖析数据，展示底层计算-通信重叠的精密编排</li>
                    </ol>

                    <img src="../images/DeepSeek2/5-DeepSeek-dualpipe-CN.png" alt="DeepSeek DualPipe架构" />
                    <div class="figure-caption">图8：DualPipe双向流水线架构</div>

                    <h3 id="zh-data-backbone">AI数据基础设施核心支撑</h3>

                    <p>
                        第五日的开源聚焦于AI系统中常被忽视的命脉——数据基础设施。<strong>3FS</strong>与<strong>smallpond</strong>这对组合，直击海量数据管理痛点，展现出DeepSeek对全栈优化的深刻理解。
                    </p>

                    <h4>3FS（萤火虫文件系统）</h4>

                    <p>
                        <strong>3FS</strong>是专为AI负载设计的高性能分布式文件系统。其架构创新之处在于：
                    </p>

                    <ul>
                        <li>基于现代SSD与RDMA网络的解耦架构，提供强一致性的共享存储层</li>
                        <li>存储层聚合180个节点的性能，每个节点配备双200Gbps InfiniBand网卡与16块14TB NVMe SSD</li>
                        <li>实测<strong>6.6 TiB/s总读取吞吐</strong>，支持500+客户端并发访问</li>
                    </ul>

                    <p>
                        这套系统的最大亮点在于其全流程覆盖能力：从数据预处理、训练加载，到推理时的KV缓存管理，3FS为各环节提供统一存储接口。通过软硬件协同，将数千块SSD的吞吐与数百节点的网络带宽解耦重组，实现"存储资源池化"。
                    </p>

                    <h4>smallpond</h4>

                    <p>
                        配套工具<strong>smallpond</strong>则是一把数据处理的瑞士军刀：
                    </p>

                    <ul>
                        <li>基于DuckDB构建的轻量级框架，提供Python API</li>
                        <li>在GraySort基准测试中，用25个存储节点+50个计算节点，30分钟完成<strong>110.5 TiB数据排序</strong>（3.66 TiB/分钟）</li>
                        <li>支持PB级数据的高效处理，打通从原始数据到训练就绪格式的全链路</li>
                    </ul>

                    <p>
                        这对黄金组合的协同效应尤为亮眼：<strong>3FS</strong>提供存储层原子能力，<strong>smallpond</strong>实现计算层灵活编排。这种分层设计既避免了传统大数据框架的臃肿，又规避了临时脚本的脆弱性，一个非常出色的AI数据工程解决方案。
                    </p>
                    <img src="../images/DeepSeek2/6-DeepSeek-3fs-CN.png" alt="3FS与smallpond" />
                    <div class="figure-caption">图9：3FS分布式存储架构与性能</div>

                    <h3 id="zh-comprehensive-stack">完整AI基础设施技术栈</h3>


                    <table>
                        <thead>
                            <tr>
                                <th>层级</th>
                                <th>组件</th>
                                <th>核心价值</th>
                                <th>实际收益</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>计算优化层</td>
                                <td>FlashMLA、DeepGEMM</td>
                                <td>专用CUDA核，FP8/BF16/FP16混合精度支持</td>
                                <td>关键算子1.5-2.7倍加速</td>
                            </tr>
                            <tr>
                                <td>并行策略层</td>
                                <td>DeepEP、DualPipe、EPLB</td>
                                <td>专家并行通信优化，流水线气泡消除</td>
                                <td>百卡级高效扩展</td>
                            </tr>
                            <tr>
                                <td>数据基建层</td>
                                <td>3FS、smallpond</td>
                                <td>分布式存储+轻量数据处理</td>
                                <td><strong>6.6 TiB/s存储吞吐</strong></td>
                            </tr>
                        </tbody>
                    </table>

                    <p>
                        当我们将这些组件拼合成完整技术栈时，DeepSeek的工程哲学跃然纸上：
                    </p>

                    <ul>
                        <li><strong>计算层</strong>榨取单卡极限算力</li>
                        <li><strong>并行层</strong>化解多卡扩展瓶颈</li>
                        <li><strong>数据层</strong>突破IO墙限制</li>
                    </ul>

                    <p>
                        这种端到端的优化思路，使得各环节的性能增益形成乘数效应。例如在万亿参数模型训练中，1.5倍的算子加速叠加2倍的并行效率提升，最终可能带来3倍以上的整体提速，而这正是系统工程的价值所在。
                    </p>

                    <h3 id="zh-performance">性能基准测试与实践验证</h3>

                    <p>
                        从公布的关键性能指标中可以窥见这些组件的实战价值：
                    </p>

                    <table>
                        <thead>
                            <tr>
                                <th>组件</th>
                                <th>测试场景</th>
                                <th>结果</th>
                                <th>对比优势</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>FlashMLA</td>
                                <td>H800 GPU吞吐测试</td>
                                <td>内存受限3000 GB/s</td>
                                <td>MLA核性能业界领先</td>
                            </tr>
                            <tr>
                                <td>DeepGEMM</td>
                                <td>128×2112×7168矩阵乘法</td>
                                <td>352 TFLOPS</td>
                                <td>超CUTLASS 3.6达2.4倍</td>
                            </tr>
                            <tr>
                                <td>DeepEP</td>
                                <td>8专家低延迟调度</td>
                                <td>163μs延迟，46 GB/s带宽</td>
                                <td>推理场景专用优化</td>
                            </tr>
                            <tr>
                                <td>3FS</td>
                                <td>读取压力测试</td>
                                <td>6.6 TiB/s聚合吞吐</td>
                                <td>180节点线性扩展</td>
                            </tr>
                            <tr>
                                <td>smallpond</td>
                                <td>GraySort基准测试</td>
                                <td>110.5 TiB/30分钟</td>
                                <td>性价比超越传统方案</td>
                            </tr>
                        </tbody>
                    </table>

                    <p>
                        对于AI基础设施的演进方向来说，或许当模型规模突破万亿参数后，系统级优化带来的边际收益将会超过单纯增加算力。堆卡仍然具有优势，大力出奇迹。但未来的局面更有可能是张飞绣花，粗中有细。系统调优和现有硬件算力利用方面会有更多的花样出现。
                    </p>

                    <h3 id="zh-philosophy">DeepSeek的工程理念</h3>

                    <p>
                        透过这些开源组件，我们可以解码DeepSeek这一波的底层技术哲学：
                    </p>

                    <ol>
                        <li><strong>领域专用优化</strong>：摈弃通用方案，针对LLM场景打造定制化组件</li>
                        <li><strong>硬件深度调谐</strong>：对GPU架构、内存层级、网络拓扑的极致理解，压榨硬件潜能</li>
                        <li><strong>实战导向思维</strong>：所有优化均源于万亿参数模型的真实训练挑战</li>
                        <li><strong>端到端系统观</strong>：组件间无缝衔接，构建全局最优而非局部最优</li>
                        <li><strong>极简主义实现</strong>：以DeepGEMM为例，核心代码仅300行却实现顶尖性能</li>
                    </ol>

                    <h3 id="zh-open-source">开源决策与技术影响</h3>

                    <p>
                        一个自然浮现的疑问是：DeepSeek为何将核心技术开源？虽然无法确知其商业考量，但可推测多重动机：
                    </p>

                    <ol>
                        <li><strong>生态构建</strong>：通过开源建立技术标准，引导行业采用其技术路线</li>
                        <li><strong>人才虹吸</strong>：展示顶尖工程能力，吸引全球顶尖系统人才</li>
                        <li><strong>社区反哺</strong>：借助开发者社区优化代码，形成正向技术循环</li>
                        <li><strong>行业教育</strong>：推动AI基建认知升级，扩大潜在市场空间</li>
                    </ol>

                    <p>
                        无论动机如何，这些开源项目为行业提供了宝贵的工程范式。例如DeepGEMM的极简设计，或DeepEP的通信重叠策略，为后续开发者提示了更多的可行方案。
                    </p>

                    <h3 id="zh-inference-system">DeepSeek-V3/R1生产级推理系统解析</h3>

                    <p>
                        "上帝创造世界，然后在第七日休息。"
                    </p>
                    <p>
                        当众人以为开源周落幕时，DeepSeek意外放出"第六日"彩蛋，内容是自家生产级推理系统DeepSeek-V3/R1的架构解密。这尤为珍贵，因为相比训练系统，推理架构往往笼罩在商业机密的面纱之下。
                    </p>

                    <p>
                        该系统的设计目标可以说是直击产业痛点：<strong>高吞吐</strong>与<strong>低延迟</strong>的平衡艺术。为实现这一目标，DeepSeek给出了两个方案：
                    </p>

                    <p><strong>超大规模专家并行</strong>：</p>

                    <ul>
                        <li><strong>预填充阶段</strong>：EP32架构，4节点部署32个冗余路由专家</li>
                        <li><strong>解码阶段</strong>：EP144架构，18节点部署144个冗余专家</li>
                    </ul>

                    <p>
                        这种设计的底层逻辑源于MoE模型的稀疏激活特性， 即每层仅激活8/256个专家。通过跨节点分布式部署，既提升GPU利用率（高吞吐），又降低单卡内存压力（低延迟）。
                    </p>

                    <p><strong>通信-计算魔法</strong>：</p>

                    <ul>
                        <li><strong>双批次重叠策略</strong>：在预填充阶段，两个微批次交替执行，用计算掩盖通信开销</li>
                        <li><strong>五级流水线设计</strong>：解码阶段将注意力层拆解，实现通信零等待</li>
                    </ul>

                    <p>
                        系统还包含多层负载均衡机制：
                    </p>

                    <ol>
                        <li>预填充负载均衡器：均衡GPU间的token数量与核心计算量</li>
                        <li>解码负载均衡器：动态平衡KV缓存与请求分布</li>
                        <li>专家并行负载均衡器：最小化跨GPU调度负载峰值</li>
                    </ol>

                    <p>
                        真实生产数据更具说服力：
                    </p>

                    <ul>
                        <li>单日处理<strong>6080亿输入token</strong> + <strong>1680亿输出token</strong></li>
                        <li>56.3%请求命中磁盘KV缓存，降低显存压力</li>
                        <li>动态伸缩集群规模（高峰278个H800节点，夜间资源转供研发）</li>
                        <li>单H800节点性能：73.7k token/s（预填充）或14.8k token/s（解码）</li>
                        <li>日均基础设施成本约<strong>8.7万美元</strong>，展现极致性价比</li>
                    </ul>

                    <p>
                        这套系统的将开源周的组件串联成完整生产管线，证明了理论优化到工程落地的可行性。
                    </p>

                    <p>
                        我们现在有理由相信，当越来越多的机构加入基建优化的竞赛，整个AI领域将迎来效率跃迁。在高质量数据几近枯竭的后训练时代，这可能比单纯增大模型参数更有助于AGI的实现。
                    </p>

                    <p>
                        建议直接阅读GitHub repo，其中的工程细节（如DeepGEMM的JIT实现、3FS的RDMA优化）远比本文所述更加精妙。
                    </p>
                    <p>
                        静待R2的发布。
                    </p>

                    <hr>

                    <h3>延伸阅读</h3>
                    <ul>
                        <li><a href="https://github.com/deepseek-ai/FlashMLA" target="_blank">FlashMLA GitHub 代码仓库</a></li>
                        <li><a href="https://github.com/deepseek-ai/DeepEP" target="_blank">DeepEP GitHub 代码仓库</a></li>
                        <li><a href="https://github.com/deepseek-ai/DeepGEMM" target="_blank">DeepGEMM GitHub 代码仓库</a></li>
                        <li><a href="https://github.com/deepseek-ai/DualPipe" target="_blank">DualPipe GitHub 代码仓库</a></li>
                        <li><a href="https://github.com/deepseek-ai/eplb" target="_blank">EPLB GitHub 代码仓库</a></li>
                        <li><a href="https://github.com/deepseek-ai/profile-data" target="_blank">Profile Data GitHub 代码仓库</a></li>
                        <li><a href="https://github.com/deepseek-ai/3FS" target="_blank">3FS GitHub 代码仓库</a></li>
                        <li><a href="https://github.com/deepseek-ai/smallpond" target="_blank">Smallpond GitHub 代码仓库</a></li>
                        <li><a href="https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md" target="_blank">DeepSeek-V3/R1 推理系统概览</a></li>
                    </ul>

                    <hr>

                    <p><em>哎哟，觉得这玩意儿有意思？那可太好了！欢迎随时来跟我唠嗑，咱们可以细说GenAI、LLM这些"生人勿进"的技术话题。当然，骑行，利物浦，如何每次去北欧都能追到极光，怎样才能在斯瓦尔巴/格陵兰岛做半年义工这些好玩一点的也来者不拒。Cheers～ 🌌🚴‍♂️☕🤖</em></p>
                </div>
            </div>
        </article>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 Beren Meng. All rights reserved.</p>
        </div>
    </footer>

    <script>
        // Language toggle functionality
        document.addEventListener('DOMContentLoaded', function() {
            const languageToggle = document.getElementById('languageToggle');
            const body = document.body;

            languageToggle.addEventListener('click', function() {
                if (body.classList.contains('zh')) {
                    body.classList.remove('zh');
                    languageToggle.textContent = 'Switch to 中文';
                } else {
                    body.classList.add('zh');
                    languageToggle.textContent = 'Switch to English';
                }
            });
        });
    </script>
</body>
</html>